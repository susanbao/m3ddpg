0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -28.11427316589223, agent episode reward: [2.097918364944138, -30.212191530836368], time: 29.816
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -18.885935822595144, agent episode reward: [-0.2797001166141206, -18.606235705981025], time: 44.704
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.571233379470042, agent episode reward: [-4.646463771723372, -8.92476960774667], time: 45.407
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.978939442227812, agent episode reward: [-3.9988115391390573, -8.980127903088755], time: 44.675
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.07569397864364, agent episode reward: [-3.1673270995692047, -9.908366879074435], time: 45.882
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -14.383341299287865, agent episode reward: [-3.8548120159867105, -10.528529283301157], time: 44.938
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -15.264447984344255, agent episode reward: [-4.025392969939681, -11.239055014404574], time: 45.206
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.846247884590364, agent episode reward: [-3.4842399173417786, -11.362007967248584], time: 44.468
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -15.277721902916474, agent episode reward: [-3.8676471019033056, -11.410074801013169], time: 44.094
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.968648593047506, agent episode reward: [-3.674425277512568, -11.29422331553494], time: 44.608
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -15.840194478586964, agent episode reward: [-4.207415475740064, -11.632779002846897], time: 44.492
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.328671691734673, agent episode reward: [-4.095166111194779, -11.233505580539894], time: 45.46
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.585958128099563, agent episode reward: [-4.108204512301252, -11.477753615798312], time: 44.784
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.324365469919627, agent episode reward: [-4.039450886940721, -11.284914582978905], time: 44.508
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.163670062549567, agent episode reward: [-3.7113790421484274, -11.452291020401141], time: 45.059
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.378721383492831, agent episode reward: [-3.6637894805867814, -11.71493190290605], time: 45.242
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.288479640551577, agent episode reward: [-3.626955099642852, -11.661524540908726], time: 44.228
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.601046974363191, agent episode reward: [-4.017587132228426, -11.583459842134765], time: 44.542
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.281058955917201, agent episode reward: [-3.8061512355091662, -11.474907720408032], time: 45.024
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.3341186620552, agent episode reward: [-3.7058226005479797, -11.628296061507223], time: 45.543
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.378929527473513, agent episode reward: [-3.724891319532514, -11.654038207940998], time: 44.578
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.664947107296696, agent episode reward: [-3.810351219726763, -11.854595887569932], time: 45.71
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.59097982143834, agent episode reward: [-3.6396327295354167, -11.951347091902925], time: 45.689
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.657337799319741, agent episode reward: [-3.505693316604773, -12.151644482714968], time: 44.281
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.7271373764506, agent episode reward: [-3.7726458013573163, -11.954491575093279], time: 45.088
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.784989491307336, agent episode reward: [-3.5088505735079183, -12.276138917799416], time: 44.618
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.418833934438819, agent episode reward: [-3.4717265338546905, -11.947107400584128], time: 45.146
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.587893207866497, agent episode reward: [-3.707400009770493, -11.880493198096003], time: 45.436
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.824695237425672, agent episode reward: [-3.4860248197096757, -12.338670417715996], time: 45.573
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -16.071542606130347, agent episode reward: [-3.838736774259106, -12.232805831871243], time: 45.193
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.76309074874896, agent episode reward: [-3.717500018452916, -12.045590730296048], time: 45.991
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.734213972758456, agent episode reward: [-3.590471619273792, -12.143742353484663], time: 45.065
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -16.153149823182105, agent episode reward: [-3.8077131454184, -12.345436677763704], time: 44.549
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.656141496519167, agent episode reward: [-3.4806407797994123, -12.175500716719755], time: 45.711
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -16.1186411848728, agent episode reward: [-3.5316435378370636, -12.586997647035732], time: 45.682
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.999665521323266, agent episode reward: [-3.403889108507438, -12.595776412815828], time: 44.701
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.99788187955384, agent episode reward: [-3.65224602437319, -12.345635855180648], time: 45.387
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -16.151757553648157, agent episode reward: [-3.6729694335464855, -12.478788120101669], time: 44.03
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.807828140042288, agent episode reward: [-3.6539863601725004, -12.153841779869788], time: 44.564
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.81429406292878, agent episode reward: [-3.475281127716863, -12.339012935211917], time: 45.496
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -16.412553351178598, agent episode reward: [-3.5866942313345618, -12.825859119844033], time: 45.382
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -16.291307794401746, agent episode reward: [-3.564256158869135, -12.72705163553261], time: 44.97
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -16.22278568472376, agent episode reward: [-3.3609908700507263, -12.861794814673035], time: 44.091
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -16.041950455739325, agent episode reward: [-3.263194040950233, -12.778756414789095], time: 45.562
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -16.0064356295291, agent episode reward: [-2.974042704543255, -13.032392924985844], time: 45.517
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.818614715533942, agent episode reward: [-2.9884956584188376, -12.830119057115105], time: 46.169
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.29973211841513, agent episode reward: [-3.1915374354569868, -13.108194682958143], time: 46.94
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.012901195253388, agent episode reward: [-3.1585287551492343, -12.854372440104154], time: 46.065
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -16.080311296555834, agent episode reward: [-3.193468900230763, -12.886842396325072], time: 45.102
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -16.419564529951202, agent episode reward: [-3.2483404563741884, -13.171224073577017], time: 46.448
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.410560047537512, agent episode reward: [-3.3265789378097725, -13.083981109727734], time: 45.095
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -16.327565299591203, agent episode reward: [-3.4841170002012603, -12.84344829938994], time: 46.655
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.89351937833462, agent episode reward: [-2.8645570428325526, -13.028962335502067], time: 46.036
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -16.3189922505626, agent episode reward: [-3.2063762907712787, -13.112615959791322], time: 46.079
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.437657330984486, agent episode reward: [-3.3984037716913646, -13.039253559293124], time: 46.186
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -16.18852916156842, agent episode reward: [-3.241178071792218, -12.947351089776205], time: 46.018
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -16.48063570861078, agent episode reward: [-3.299937137486062, -13.180698571124719], time: 46.001
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.423590966873665, agent episode reward: [-2.9674752506036044, -13.456115716270062], time: 46.774
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.082013419660473, agent episode reward: [-3.272145432461472, -12.809867987199002], time: 46.306
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.576968878676695, agent episode reward: [-3.3556537192462352, -13.221315159430462], time: 45.972
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -16.445240890875095, agent episode reward: [-3.340288060355995, -13.1049528305191], time: 45.96
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.12265211043032, agent episode reward: [-3.27503427998416, -12.84761783044616], time: 45.842
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.20833938090035, agent episode reward: [-3.435982248956947, -12.772357131943401], time: 45.803
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.11460829883898, agent episode reward: [-3.376822451240407, -12.737785847598573], time: 46.519
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.286172463122867, agent episode reward: [-3.4142691707127435, -12.871903292410124], time: 45.999
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.14859975407511, agent episode reward: [-3.595306899493876, -12.553292854581237], time: 46.475
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.29709413996751, agent episode reward: [-3.5194046440099243, -12.777689495957583], time: 46.184
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.239178013439115, agent episode reward: [-3.4742335436050706, -12.764944469834044], time: 46.727
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.597761338951795, agent episode reward: [-3.9687372509075542, -12.62902408804424], time: 46.18
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.209113888233283, agent episode reward: [-3.4455448879782224, -12.763569000255059], time: 46.283
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.336634709118496, agent episode reward: [-3.714659113509689, -12.621975595608802], time: 46.065
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.986518751364013, agent episode reward: [-3.2263905362853422, -12.76012821507867], time: 46.806
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -16.36526355765553, agent episode reward: [-3.5515043085405043, -12.813759249115025], time: 45.791
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.1963384417175, agent episode reward: [-3.596968352433958, -12.599370089283545], time: 45.39
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.5920917085978, agent episode reward: [-3.6758459560813446, -12.916245752516454], time: 46.099
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.292722124211778, agent episode reward: [-3.6226154739057757, -12.670106650306003], time: 45.87
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.524651592212578, agent episode reward: [-3.429830936142991, -13.094820656069587], time: 45.33
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -16.426006566353895, agent episode reward: [-3.4988973439686815, -12.927109222385214], time: 44.651
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.439953723817432, agent episode reward: [-3.549856014483581, -12.890097709333851], time: 45.946
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.106554598586868, agent episode reward: [-3.1100004374553545, -12.996554161131515], time: 46.025
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.372092889236647, agent episode reward: [-3.5024046754205385, -12.869688213816106], time: 46.563
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.328861426678188, agent episode reward: [-3.6258868497829972, -12.702974576895187], time: 46.249
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.30244834819906, agent episode reward: [-3.7814327682130053, -12.521015579986054], time: 45.264
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -16.35105372203435, agent episode reward: [-3.409919418219443, -12.941134303814906], time: 46.743
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.274462382587476, agent episode reward: [-3.4337836112009454, -12.84067877138653], time: 46.856
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.44487375070063, agent episode reward: [-3.617877102953036, -12.826996647747592], time: 46.162
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.389254428434032, agent episode reward: [-3.796083209976908, -12.593171218457123], time: 46.007
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.466233056168093, agent episode reward: [-3.7667112214456573, -12.699521834722436], time: 45.928
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.67852783536218, agent episode reward: [-3.582743583750874, -13.095784251611304], time: 46.154
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.320451989264775, agent episode reward: [-3.786746510789241, -12.533705478475536], time: 46.351
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.89092178843897, agent episode reward: [-3.4750571895373645, -12.415864598901605], time: 45.894
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.66168405240576, agent episode reward: [-3.628855954899279, -13.032828097506487], time: 46.318
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.438629797180745, agent episode reward: [-3.5612635508976735, -12.877366246283072], time: 46.563
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.4138880012199, agent episode reward: [-3.697324325602033, -12.71656367561787], time: 45.933
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -16.44296374622055, agent episode reward: [-3.588097386796505, -12.854866359424049], time: 46.522
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.168470815975212, agent episode reward: [-3.3598079806569556, -12.808662835318259], time: 46.4
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.236269066116154, agent episode reward: [-3.4415208603476595, -12.794748205768496], time: 46.14
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -16.17445299884089, agent episode reward: [-3.5598376209746583, -12.614615377866233], time: 46.385
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.4719117032208, agent episode reward: [-3.602399349231585, -12.869512353989219], time: 45.379
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.712228726390112, agent episode reward: [-3.598136600834232, -13.114092125555878], time: 45.85
...Finished total of 100001 episodes.
