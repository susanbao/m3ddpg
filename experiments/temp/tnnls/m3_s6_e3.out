0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.562285937638933, agent episode reward: [-0.9558790741810058, -26.606406863457927], time: 37.866
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.560115632395142, agent episode reward: [-5.010995734754807, -17.549119897640335], time: 47.295
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -10.707107207589459, agent episode reward: [-3.6515063807235166, -7.055600826865942], time: 46.797
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -9.269508158207843, agent episode reward: [-2.5538315243724954, -6.715676633835348], time: 46.319
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -9.165619048339714, agent episode reward: [-2.508821336210514, -6.656797712129198], time: 46.554
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -9.05388222080859, agent episode reward: [-2.3233035093986727, -6.730578711409918], time: 46.989
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -8.597367837277574, agent episode reward: [-1.920185310328837, -6.677182526948739], time: 45.808
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -9.2487862926961, agent episode reward: [-2.588068775585655, -6.6607175171104425], time: 46.115
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -8.732540581613671, agent episode reward: [-1.982091024085887, -6.750449557527785], time: 46.511
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -8.861375840762735, agent episode reward: [-1.7102472538892113, -7.151128586873523], time: 46.865
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -9.070284269146049, agent episode reward: [-1.9476874984302746, -7.1225967707157745], time: 46.906
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -8.69557381497854, agent episode reward: [-1.9700582974352105, -6.725515517543327], time: 47.072
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -8.752158186202793, agent episode reward: [-1.6891875769414315, -7.062970609261361], time: 46.451
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -8.893002635563857, agent episode reward: [-1.9574464579049522, -6.935556177658904], time: 46.946
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -8.885247200240574, agent episode reward: [-2.0409846974270214, -6.844262502813553], time: 46.679
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -8.70362629659738, agent episode reward: [-1.8667669793229307, -6.836859317274448], time: 46.646
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -9.039699544727506, agent episode reward: [-2.1211717474166987, -6.918527797310807], time: 46.934
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -8.878690811742816, agent episode reward: [-2.073679843518176, -6.80501096822464], time: 47.171
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -8.848702474964462, agent episode reward: [-2.1162310124044175, -6.732471462560045], time: 46.33
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -8.897363229025908, agent episode reward: [-2.1215064824877596, -6.7758567465381505], time: 47.283
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -8.983659020509352, agent episode reward: [-2.432869035649623, -6.55078998485973], time: 46.987
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -9.20441794844925, agent episode reward: [-2.321938626518086, -6.882479321931163], time: 46.401
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -9.018548673677431, agent episode reward: [-2.252431235590071, -6.766117438087358], time: 47.203
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -8.700306839661096, agent episode reward: [-2.274375718951673, -6.425931120709423], time: 46.348
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -8.758104384007835, agent episode reward: [-2.0595877397233577, -6.698516644284478], time: 46.954
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -8.539693400912121, agent episode reward: [-1.8816615839849518, -6.6580318169271715], time: 46.735
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -8.864338311578711, agent episode reward: [-2.2274830808439376, -6.636855230734772], time: 47.322
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -9.234442322830398, agent episode reward: [-2.4524878798555623, -6.781954442974835], time: 46.338
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -8.6486961493618, agent episode reward: [-2.1413909044269603, -6.50730524493484], time: 47.219
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -9.138491947521363, agent episode reward: [-2.3148896287915433, -6.8236023187298205], time: 46.511
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -9.057707799279745, agent episode reward: [-2.323245378436062, -6.734462420843683], time: 47.277
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -8.846397426092878, agent episode reward: [-2.1829447989797055, -6.66345262711317], time: 46.881
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -8.860170723375948, agent episode reward: [-2.240009503980261, -6.620161219395685], time: 46.61
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -9.117225284506032, agent episode reward: [-2.0053903445091423, -7.11183493999689], time: 46.339
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -9.123549814819905, agent episode reward: [-1.9180162897532322, -7.205533525066668], time: 46.238
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -8.962039837157263, agent episode reward: [-2.0748163977772807, -6.887223439379982], time: 46.003
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -8.891165326427348, agent episode reward: [-1.954284606838516, -6.936880719588832], time: 46.972
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -8.96563271498971, agent episode reward: [-1.974458438280075, -6.991174276709635], time: 45.736
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -8.761050663607232, agent episode reward: [-1.8953665150494694, -6.865684148557762], time: 46.736
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -8.981222931318445, agent episode reward: [-2.2135143030634836, -6.7677086282549626], time: 45.692
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -8.757829686829616, agent episode reward: [-1.9693289508904894, -6.788500735939128], time: 47.071
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -9.011904512118852, agent episode reward: [-2.2376261537099316, -6.774278358408921], time: 45.137
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -9.023055122835201, agent episode reward: [-2.3077397834573805, -6.71531533937782], time: 44.621
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -9.125700048408088, agent episode reward: [-2.401366936725745, -6.724333111682345], time: 44.733
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -9.214008569420127, agent episode reward: [-2.067167470264578, -7.146841099155546], time: 44.955
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -8.979702597203605, agent episode reward: [-1.66700243233184, -7.312700164871763], time: 44.812
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -8.936203917505996, agent episode reward: [-1.6648852549272468, -7.27131866257875], time: 45.1
m
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -8.996495240322433, agent episode reward: [-1.3930498634079869, -7.60344537691445], time: 44.93
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -9.203779165645242, agent episode reward: [-0.4907139540435397, -8.713065211601704], time: 45.8
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -9.257307104284843, agent episode reward: [-0.6269632995111717, -8.630343804773672], time: 46.731
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -8.957738607342728, agent episode reward: [-0.5647588866795357, -8.392979720663192], time: 45.902
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -8.915294215062229, agent episode reward: [-0.0906948879873959, -8.824599327074832], time: 46.167
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -8.652322267772362, agent episode reward: [-0.6022780903636822, -8.05004417740868], time: 45.609
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -8.810119928416322, agent episode reward: [-1.097720389774, -7.712399538642321], time: 45.797
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -9.074593517172534, agent episode reward: [-1.2255065298504224, -7.849086987322113], time: 46.881
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -9.140620281095863, agent episode reward: [-0.9472626565938985, -8.193357624501965], time: 46.171
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -9.282383107506819, agent episode reward: [-0.6374117603255349, -8.644971347181283], time: 45.87
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -9.31408824946287, agent episode reward: [-0.5139459257767978, -8.800142323686073], time: 46.359
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -9.150265411153137, agent episode reward: [-0.807056515180542, -8.343208895972595], time: 46.295
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -8.683710006927686, agent episode reward: [-1.5953178728138022, -7.088392134113884], time: 45.579
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -8.721317955930077, agent episode reward: [-1.9442195596530127, -6.777098396277064], time: 45.835
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -8.784267568091694, agent episode reward: [-2.1426598515161297, -6.641607716575564], time: 45.658
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -9.197303679952865, agent episode reward: [-2.296129587235607, -6.901174092717259], time: 45.983
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -9.175630154679427, agent episode reward: [-2.6062426109859906, -6.569387543693435], time: 45.178
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -9.028873504816776, agent episode reward: [-1.5658403524978304, -7.463033152318946], time: 46.549
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -9.124481793972345, agent episode reward: [-1.6000107264000156, -7.524471067572329], time: 45.888
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -8.884763446385502, agent episode reward: [-1.332426348001967, -7.552337098383536], time: 45.938
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -9.15103849043692, agent episode reward: [-1.735989696382393, -7.415048794054526], time: 46.165
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -8.969446627198199, agent episode reward: [-1.7764551362255443, -7.192991490972652], time: 46.113
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -9.276127448290312, agent episode reward: [-2.2777990867140843, -6.998328361576227], time: 46.356
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -9.082353542326455, agent episode reward: [-2.154520743235471, -6.927832799090986], time: 45.995
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -9.17480583861189, agent episode reward: [-2.3795716902604926, -6.795234148351397], time: 46.39
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -9.035572145325528, agent episode reward: [-2.3818288925082354, -6.653743252817294], time: 46.116
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -8.878639975726436, agent episode reward: [-1.8262378088884001, -7.0524021668380374], time: 45.592
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -9.04031986582319, agent episode reward: [-2.0486484759104524, -6.991671389912735], time: 45.906
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -9.172359808271512, agent episode reward: [-1.9090881581624668, -7.263271650109045], time: 45.261
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -9.22476472204888, agent episode reward: [-2.3209972961987084, -6.903767425850173], time: 46.04
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -9.854012038105179, agent episode reward: [-2.8242033385453458, -7.029808699559833], time: 46.28
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -9.337778431033271, agent episode reward: [-2.2685745189712723, -7.069203912062001], time: 46.109
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -9.701968524703691, agent episode reward: [-2.1893758866024013, -7.512592638101291], time: 46.237
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -9.684302654446562, agent episode reward: [-1.463043514197274, -8.221259140249288], time: 46.425
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -9.664047083040051, agent episode reward: [-2.239597152023368, -7.424449931016686], time: 46.511
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -9.867013596839755, agent episode reward: [-2.689678587952472, -7.177335008887285], time: 46.1
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -9.580954682468757, agent episode reward: [-2.877634782883236, -6.703319899585522], time: 45.851
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -9.322964442620053, agent episode reward: [-2.884333407104745, -6.438631035515309], time: 45.905
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -9.220725793209498, agent episode reward: [-2.563223395041915, -6.657502398167582], time: 45.938
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -9.029030542096935, agent episode reward: [-2.3332037716260308, -6.695826770470903], time: 46.278
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -8.889778423075704, agent episode reward: [-2.045115945629785, -6.84466247744592], time: 45.686
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -9.073534991163294, agent episode reward: [-1.7431940869548102, -7.3303409042084855], time: 45.224
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -9.082300128924665, agent episode reward: [-1.5144817356145417, -7.567818393310123], time: 45.109
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -8.911302377117995, agent episode reward: [-1.6944879969947324, -7.21681438012326], time: 45.762
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -9.254147652115204, agent episode reward: [-2.0141973004256153, -7.239950351689586], time: 45.818
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -8.84824145810707, agent episode reward: [-2.1108633262879586, -6.737378131819112], time: 45.144
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -8.955126634056883, agent episode reward: [-1.990385335429372, -6.96474129862751], time: 46.081
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -9.025062000137357, agent episode reward: [-2.317151635979969, -6.70791036415739], time: 46.774
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -9.021921268862616, agent episode reward: [-2.2557494513909906, -6.766171817471626], time: 45.31
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -9.1244866288259, agent episode reward: [-2.3234084740539926, -6.801078154771907], time: 45.042
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -9.154329058689887, agent episode reward: [-2.224729233837409, -6.929599824852479], time: 45.391
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -9.089552747089485, agent episode reward: [-2.452959069920228, -6.636593677169257], time: 45.872
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -9.131710612446396, agent episode reward: [-2.592360276038238, -6.539350336408158], time: 38.97
...Finished total of 100001 episodes.
01 episodes.
