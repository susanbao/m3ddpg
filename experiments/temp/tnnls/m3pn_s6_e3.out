0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.97429123447794, agent episode reward: [-2.295842844773409, -25.678448389704528], time: 33.355
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.11272142573774, agent episode reward: [-4.2074350079607115, -17.905286417777027], time: 44.804
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -14.105398582000792, agent episode reward: [-4.458548055010031, -9.646850526990763], time: 44.742
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -13.123107571147816, agent episode reward: [-4.421501385715271, -8.701606185432544], time: 44.969
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.681273012619656, agent episode reward: [-3.2474808444777854, -9.43379216814187], time: 45.426
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -14.140582175786374, agent episode reward: [-3.7732489687308934, -10.367333207055482], time: 45.383
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.486226954894832, agent episode reward: [-3.7445755465822748, -10.741651408312558], time: 45.958
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.885723447856689, agent episode reward: [-3.8126308037820364, -11.073092644074654], time: 45.169
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.954750553923802, agent episode reward: [-3.6950740513853444, -11.25967650253846], time: 45.098
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -15.23345744681108, agent episode reward: [-4.025877164979745, -11.207580281831335], time: 45.911
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -15.086140314831766, agent episode reward: [-3.343760992345677, -11.74237932248609], time: 45.377
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.15996985627149, agent episode reward: [-3.600201907713955, -11.559767948557534], time: 46.042
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.634956172117892, agent episode reward: [-3.9526240879951837, -11.68233208412271], time: 45.619
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.3381809252036, agent episode reward: [-3.700854691983147, -11.637326233220454], time: 44.752
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.326843466756587, agent episode reward: [-3.7142883358363004, -11.612555130920287], time: 44.496
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.06708581036721, agent episode reward: [-3.171723342902592, -11.89536246746462], time: 45.717
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.399073452335763, agent episode reward: [-3.5001025016274174, -11.898970950708343], time: 44.974
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.256233496135996, agent episode reward: [-3.171931813835105, -12.084301682300893], time: 45.286
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.486910019318938, agent episode reward: [-3.5309809540595714, -11.955929065259367], time: 45.975
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.544262490356811, agent episode reward: [-3.4881660530694543, -12.056096437287355], time: 45.739
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.243671606075932, agent episode reward: [-3.319244198260338, -11.924427407815594], time: 45.14
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.674686480434957, agent episode reward: [-3.68415291428585, -11.990533566149105], time: 45.484
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.563360030281604, agent episode reward: [-3.4242240599873734, -12.139135970294232], time: 44.43
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.589133163326885, agent episode reward: [-3.3032232728238147, -12.285909890503074], time: 44.771
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.47834474746084, agent episode reward: [-2.987670275412211, -12.490674472048628], time: 45.383
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.551569794650176, agent episode reward: [-3.087368940436599, -12.464200854213574], time: 46.336
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.415715682956154, agent episode reward: [-3.0830612512666993, -12.332654431689454], time: 45.134
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.742476712690351, agent episode reward: [-2.9251180926775353, -12.817358620012815], time: 45.466
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.358449124227601, agent episode reward: [-2.6473407527636255, -12.711108371463979], time: 44.137
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.418209956318897, agent episode reward: [-2.8930260734355144, -12.525183882883383], time: 45.766
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.85833349354747, agent episode reward: [-3.0054453197946676, -12.852888173752804], time: 45.972
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.810559240546212, agent episode reward: [-3.14389122382498, -12.666668016721232], time: 45.188
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.847577010889065, agent episode reward: [-3.282382618476129, -12.565194392412938], time: 44.98
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -16.014311396528296, agent episode reward: [-3.0927460731812397, -12.921565323347057], time: 45.925
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.585589413415873, agent episode reward: [-3.0681432090645884, -12.517446204351286], time: 45.66
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.675208418947449, agent episode reward: [-3.0036516954629375, -12.671556723484509], time: 45.106
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.85229521158243, agent episode reward: [-3.0661946390105883, -12.786100572571844], time: 46.416
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.743495917202708, agent episode reward: [-3.3623019391896714, -12.381193978013036], time: 45.442
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.587972685808682, agent episode reward: [-3.200507036412432, -12.387465649396248], time: 44.778
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.8444247390615, agent episode reward: [-3.0257485489805505, -12.81867619008095], time: 44.764
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.676800360808395, agent episode reward: [-2.6465035385868108, -13.030296822221587], time: 46.51
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.308304813653365, agent episode reward: [-2.818408589425503, -12.489896224227865], time: 45.428
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.812287536868078, agent episode reward: [-2.86257695506838, -12.949710581799696], time: 46.668
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.975435800058573, agent episode reward: [-3.002545635019843, -12.97289016503873], time: 47.283
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -16.01382163831751, agent episode reward: [-3.3768628867199304, -12.636958751597577], time: 46.828
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.908813687964459, agent episode reward: [-3.2363397464249792, -12.67247394153948], time: 45.662
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.898334673116405, agent episode reward: [-2.9856332259938108, -12.912701447122593], time: 46.005
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.83437368828325, agent episode reward: [-3.0675789420863957, -12.766794746196856], time: 45.671
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -16.062398164943072, agent episode reward: [-3.3988068140845584, -12.663591350858512], time: 45.896
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.698092720658602, agent episode reward: [-2.7698578645998113, -12.928234856058792], time: 44.788
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.468009057997794, agent episode reward: [-3.760420978520991, -12.707588079476803], time: 46.941
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -16.16847193398123, agent episode reward: [-3.3222300310352697, -12.846241902945959], time: 46.701
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.975799954392556, agent episode reward: [-3.3279005282318574, -12.647899426160699], time: 45.554
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.77662851215903, agent episode reward: [-3.001265042930858, -12.775363469228173], time: 46.877
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.807750050128941, agent episode reward: [-3.127145715063858, -12.680604335065084], time: 45.882
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.91011810303513, agent episode reward: [-3.3222185101904302, -12.587899592844696], time: 44.982
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.66801633512003, agent episode reward: [-2.712577404609309, -12.955438930510722], time: 45.984
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.08186278479157, agent episode reward: [-3.1842872991269795, -12.897575485664595], time: 46.092
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.031172512626476, agent episode reward: [-3.526114732198838, -12.505057780427638], time: 45.394
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.163890955852214, agent episode reward: [-3.504577490903619, -12.659313464948593], time: 46.004
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -16.077100205311776, agent episode reward: [-3.3371102873024876, -12.739989918009288], time: 46.548
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.06272889818378, agent episode reward: [-3.4078990795719095, -12.654829818611873], time: 46.413
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.055005922658864, agent episode reward: [-3.5207653014009366, -12.53424062125793], time: 46.528
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.166603810431567, agent episode reward: [-3.3346584202156797, -12.831945390215886], time: 46.361
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.157985599841727, agent episode reward: [-3.448904937765524, -12.709080662076202], time: 46.164
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.299399426653242, agent episode reward: [-3.511895701916539, -12.787503724736707], time: 45.494
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.196168328742342, agent episode reward: [-3.388438507850716, -12.807729820891625], time: 46.957
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.12839100427117, agent episode reward: [-3.4982582813728706, -12.6301327228983], time: 44.959
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.02461619072256, agent episode reward: [-3.1592937053044166, -12.865322485418144], time: 44.416
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.061658316171037, agent episode reward: [-3.211850625678414, -12.849807690492623], time: 46.717
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.19221596307718, agent episode reward: [-3.504357915978771, -12.687858047098409], time: 45.751
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.246795141799485, agent episode reward: [-3.572838736945769, -12.673956404853715], time: 46.297
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.707689434235128, agent episode reward: [-3.10349903275259, -12.604190401482537], time: 46.569
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.02803247531694, agent episode reward: [-3.3257020020614543, -12.702330473255486], time: 46.984
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.06097108426992, agent episode reward: [-3.407965290357475, -12.653005793912445], time: 45.903
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.927319386334002, agent episode reward: [-3.2177636638187104, -12.709555722515292], time: 46.784
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.000368612693887, agent episode reward: [-3.1375291249399213, -12.862839487753966], time: 46.394
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.820246529893721, agent episode reward: [-3.193784282215605, -12.626462247678116], time: 47.267
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.858521642358406, agent episode reward: [-3.1352398686156637, -12.723281773742745], time: 46.865
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.9316014611249, agent episode reward: [-3.245583399123978, -12.686018062000922], time: 46.35
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -15.988934072696647, agent episode reward: [-3.1815170314664347, -12.807417041230211], time: 44.849
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.826981851681102, agent episode reward: [-3.1932675246413464, -12.633714327039756], time: 46.086
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.158362179463552, agent episode reward: [-3.4019387182212086, -12.756423461242342], time: 45.75
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -16.20185388568598, agent episode reward: [-3.6265791849572415, -12.57527470072874], time: 46.216
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.848328290888963, agent episode reward: [-3.3798079111148356, -12.468520379774127], time: 45.843
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.23835859659779, agent episode reward: [-3.6927617218136537, -12.545596874784136], time: 46.026
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.19544061993551, agent episode reward: [-3.648032058825435, -12.547408561110078], time: 45.801
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.30782476138363, agent episode reward: [-3.577513145033005, -12.73031161635062], time: 45.883
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.95939772838596, agent episode reward: [-3.5367036294472483, -12.422694098938711], time: 45.542
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.23321536017926, agent episode reward: [-3.641489928410974, -12.591725431768287], time: 45.685
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.21831286923748, agent episode reward: [-3.6474850330641346, -12.570827836173343], time: 46.172
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.917638118520216, agent episode reward: [-3.095415757244745, -12.822222361275472], time: 45.371
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.098943491892424, agent episode reward: [-3.492523756805248, -12.606419735087174], time: 45.816
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.169903752827096, agent episode reward: [-3.3864769979594556, -12.783426754867639], time: 46.684
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.875238629459528, agent episode reward: [-3.399923126696022, -12.475315502763506], time: 46.39
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.765991906918833, agent episode reward: [-3.38571722653151, -12.380274680387323], time: 46.306
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.82998079179406, agent episode reward: [-3.634226069635438, -12.195754722158625], time: 46.761
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -16.189593984899467, agent episode reward: [-3.717609830745421, -12.471984154154047], time: 45.707
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.914127655302256, agent episode reward: [-3.406861479025706, -12.507266176276548], time: 44.482
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.923168599358899, agent episode reward: [-3.4682388770647457, -12.454929722294155], time: 38.111
...Finished total of 100001 episodes.
