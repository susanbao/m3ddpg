0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.369692487525054, agent episode reward: [-0.569214388676152, -26.8004780988489], time: 31.046
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.246519499285444, agent episode reward: [-3.343344613568658, -18.903174885716787], time: 44.642
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.242782632096143, agent episode reward: [-4.851546775715987, -8.391235856380154], time: 44.387
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -11.276002166985156, agent episode reward: [-3.3180145241150596, -7.957987642870094], time: 44.949
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -11.282517906630373, agent episode reward: [-2.794596149435051, -8.487921757195323], time: 45.587
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -11.48971929973757, agent episode reward: [-2.948089677534018, -8.541629622203553], time: 45.906
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -12.189640657394841, agent episode reward: [-3.360983408920709, -8.828657248474133], time: 46.326
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -11.942703949017476, agent episode reward: [-3.1473351626473676, -8.795368786370107], time: 46.3
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -12.281660726355295, agent episode reward: [-3.233308327253738, -9.048352399101555], time: 45.956
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -12.243252342578726, agent episode reward: [-3.1765447030719223, -9.066707639506804], time: 45.81
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -12.497873612518218, agent episode reward: [-3.3127211213307657, -9.185152491187456], time: 45.633
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -12.593085218622761, agent episode reward: [-3.4411160112126598, -9.151969207410101], time: 46.612
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -12.364906060010162, agent episode reward: [-3.130810789198757, -9.234095270811405], time: 45.382
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -12.483697875503447, agent episode reward: [-3.2605306790763082, -9.223167196427141], time: 45.493
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -12.47873912760318, agent episode reward: [-3.400634213952464, -9.078104913650717], time: 45.006
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -12.341409929984522, agent episode reward: [-3.253884952676263, -9.087524977308261], time: 44.64
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -12.561551482970703, agent episode reward: [-3.3345274858313636, -9.22702399713934], time: 45.415
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -12.484000828026794, agent episode reward: [-3.451136716630016, -9.032864111396778], time: 46.206
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -12.356089439067107, agent episode reward: [-3.0095280355377683, -9.346561403529341], time: 46.443
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -12.709896715208966, agent episode reward: [-3.518865941576251, -9.191030773632717], time: 45.826
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -12.805357794975398, agent episode reward: [-3.574697026106024, -9.230660768869374], time: 46.002
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -13.033714102718063, agent episode reward: [-3.69429841780659, -9.339415684911474], time: 45.895
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -12.378791098060656, agent episode reward: [-3.1646891734287372, -9.214101924631919], time: 47.171
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -12.536349008795337, agent episode reward: [-3.3290223371656, -9.20732667162974], time: 45.628
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -12.448427355201162, agent episode reward: [-3.024657312225023, -9.42377004297614], time: 45.986
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -12.299154537175495, agent episode reward: [-3.0803915879091965, -9.218762949266297], time: 45.122
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -12.468327457113613, agent episode reward: [-3.043843531733997, -9.424483925379617], time: 45.105
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -12.495043946624188, agent episode reward: [-3.130281472969214, -9.364762473654972], time: 45.993
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -12.827093937595892, agent episode reward: [-3.628155802461136, -9.198938135134755], time: 46.243
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -12.73853910652137, agent episode reward: [-3.3134657669245775, -9.425073339596791], time: 47.262
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -12.77674741513549, agent episode reward: [-3.5737672257600392, -9.20298018937545], time: 46.021
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -12.829768636753743, agent episode reward: [-3.4939005803929284, -9.335868056360814], time: 46.619
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -12.848510455402216, agent episode reward: [-3.4057522620562186, -9.442758193345998], time: 44.684
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -12.473702975005018, agent episode reward: [-3.3962173163689866, -9.077485658636032], time: 45.479
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -12.432601675435606, agent episode reward: [-3.385689171419722, -9.046912504015884], time: 46.183
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -12.776034000699427, agent episode reward: [-3.660294083753066, -9.115739916946357], time: 46.674
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -12.792718420762363, agent episode reward: [-3.455171450369824, -9.33754697039254], time: 46.611
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -12.588618604425164, agent episode reward: [-3.3723856385060507, -9.216232965919115], time: 46.027
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -12.620193857568317, agent episode reward: [-3.4211314021565618, -9.199062455411756], time: 46.44
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -12.792486863684674, agent episode reward: [-3.5450404786067113, -9.247446385077962], time: 46.116
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -12.747606776342868, agent episode reward: [-3.509545491277312, -9.238061285065559], time: 46.558
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -13.056886556068356, agent episode reward: [-3.518446038647442, -9.538440517420913], time: 45.705
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -13.043457895991205, agent episode reward: [-3.4847169683143138, -9.55874092767689], time: 47.057
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -12.887140165132696, agent episode reward: [-3.4950804950085357, -9.392059670124159], time: 47.827
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -13.162699866005076, agent episode reward: [-3.4426200326537266, -9.720079833351349], time: 46.554
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -13.319360450030747, agent episode reward: [-3.630278296825824, -9.689082153204922], time: 47.605
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -13.33948764052296, agent episode reward: [-3.695569288186457, -9.643918352336502], time: 47.133
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -13.421270109408065, agent episode reward: [-3.653607038604777, -9.767663070803286], time: 45.831
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -13.309395499588645, agent episode reward: [-3.4771135949274923, -9.832281904661151], time: 46.647
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -13.494838578804524, agent episode reward: [-3.716351145294542, -9.778487433509982], time: 46.465
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -13.161768546425465, agent episode reward: [-3.3606417821163266, -9.801126764309139], time: 46.606
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -13.396336019487146, agent episode reward: [-3.7027496465058922, -9.693586372981253], time: 46.565
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -13.14746569116421, agent episode reward: [-3.5875229757806246, -9.559942715383587], time: 47.541
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -13.32943180974279, agent episode reward: [-3.6696663684336563, -9.659765441309133], time: 47.192
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -13.159469249302854, agent episode reward: [-3.478751958100901, -9.680717291201953], time: 47.633
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -13.06760607828094, agent episode reward: [-3.6513515142683337, -9.416254564012606], time: 47.375
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -13.583524043981841, agent episode reward: [-3.918482644567373, -9.665041399414468], time: 47.439
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -13.247118744784164, agent episode reward: [-3.666843926018713, -9.580274818765451], time: 47.471
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -13.290191996315057, agent episode reward: [-3.779214174449287, -9.51097782186577], time: 46.95
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -13.430675239917047, agent episode reward: [-3.966512742952221, -9.464162496964825], time: 47.031
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -12.869656582492519, agent episode reward: [-3.431701728798057, -9.437954853694462], time: 47.27
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -13.234584613373597, agent episode reward: [-3.610514896040245, -9.624069717333354], time: 47.22
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -13.459657813417058, agent episode reward: [-3.7999104259079597, -9.659747387509098], time: 46.881
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -13.187687517348294, agent episode reward: [-3.5492937440253653, -9.638393773322928], time: 47.146
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -13.504951967107536, agent episode reward: [-3.9036875940753917, -9.601264373032144], time: 46.795
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -13.254286342234806, agent episode reward: [-3.631440673661997, -9.62284566857281], time: 45.67
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -13.118754933672477, agent episode reward: [-3.5118742299636896, -9.60688070370879], time: 46.54
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -13.19006569377994, agent episode reward: [-3.651186588605326, -9.538879105174615], time: 46.784
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -13.318582170336231, agent episode reward: [-3.7090958049107727, -9.60948636542546], time: 47.81
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -13.27778784063507, agent episode reward: [-3.5868031772351388, -9.690984663399929], time: 46.001
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -13.121614354750113, agent episode reward: [-3.46248043647561, -9.659133918274502], time: 46.973
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -13.375132339223393, agent episode reward: [-3.5154657357771995, -9.859666603446193], time: 47.062
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -13.285247239484303, agent episode reward: [-3.609507979143879, -9.675739260340423], time: 47.537
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -13.374187350419867, agent episode reward: [-3.909928995284147, -9.464258355135721], time: 47.04
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -13.371341647901005, agent episode reward: [-3.808529049575272, -9.562812598325733], time: 46.979
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -13.261973188792204, agent episode reward: [-3.275662255356345, -9.98631093343586], time: 46.935
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -13.179700076112407, agent episode reward: [-3.56027984585603, -9.619420230256377], time: 47.713
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -13.634898577171144, agent episode reward: [-3.754248459055672, -9.88065011811547], time: 47.025
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -13.546033109985702, agent episode reward: [-3.8174442157242767, -9.728588894261424], time: 46.82
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -12.929811883153297, agent episode reward: [-3.2467045559014394, -9.683107327251859], time: 46.87
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -13.226502975874032, agent episode reward: [-3.167204409129952, -10.059298566744085], time: 47.307
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -13.503679331234816, agent episode reward: [-3.6019889780528813, -9.901690353181936], time: 47.134
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -13.31734488170181, agent episode reward: [-3.1986283405255107, -10.118716541176301], time: 47.617
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -13.53788425006927, agent episode reward: [-3.415229004102796, -10.122655245966474], time: 46.915
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -13.095767892688047, agent episode reward: [-3.2593993607086205, -9.836368531979424], time: 47.208
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -13.435592970994032, agent episode reward: [-3.5723714307344054, -9.863221540259627], time: 47.739
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -13.528422355280386, agent episode reward: [-3.6092992309233405, -9.919123124357045], time: 47.551
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -13.189602462204308, agent episode reward: [-3.39224757129984, -9.797354890904465], time: 47.776
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -13.133641818434004, agent episode reward: [-3.4988720912366307, -9.634769727197375], time: 47.824
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -12.92674066741676, agent episode reward: [-3.395328662301362, -9.531412005115401], time: 46.519
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -13.185867166367192, agent episode reward: [-3.208594282837478, -9.977272883529714], time: 46.343
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -13.0816820185461, agent episode reward: [-3.38494719658702, -9.696734821959081], time: 46.731
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -12.863034757852121, agent episode reward: [-3.157148781042829, -9.705885976809292], time: 47.899
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -13.020790269728998, agent episode reward: [-3.3879322633857893, -9.63285800634321], time: 48.011
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -13.427038512403975, agent episode reward: [-3.675079586634347, -9.751958925769628], time: 46.997
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -13.036008647951457, agent episode reward: [-3.2448418930085383, -9.791166754942921], time: 48.044
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -13.358856779020119, agent episode reward: [-3.6693507058473345, -9.689506073172787], time: 46.513
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -13.361623998682306, agent episode reward: [-3.7114547035916186, -9.650169295090688], time: 47.318
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -13.235509605109634, agent episode reward: [-3.4075679057262813, -9.827941699383352], time: 45.864
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -13.094896955733448, agent episode reward: [-3.581652627477639, -9.513244328255809], time: 39.74
...Finished total of 100001 episodes.
