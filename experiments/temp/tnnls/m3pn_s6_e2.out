0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.194780467937072, agent episode reward: [0.4651613566001872, -26.65994182453726], time: 33.679
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.972646258862603, agent episode reward: [-4.281979236225312, -16.69066702263729], time: 44.205
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -14.108736663423848, agent episode reward: [-4.948513848945557, -9.160222814478292], time: 44.138
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.861572200477967, agent episode reward: [-4.149056495806969, -8.712515704670995], time: 43.508
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.937704362393033, agent episode reward: [-3.61702680642786, -9.320677555965172], time: 43.006
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -14.165085389671832, agent episode reward: [-3.6276405297111496, -10.537444859960683], time: 45.616
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.870998846270478, agent episode reward: [-3.8479283778022215, -11.023070468468257], time: 45.341
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.90014516310963, agent episode reward: [-3.761122103431725, -11.139023059677907], time: 45.431
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -15.19019630276433, agent episode reward: [-3.7338717076755454, -11.456324595088784], time: 45.627
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -15.250376259570421, agent episode reward: [-4.079515196665195, -11.170861062905226], time: 44.574
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -15.525815619620863, agent episode reward: [-3.838334138499648, -11.687481481121214], time: 44.596
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.372580357263713, agent episode reward: [-3.668340119922106, -11.704240237341608], time: 46.226
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.30875051369589, agent episode reward: [-3.7761976083649564, -11.532552905330933], time: 45.499
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.389900489150115, agent episode reward: [-3.927171454152725, -11.46272903499739], time: 46.234
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.153377071977433, agent episode reward: [-3.627717415654025, -11.525659656323405], time: 45.774
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.552233740795597, agent episode reward: [-3.7254204162700426, -11.826813324525554], time: 44.973
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.454233913383277, agent episode reward: [-3.896040607562085, -11.55819330582119], time: 45.436
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.516777610654104, agent episode reward: [-3.69959060649926, -11.817187004154842], time: 44.255
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.803535056459426, agent episode reward: [-4.018493311090086, -11.785041745369341], time: 44.374
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.851448706122634, agent episode reward: [-3.8247430361035586, -12.026705670019075], time: 43.817
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.94637147929988, agent episode reward: [-4.019047702074121, -11.92732377722576], time: 44.695
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.869248481796486, agent episode reward: [-3.994272470892268, -11.874976010904218], time: 45.607
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.860467290462067, agent episode reward: [-3.863225493119574, -11.997241797342495], time: 45.748
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.422979479791833, agent episode reward: [-3.246547025451414, -12.176432454340418], time: 44.562
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -16.126555584410497, agent episode reward: [-3.5874375320774408, -12.539118052333059], time: 44.988
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -16.309615767830138, agent episode reward: [-4.260307262989857, -12.049308504840281], time: 45.263
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.987736922910567, agent episode reward: [-3.701893374022259, -12.285843548888307], time: 45.603
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.958530576424735, agent episode reward: [-3.6069821732452354, -12.351548403179498], time: 45.133
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.713535596193525, agent episode reward: [-3.4686580946562007, -12.244877501537324], time: 45.659
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.96037744061823, agent episode reward: [-3.6597895561548257, -12.300587884463406], time: 45.54
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.945560244057353, agent episode reward: [-3.4252806979134762, -12.520279546143879], time: 45.17
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.911679012329227, agent episode reward: [-3.367351421438011, -12.544327590891214], time: 45.668
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.837768812535815, agent episode reward: [-3.397501388653337, -12.440267423882476], time: 45.012
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -16.26655511695592, agent episode reward: [-3.53540471570871, -12.731150401247213], time: 44.808
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -16.013885968047486, agent episode reward: [-3.2630324979895726, -12.750853470057912], time: 45.862
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -16.02117786761808, agent episode reward: [-3.5092742743218195, -12.511903593296262], time: 45.119
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -16.206103763810567, agent episode reward: [-3.54022540941724, -12.665878354393328], time: 45.673
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -16.218214964700852, agent episode reward: [-3.756565078780246, -12.461649885920606], time: 45.562
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -16.10018881308732, agent episode reward: [-3.6934914260680305, -12.406697387019289], time: 45.482
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.98394820672985, agent episode reward: [-3.5289118340918617, -12.455036372637991], time: 45.58
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -16.09774042642879, agent episode reward: [-3.42139222055315, -12.676348205875641], time: 45.248
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -16.312234612571913, agent episode reward: [-3.6507839529605044, -12.661450659611408], time: 45.593
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -16.03064927825109, agent episode reward: [-3.4514048694509394, -12.579244408800148], time: 46.274
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -16.153272968184577, agent episode reward: [-3.625353988633, -12.527918979551577], time: 46.236
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.829518390574078, agent episode reward: [-2.9771356419109924, -12.852382748663084], time: 46.263
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.975350915350962, agent episode reward: [-3.104265648779246, -12.871085266571718], time: 46.084
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.103356370827868, agent episode reward: [-3.6621883603718897, -12.441168010455979], time: 45.796
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.631687012997928, agent episode reward: [-3.189838321913193, -12.441848691084735], time: 46.344
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.983022763494075, agent episode reward: [-3.35289126455248, -12.630131498941594], time: 46.701
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.66257268845202, agent episode reward: [-3.1522019764494873, -12.51037071200253], time: 46.976
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.841812770141285, agent episode reward: [-3.5029157659641412, -12.338897004177147], time: 45.855
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.609278529564941, agent episode reward: [-3.186775701886706, -12.422502827678237], time: 45.123
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.830840365012483, agent episode reward: [-3.1999246146073967, -12.630915750405086], time: 44.797
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.62109862305068, agent episode reward: [-2.9747677744905068, -12.646330848560172], time: 45.302
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.115703128403947, agent episode reward: [-3.300260411257732, -12.815442717146215], time: 45.232
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.909939367536925, agent episode reward: [-3.2249536688862066, -12.684985698650719], time: 45.983
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.66909792691901, agent episode reward: [-3.2288267306733878, -12.440271196245625], time: 46.227
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.227638053844814, agent episode reward: [-3.907463133187555, -12.320174920657257], time: 45.014
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.271310138516768, agent episode reward: [-3.6938854411589537, -12.577424697357815], time: 45.556
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.97502332696756, agent episode reward: [-3.401777979087031, -12.57324534788053], time: 46.335
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.984240896507494, agent episode reward: [-3.2310676731492625, -12.75317322335823], time: 45.194
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.023212228168713, agent episode reward: [-3.5012235162178422, -12.52198871195087], time: 46.827
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.693248845814368, agent episode reward: [-3.2153765961914815, -12.477872249622887], time: 46.231
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.33947402357496, agent episode reward: [-3.7366000570528133, -12.602873966522152], time: 45.75
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.1719191331718, agent episode reward: [-3.541904952435079, -12.630014180736724], time: 46.477
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.108620532367585, agent episode reward: [-3.659378094282075, -12.449242438085514], time: 46.137
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.90859226226691, agent episode reward: [-3.4678598673370895, -12.440732394929821], time: 44.983
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.07333984948086, agent episode reward: [-3.3495463115013253, -12.723793537979537], time: 44.622
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.966567999203278, agent episode reward: [-3.178631144590697, -12.787936854612582], time: 45.29
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.25642323385207, agent episode reward: [-3.901857716277464, -12.354565517574606], time: 45.592
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.391126479828664, agent episode reward: [-4.0701417042088215, -12.320984775619847], time: 45.327
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.58578225315607, agent episode reward: [-3.9653860814182464, -12.620396171737822], time: 46.606
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -16.544139909846947, agent episode reward: [-3.9849753181019305, -12.559164591745018], time: 45.543
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.3899503435704, agent episode reward: [-3.66299987485771, -12.726950468712692], time: 46.475
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.441206041031773, agent episode reward: [-4.0842639754605115, -12.356942065571262], time: 46.32
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.3116361007279, agent episode reward: [-3.7724211414717947, -12.539214959256102], time: 46.305
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.529871006721443, agent episode reward: [-3.9066003456723233, -12.62327066104912], time: 46.534
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -16.485556786397623, agent episode reward: [-4.072192526673006, -12.413364259724617], time: 46.721
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.519878132289115, agent episode reward: [-3.8640249627016776, -12.655853169587438], time: 47.077
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.550380353204375, agent episode reward: [-3.9856795699161855, -12.56470078328819], time: 46.013
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.218101277308133, agent episode reward: [-3.2887490986508228, -12.92935217865731], time: 46.029
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.48074039930135, agent episode reward: [-4.215165953399191, -12.26557444590216], time: 46.035
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.03398328008988, agent episode reward: [-3.5756719653184437, -12.45831131477144], time: 45.96
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -16.544734059048206, agent episode reward: [-3.7938667462378572, -12.750867312810346], time: 46.59
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.39997738364648, agent episode reward: [-3.5252350622890076, -12.874742321357472], time: 45.67
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.118108777488025, agent episode reward: [-3.5799611454305995, -12.538147632057427], time: 45.95
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.990739855505293, agent episode reward: [-3.6228256698630776, -12.367914185642213], time: 46.285
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.237143775422975, agent episode reward: [-3.6383816011606416, -12.598762174262335], time: 46.218
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.144980134260678, agent episode reward: [-3.577576368336083, -12.567403765924595], time: 45.913
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.40928933737705, agent episode reward: [-3.825590834643203, -12.58369850273385], time: 46.483
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.004641892000816, agent episode reward: [-3.5440603430478737, -12.460581548952941], time: 46.363
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.195012176501137, agent episode reward: [-3.885054456042104, -12.309957720459035], time: 45.887
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.044876625999006, agent episode reward: [-3.6581386947559964, -12.386737931243006], time: 46.801
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.605916804894093, agent episode reward: [-4.024284595325902, -12.581632209568191], time: 45.643
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.574655534063547, agent episode reward: [-2.7379441178372708, -12.836711416226278], time: 46.791
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.39099251901938, agent episode reward: [-3.5356553992432422, -12.855337119776141], time: 46.204
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.123214745286298, agent episode reward: [-3.355653455652358, -12.767561289633939], time: 45.706
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -16.202378843543958, agent episode reward: [-3.619840237690495, -12.582538605853461], time: 46.372
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.30553056321367, agent episode reward: [-3.4884695005875455, -12.817061062626125], time: 45.895
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.146062799640383, agent episode reward: [-3.6035267917494704, -12.542536007890915], time: 42.577
...Finished total of 100001 episodes.
