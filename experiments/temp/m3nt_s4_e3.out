0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -23.23672604352391, agent episode reward: [-35.58367093851535, 6.173472447495718, 6.173472447495718], time: 54.776
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -18.96745900840659, agent episode reward: [-27.271011337915063, 4.15177616475424, 4.15177616475424], time: 77.681
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 2.1485365538933863, agent episode reward: [-11.026599991302055, 6.5875682725977205, 6.5875682725977205], time: 76.776
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 2.331687371641536, agent episode reward: [-10.386782220812849, 6.359234796227193, 6.359234796227193], time: 77.911
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 2.256427655456422, agent episode reward: [-9.338597970383086, 5.797512812919754, 5.797512812919754], time: 77.821
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 1.5724328709576039, agent episode reward: [-9.593500983997016, 5.582966927477311, 5.582966927477311], time: 75.607
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 1.118140497269642, agent episode reward: [-9.570423505349947, 5.344282001309796, 5.344282001309796], time: 76.099
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 0.7861386267983319, agent episode reward: [-9.102726565712597, 4.944432596255463, 4.944432596255463], time: 75.666
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 0.5676056064828225, agent episode reward: [-9.28556678897839, 4.926586197730605, 4.926586197730605], time: 75.559
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 0.32423799343829357, agent episode reward: [-9.110594691971091, 4.717416342704693, 4.717416342704693], time: 78.743
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -0.32491669589348016, agent episode reward: [-9.894116099511445, 4.784599701808982, 4.784599701808982], time: 76.747
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -0.26698375392867313, agent episode reward: [-11.315344476776856, 5.524180361424092, 5.524180361424092], time: 75.205
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -0.6478240325128284, agent episode reward: [-11.125649448668941, 5.238912708078057, 5.238912708078057], time: 73.795
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -0.8481195025688184, agent episode reward: [-11.500194023585614, 5.326037260508398, 5.326037260508398], time: 75.972
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -1.3669680626917189, agent episode reward: [-11.65533257128911, 5.144182254298696, 5.144182254298696], time: 76.297
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -0.9908924670101017, agent episode reward: [-11.175445247228247, 5.092276390109071, 5.092276390109071], time: 76.229
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -1.5169583600535064, agent episode reward: [-11.10871943100831, 4.795880535477403, 4.795880535477403], time: 75.192
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -1.6715655671820164, agent episode reward: [-11.273914897051041, 4.801174664934513, 4.801174664934513], time: 76.086
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -2.4005413546535057, agent episode reward: [-11.187397396491043, 4.393428020918769, 4.393428020918769], time: 75.666
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -1.879192332577818, agent episode reward: [-11.36514941324385, 4.742978540333017, 4.742978540333017], time: 77.456
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -2.1801089288003643, agent episode reward: [-11.857462563346184, 4.83867681727291, 4.83867681727291], time: 76.184
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -2.484779428055236, agent episode reward: [-11.23229880976125, 4.3737596908530065, 4.3737596908530065], time: 77.833
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -2.6892947593840106, agent episode reward: [-11.248280886889203, 4.279493063752596, 4.279493063752596], time: 78.373
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -2.980781825840231, agent episode reward: [-10.818889350554107, 3.9190537623569384, 3.9190537623569384], time: 77.627
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -2.8035338089625546, agent episode reward: [-11.128757673884586, 4.162611932461015, 4.162611932461015], time: 75.94
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -3.377117313156031, agent episode reward: [-10.746801191879287, 3.6848419393616285, 3.6848419393616285], time: 75.775
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -3.0136059377901154, agent episode reward: [-11.057826968584452, 4.022110515397168, 4.022110515397168], time: 76.967
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -1.851975653988178, agent episode reward: [-10.896366441940978, 4.5221953939764, 4.5221953939764], time: 74.979
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -1.2594663014778895, agent episode reward: [-11.158755294260397, 4.949644496391254, 4.949644496391254], time: 76.543
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -1.3658853716399877, agent episode reward: [-11.765732521392685, 5.1999235748763475, 5.1999235748763475], time: 76.958
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -1.1390538451500785, agent episode reward: [-11.672327831665168, 5.266636993257546, 5.266636993257546], time: 78.851
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -0.6109625983675046, agent episode reward: [-11.719444106792965, 5.5542407542127314, 5.5542407542127314], time: 77.688
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -0.44878057877160815, agent episode reward: [-11.809151858014252, 5.680185639621322, 5.680185639621322], time: 77.633
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -0.9969257984377733, agent episode reward: [-12.1055303693005, 5.554302285431362, 5.554302285431362], time: 79.278
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -0.40825064552445545, agent episode reward: [-12.091059377809298, 5.841404366142422, 5.841404366142422], time: 77.256
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -0.9510046668726215, agent episode reward: [-12.158667708638779, 5.603831520883078, 5.603831520883078], time: 79.01
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -1.6801661207474243, agent episode reward: [-12.986948512592978, 5.653391195922777, 5.653391195922777], time: 75.896
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -1.04870274228815, agent episode reward: [-11.980354495721057, 5.465825876716454, 5.465825876716454], time: 77.074
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -0.2727244662322541, agent episode reward: [-12.647743489396122, 6.1875095115819345, 6.1875095115819345], time: 77.541
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -0.7996474207596164, agent episode reward: [-12.301473513241646, 5.750913046241016, 5.750913046241016], time: 78.603
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -1.3557285023450045, agent episode reward: [-11.983440350552494, 5.313855924103745, 5.313855924103745], time: 76.04
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -1.8507650887295133, agent episode reward: [-12.508470856365246, 5.3288528838178655, 5.3288528838178655], time: 79.319
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -2.2913828442015345, agent episode reward: [-13.608565080430399, 5.658591118114433, 5.658591118114433], time: 80.804
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -2.123388278176054, agent episode reward: [-12.323608793880918, 5.100110257852432, 5.100110257852432], time: 80.067
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -1.3319558114480197, agent episode reward: [-13.166841819338861, 5.91744300394542, 5.91744300394542], time: 76.929
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -0.4699919319127664, agent episode reward: [-12.337829600694414, 5.933918834390823, 5.933918834390823], time: 78.828
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -0.6997837279555004, agent episode reward: [-13.189307436329255, 6.244761854186876, 6.244761854186876], time: 80.099
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 0.16698846871962295, agent episode reward: [-12.156628454784004, 6.161808461751813, 6.161808461751813], time: 79.098
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 0.015949745196375716, agent episode reward: [-12.263902862765049, 6.139926303980712, 6.139926303980712], time: 80.23
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 0.22129106035735907, agent episode reward: [-12.716513976981089, 6.468902518669224, 6.468902518669224], time: 79.478
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -1.208114999504974, agent episode reward: [-12.207109987914999, 5.499497494205012, 5.499497494205012], time: 79.547
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -0.21809058920048568, agent episode reward: [-12.634138126743125, 6.208023768771319, 6.208023768771319], time: 79.2
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 0.3702525575306891, agent episode reward: [-12.247279568267599, 6.308766062899144, 6.308766062899144], time: 75.727
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 0.5118057002409432, agent episode reward: [-12.164931241917536, 6.3383684710792405, 6.3383684710792405], time: 77.966
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 0.8472089871827602, agent episode reward: [-12.916743030063335, 6.881976008623047, 6.881976008623047], time: 80.663
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 0.46792608288249815, agent episode reward: [-12.887971642652817, 6.677948862767658, 6.677948862767658], time: 80.163
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 0.3300333915513297, agent episode reward: [-12.832411810521677, 6.581222601036503, 6.581222601036503], time: 78.171
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 0.3204260773594352, agent episode reward: [-13.414052383817063, 6.8672392305882495, 6.8672392305882495], time: 78.761
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 0.6782300256861424, agent episode reward: [-13.044920871632334, 6.861575448659238, 6.861575448659238], time: 77.578
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 0.963043381515205, agent episode reward: [-12.876858524684634, 6.919950953099918, 6.919950953099918], time: 78.305
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: 0.7435114954789809, agent episode reward: [-12.545605926669019, 6.644558711074, 6.644558711074], time: 77.301
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: 0.9217603381354486, agent episode reward: [-12.75361634535428, 6.8376883417448635, 6.8376883417448635], time: 80.059
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: 0.9821451250868429, agent episode reward: [-12.522336530900976, 6.7522408279939095, 6.7522408279939095], time: 78.535
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: 0.4148786330754996, agent episode reward: [-13.312970026081826, 6.863924329578663, 6.863924329578663], time: 79.608
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: 0.6447142194760874, agent episode reward: [-12.459573394727219, 6.552143807101653, 6.552143807101653], time: 76.156
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -0.22715992593302844, agent episode reward: [-12.868670121176606, 6.32075509762179, 6.32075509762179], time: 76.373
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: 0.5488932707297326, agent episode reward: [-12.61773079487107, 6.583312032800401, 6.583312032800401], time: 79.611
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -0.6403133584943839, agent episode reward: [-12.243838215547994, 5.801762428526805, 5.801762428526805], time: 79.735
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: 0.9082601091563859, agent episode reward: [-12.953063851577529, 6.930661980366956, 6.930661980366956], time: 76.754
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: 0.8251663243934059, agent episode reward: [-13.310219403343671, 7.067692863868539, 7.067692863868539], time: 78.814
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: 0.6352622621869611, agent episode reward: [-13.061988186471721, 6.848625224329341, 6.848625224329341], time: 80.672
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: 1.1132976182112853, agent episode reward: [-13.047769696812711, 7.080533657511999, 7.080533657511999], time: 79.359
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: 0.6224888776549913, agent episode reward: [-12.839499891389808, 6.7309943845224005, 6.7309943845224005], time: 77.604
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: 0.8994340319183896, agent episode reward: [-12.944804896696082, 6.922119464307236, 6.922119464307236], time: 79.362
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: 0.5680764509836683, agent episode reward: [-12.794316526641214, 6.68119648881244, 6.68119648881244], time: 80.18
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: 1.200912178861367, agent episode reward: [-12.585631554035984, 6.893271866448676, 6.893271866448676], time: 76.995
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: 0.613818955381314, agent episode reward: [-12.686080605821374, 6.649949780601343, 6.649949780601343], time: 77.278
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: 0.38702909404717956, agent episode reward: [-12.815791719824265, 6.601410406935722, 6.601410406935722], time: 77.438
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: 0.8325481186904319, agent episode reward: [-12.131743593936045, 6.482145856313238, 6.482145856313238], time: 77.513
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: 0.8635834563984497, agent episode reward: [-12.930754390237878, 6.897168923318164, 6.897168923318164], time: 79.748
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: 0.6870698775426072, agent episode reward: [-13.10012096039218, 6.893595418967392, 6.893595418967392], time: 77.926
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -0.3147294946558892, agent episode reward: [-12.435100536584402, 6.0601855209642554, 6.0601855209642554], time: 79.87
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: 0.13353348243456584, agent episode reward: [-13.247027335287493, 6.69028040886103, 6.69028040886103], time: 79.493
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: 0.9550876861435454, agent episode reward: [-12.931884150184628, 6.943485918164086, 6.943485918164086], time: 80.587
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: 0.37614757240821267, agent episode reward: [-12.292688712876462, 6.3344181426423365, 6.3344181426423365], time: 78.331
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: 0.25731624757584665, agent episode reward: [-12.53708014775511, 6.397198197665479, 6.397198197665479], time: 79.499
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: 0.5102784092959655, agent episode reward: [-13.076360739580478, 6.793319574438222, 6.793319574438222], time: 79.868
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: 0.9207071324604913, agent episode reward: [-12.986519000949855, 6.953613066705173, 6.953613066705173], time: 79.371
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: 0.2968102732263884, agent episode reward: [-12.506575865779661, 6.401693069503025, 6.401693069503025], time: 78.991
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: 0.2622722379420608, agent episode reward: [-12.925637980289304, 6.593955109115683, 6.593955109115683], time: 79.478
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: 0.24881561847701256, agent episode reward: [-12.975056106143795, 6.611935862310402, 6.611935862310402], time: 81.127
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: 0.7676893332515392, agent episode reward: [-12.839803043594173, 6.803746188422857, 6.803746188422857], time: 77.043
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: 0.7080798039400225, agent episode reward: [-12.755206280876939, 6.731643042408481, 6.731643042408481], time: 79.452
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: 0.14176144295686463, agent episode reward: [-13.06642111076189, 6.604091276859377, 6.604091276859377], time: 79.196
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: 0.4225259960479248, agent episode reward: [-13.067895093877029, 6.745210544962475, 6.745210544962475], time: 80.352
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: 0.4712692096353827, agent episode reward: [-12.781830893086562, 6.626550051360972, 6.626550051360972], time: 76.396
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: 0.8830814238758787, agent episode reward: [-12.93255856151891, 6.9078199926973936, 6.9078199926973936], time: 77.568
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: 0.8882811195205393, agent episode reward: [-12.98681714624842, 6.93754913288448, 6.93754913288448], time: 78.093
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: 1.162413519955868, agent episode reward: [-12.860007783665559, 7.011210651810713, 7.011210651810713], time: 77.27
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: 0.4476982450345778, agent episode reward: [-11.990515548919463, 6.219106896977021, 6.219106896977021], time: 75.306
...Finished total of 100001 episodes.
