0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.242738733116916, agent episode reward: [1.455717036899227, -27.698455770016142], time: 10.29
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -24.258881719027777, agent episode reward: [-7.182103459484919, -17.07677825954286], time: 23.821
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.653217225773872, agent episode reward: [-4.658881686142531, -8.994335539631342], time: 24.084
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.84146525490312, agent episode reward: [-4.157600953988214, -8.683864300914909], time: 23.858
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.545706606942446, agent episode reward: [-3.1424397753552484, -9.403266831587196], time: 23.798
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.412364046645596, agent episode reward: [-3.4382521872544283, -9.974111859391167], time: 23.792
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.487598395518301, agent episode reward: [-3.405173516772932, -10.082424878745368], time: 24.38
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -13.71350803211355, agent episode reward: [-3.657023832411268, -10.056484199702282], time: 24.408
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -13.59415504111661, agent episode reward: [-3.305634650336002, -10.288520390780606], time: 24.511
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -13.52397377633882, agent episode reward: [-3.00793465661934, -10.51603911971948], time: 24.006
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -13.974207693125837, agent episode reward: [-3.2795403056468833, -10.694667387478956], time: 24.439
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.13434220918427, agent episode reward: [-3.322280727316868, -10.812061481867397], time: 24.01
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.086856818059285, agent episode reward: [-3.244747040603159, -10.842109777456125], time: 23.81
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.265423445146181, agent episode reward: [-3.358082392086259, -10.907341053059922], time: 23.874
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -13.702573128627227, agent episode reward: [-3.0207551267631234, -10.681818001864103], time: 24.153
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.327531673320516, agent episode reward: [-3.4140348534556426, -10.913496819864875], time: 24.521
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.079909106862775, agent episode reward: [-3.3139432741203954, -10.765965832742378], time: 24.298
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -13.804429473633848, agent episode reward: [-3.0110860417272836, -10.793343431906564], time: 24.005
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.101332466868126, agent episode reward: [-3.4539071139642146, -10.647425352903909], time: 24.247
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.118160492163057, agent episode reward: [-3.379560337755761, -10.738600154407298], time: 24.179
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.287203760685072, agent episode reward: [-3.5141294634180005, -10.77307429726707], time: 24.722
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.382824093829473, agent episode reward: [-3.5491088990258697, -10.833715194803602], time: 23.933
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.31645686634477, agent episode reward: [-3.506776004360423, -10.809680861984347], time: 24.945
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.764668556990818, agent episode reward: [-3.877908377214992, -10.886760179775829], time: 23.754
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.489616764579, agent episode reward: [-3.541482639903692, -10.94813412467531], time: 23.77
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -14.706452692166296, agent episode reward: [-3.8795989702287015, -10.826853721937596], time: 23.75
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -14.264830001876788, agent episode reward: [-3.1068933810649515, -11.157936620811837], time: 24.059
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -14.319961876304944, agent episode reward: [-3.2722800483515537, -11.04768182795339], time: 24.089
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.326914482139436, agent episode reward: [-3.247657387627255, -11.079257094512181], time: 24.899
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.627513709480962, agent episode reward: [-3.252574438953221, -11.374939270527742], time: 24.176
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -14.72405699277715, agent episode reward: [-3.730599901512095, -10.993457091265055], time: 23.589
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -14.225954638095146, agent episode reward: [-3.2978856573224244, -10.928068980772723], time: 24.349
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -14.788853536061367, agent episode reward: [-3.8680977146845663, -10.920755821376805], time: 24.226
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -14.574060421003486, agent episode reward: [-3.398405467020438, -11.175654953983049], time: 24.834
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -14.636365860133033, agent episode reward: [-3.60249237274757, -11.033873487385463], time: 24.076
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -14.687298851661529, agent episode reward: [-3.8029743489602796, -10.884324502701249], time: 24.801
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -14.63991256364623, agent episode reward: [-3.7915806782488524, -10.848331885397377], time: 24.54
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -14.549268684117258, agent episode reward: [-3.7426761505492383, -10.806592533568018], time: 24.559
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -14.554902506416136, agent episode reward: [-3.5691805795482487, -10.985721926867889], time: 24.333
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -14.821428705212343, agent episode reward: [-3.8592722149315555, -10.962156490280787], time: 23.706
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -14.47275414336178, agent episode reward: [-3.5743195045615384, -10.898434638800241], time: 24.734
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -14.644043414597917, agent episode reward: [-3.880888943833885, -10.763154470764027], time: 24.841
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.010715651869983, agent episode reward: [-4.179654778544833, -10.83106087332515], time: 24.78
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.058610819445155, agent episode reward: [-3.797838879871397, -11.260771939573758], time: 24.894
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -14.721881552666527, agent episode reward: [-3.694067635290822, -11.027813917375704], time: 24.723
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.112992375146263, agent episode reward: [-3.7996498622849857, -11.313342512861276], time: 23.431
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.093295244952992, agent episode reward: [-3.683761739441078, -11.409533505511915], time: 25.429
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.237788364528983, agent episode reward: [-3.6705363715020316, -11.567251993026952], time: 24.918
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.175075742049927, agent episode reward: [-3.8307869929864755, -11.34428874906345], time: 25.214
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.103873344623445, agent episode reward: [-3.919265048209801, -11.184608296413645], time: 24.933
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.128803069353024, agent episode reward: [-3.8126397846163664, -11.316163284736659], time: 24.677
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -14.789857135292753, agent episode reward: [-3.4553508501578274, -11.334506285134921], time: 25.087
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.072793933191798, agent episode reward: [-3.7630936412837728, -11.309700291908028], time: 24.702
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.057328589467152, agent episode reward: [-3.5596741404483216, -11.497654449018828], time: 24.61
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.254865673596436, agent episode reward: [-3.8817486624383117, -11.373117011158127], time: 24.77
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.00066999534648, agent episode reward: [-3.4629789571919956, -11.537691038154485], time: 25.898
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -14.713802843638147, agent episode reward: [-3.220119923719626, -11.493682919918523], time: 24.607
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -14.953143274750733, agent episode reward: [-3.3513833013572927, -11.60175997339344], time: 25.205
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -14.84125023093936, agent episode reward: [-3.59069298763123, -11.250557243308128], time: 24.937
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -14.946574013887163, agent episode reward: [-3.4458523610137908, -11.500721652873374], time: 25.036
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -14.715674481632927, agent episode reward: [-3.3946821688408155, -11.320992312792113], time: 25.13
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -14.650028463436055, agent episode reward: [-3.347406529444345, -11.30262193399171], time: 25.348
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -14.573438273067463, agent episode reward: [-3.605983267907406, -10.967455005160057], time: 24.467
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.187266663129154, agent episode reward: [-3.775054596693988, -11.412212066435165], time: 25.356
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -14.669617451966616, agent episode reward: [-3.4180932855055306, -11.251524166461088], time: 25.576
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -14.621659324819456, agent episode reward: [-3.4878080102436297, -11.133851314575827], time: 24.075
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -14.332772326129795, agent episode reward: [-3.3011589099162046, -11.031613416213593], time: 25.59
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -14.866809757198098, agent episode reward: [-3.460140731473259, -11.40666902572484], time: 25.456
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.316505338213519, agent episode reward: [-3.6154913568075067, -11.70101398140601], time: 24.475
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.086846238972205, agent episode reward: [-3.6838084654659258, -11.403037773506279], time: 24.882
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -14.815587862112727, agent episode reward: [-3.4887423531160784, -11.326845508996648], time: 24.435
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -14.964877461434508, agent episode reward: [-3.430445832997369, -11.534431628437138], time: 24.313
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -14.942084649999266, agent episode reward: [-3.4395784312121225, -11.502506218787145], time: 24.118
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -14.627953507877411, agent episode reward: [-3.2538718240175086, -11.374081683859904], time: 25.325
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -14.779340367652843, agent episode reward: [-3.249768557374574, -11.529571810278268], time: 25.145
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.089449952529915, agent episode reward: [-3.812588802700283, -11.27686114982963], time: 25.463
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -14.783409351447837, agent episode reward: [-3.6746839305041417, -11.108725420943696], time: 25.209
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.179991842845633, agent episode reward: [-3.8403008910127707, -11.33969095183286], time: 25.294
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.034956944766204, agent episode reward: [-3.4718957859044006, -11.563061158861805], time: 25.236
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.028748771869953, agent episode reward: [-3.4530994953201684, -11.575649276549782], time: 23.956
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -14.971718860689643, agent episode reward: [-3.519394188897467, -11.452324671792177], time: 23.805
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.125496124185231, agent episode reward: [-3.909604570144137, -11.215891554041095], time: 24.118
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.263221781187188, agent episode reward: [-4.099886407438691, -11.163335373748497], time: 23.344
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -14.889663892765943, agent episode reward: [-3.8030281412127342, -11.08663575155321], time: 25.132
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -14.833917629242794, agent episode reward: [-3.658253018110412, -11.175664611132385], time: 22.217
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -14.975618848779018, agent episode reward: [-3.6625155499611455, -11.31310329881787], time: 24.081
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.304747266364869, agent episode reward: [-3.9983193270739665, -11.306427939290902], time: 23.567
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.257946849081485, agent episode reward: [-3.9032422555536255, -11.354704593527858], time: 23.652
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.205699757226231, agent episode reward: [-3.7204110497821365, -11.485288707444095], time: 23.753
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -14.929147608110737, agent episode reward: [-3.847328823542248, -11.08181878456849], time: 23.615
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.166652137482021, agent episode reward: [-3.9663650069800562, -11.200287130501968], time: 23.252
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.352305364230764, agent episode reward: [-3.5788620508462965, -11.773443313384469], time: 23.703
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -14.99407591985575, agent episode reward: [-3.9956932818034336, -10.998382638052313], time: 23.629
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.352431775129945, agent episode reward: [-3.9607287990501514, -11.391702976079795], time: 23.706
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.626997281105151, agent episode reward: [-4.330357719958714, -11.296639561146437], time: 23.568
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -14.916764243288647, agent episode reward: [-4.202204690214063, -10.714559553074581], time: 23.131
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.153147581786408, agent episode reward: [-3.970924133506553, -11.182223448279856], time: 23.152
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -14.686771509427377, agent episode reward: [-3.7653190458938264, -10.921452463533551], time: 23.4
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.193450471458755, agent episode reward: [-3.930378103937627, -11.26307236752113], time: 23.564
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.350936313147665, agent episode reward: [-4.12171470446495, -11.229221608682716], time: 23.609
...Finished total of 100001 episodes.
