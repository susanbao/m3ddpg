0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.997892538830808, agent episode reward: [-0.6255111375651856, -26.372381401265628], time: 36.944
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -25.24250778685622, agent episode reward: [-9.866701981011554, -15.375805805844665], time: 46.604
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -10.630603484644878, agent episode reward: [-3.968884223165033, -6.661719261479844], time: 45.476
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -9.35612185630709, agent episode reward: [-2.6651386757706783, -6.690983180536409], time: 46.21
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -9.588410935615796, agent episode reward: [-2.4131105534833703, -7.175300382132425], time: 45.57
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -9.303143263688074, agent episode reward: [-2.517530560481231, -6.785612703206845], time: 45.822
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -8.784687715667168, agent episode reward: [-2.054996960406302, -6.729690755260868], time: 46.406
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -9.040166933952694, agent episode reward: [-2.1998369245718306, -6.840330009380865], time: 46.069
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -8.859835561363532, agent episode reward: [-1.9710282947679831, -6.88880726659555], time: 45.153
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -8.699632165201907, agent episode reward: [-2.0347866427818606, -6.6648455224200465], time: 46.295
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -9.05614287450078, agent episode reward: [-2.2266846594633316, -6.829458215037447], time: 46.406
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -9.054333948886551, agent episode reward: [-2.2490644761526473, -6.805269472733904], time: 46.518
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -8.992180964921921, agent episode reward: [-2.1411002122508553, -6.851080752671065], time: 46.046
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -9.031845321961212, agent episode reward: [-2.2973985923341833, -6.734446729627028], time: 45.431
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -8.961194510477501, agent episode reward: [-2.273475970293309, -6.687718540184191], time: 45.987
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -8.785471070434419, agent episode reward: [-2.1710543030977436, -6.614416767336674], time: 45.506
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -9.073394708748793, agent episode reward: [-2.3474684458087443, -6.725926262940051], time: 44.965
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -8.892906431532603, agent episode reward: [-2.3446664437774536, -6.54823998775515], time: 46.504
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -8.663509140751273, agent episode reward: [-1.7964229838276013, -6.867086156923672], time: 45.844
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -8.472554148764754, agent episode reward: [-1.7313204535676325, -6.7412336951971215], time: 46.368
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -8.975842121607231, agent episode reward: [-2.1287309378674584, -6.8471111837397745], time: 46.649
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -8.703105978877392, agent episode reward: [-2.002485642036136, -6.700620336841258], time: 45.995
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -8.800303627194905, agent episode reward: [-2.0597755645661824, -6.740528062628726], time: 46.367
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -8.89599183362942, agent episode reward: [-1.9752727936781866, -6.920719039951233], time: 46.507
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -9.127470591163608, agent episode reward: [-2.4091564539849406, -6.718314137178668], time: 45.811
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -9.303278134484687, agent episode reward: [-2.5880393915600197, -6.715238742924667], time: 46.341
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -8.737306052547844, agent episode reward: [-2.0907646023353363, -6.646541450212507], time: 45.942
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -9.192234728172986, agent episode reward: [-2.341188592284608, -6.851046135888376], time: 45.801
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -9.238193681197652, agent episode reward: [-2.1762083633443186, -7.061985317853333], time: 45.216
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -8.972935979216269, agent episode reward: [-2.134769089936925, -6.838166889279346], time: 44.948
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -8.848837584825763, agent episode reward: [-2.1460061469353406, -6.702831437890421], time: 45.298
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -8.890767194653272, agent episode reward: [-1.951492992949606, -6.939274201703667], time: 46.067
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -8.644576097566052, agent episode reward: [-1.9397945353658217, -6.704781562200231], time: 46.409
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -8.65302432645248, agent episode reward: [-2.0329646454114583, -6.620059681041022], time: 45.81
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -8.806894155479377, agent episode reward: [-2.12320601373609, -6.683688141743286], time: 45.553
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -8.984719739249092, agent episode reward: [-1.8329087520313743, -7.151810987217719], time: 45.397
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -9.117080824586804, agent episode reward: [-1.546152578446735, -7.570928246140069], time: 46.098
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -9.706120630885081, agent episode reward: [-2.664271700311374, -7.041848930573709], time: 45.907
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -9.945114535203828, agent episode reward: [-2.1483048694324673, -7.7968096657713595], time: 46.455
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -10.01523444007636, agent episode reward: [-2.374574520644933, -7.640659919431429], time: 42.452
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -10.169057743128768, agent episode reward: [-2.989581416008243, -7.179476327120525], time: 40.99
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -9.458490602597188, agent episode reward: [-2.2308771341737637, -7.227613468423422], time: 42.219
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -9.12671781213339, agent episode reward: [-1.560969912817815, -7.565747899315575], time: 41.245
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -8.935803321749413, agent episode reward: [-1.3334459039383002, -7.602357417811113], time: 41.926
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -8.707489797692539, agent episode reward: [-1.5436579824850543, -7.163831815207485], time: 40.683
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -9.17431619958483, agent episode reward: [-1.7201419825128315, -7.454174217071999], time: 40.409
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -8.897937656299323, agent episode reward: [-1.2463245275784052, -7.6516131287209195], time: 40.055
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -8.943719443004234, agent episode reward: [-1.831963726469985, -7.111755716534249], time: 39.867
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -8.571088936058192, agent episode reward: [-1.47494140810026, -7.096147527957935], time: 40.178
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -8.958346451055785, agent episode reward: [-1.9199154305300592, -7.038431020525728], time: 40.022
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -8.856277665840306, agent episode reward: [-2.0031110445069817, -6.853166621333324], time: 40.276
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -9.13245805843709, agent episode reward: [-2.027309680834925, -7.105148377602163], time: 40.16
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -8.285903129434626, agent episode reward: [-0.8614199291137288, -7.424483200320898], time: 39.767
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -8.76622556880153, agent episode reward: [-0.49517160897856927, -8.27105395982296], time: 39.519
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -8.523235084219323, agent episode reward: [-0.18589617077786355, -8.337338913441458], time: 39.993
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -8.871213366451268, agent episode reward: [-0.43236927623140975, -8.438844090219856], time: 39.943
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -9.017179414601195, agent episode reward: [-0.27090875380117707, -8.746270660800016], time: 39.869
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -8.920680618265278, agent episode reward: [-0.13327009772845438, -8.787410520536826], time: 39.837
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -8.867129357247359, agent episode reward: [-0.23770806840707404, -8.629421288840284], time: 39.821
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -8.86532243318415, agent episode reward: [-0.1802937825427482, -8.685028650641403], time: 40.105
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -9.308318055581145, agent episode reward: [-0.7212885953773973, -8.58702946020375], time: 39.993
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -9.19091002687906, agent episode reward: [-1.4035368214591377, -7.787373205419922], time: 39.727
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -9.182531426270485, agent episode reward: [-2.184961388591947, -6.997570037678539], time: 39.871
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -9.488363778231202, agent episode reward: [-1.6565385615162667, -7.831825216714936], time: 40.179
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -9.42234809878259, agent episode reward: [-0.48664969634794353, -8.935698402434648], time: 39.768
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -9.748155576897505, agent episode reward: [0.31758611261144476, -10.06574168950895], time: 39.92
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -9.307371282421652, agent episode reward: [-0.6617612790387755, -8.645610003382878], time: 40.124
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -9.065460951450342, agent episode reward: [-1.4424607435827503, -7.623000207867593], time: 39.818
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -8.784826048558516, agent episode reward: [-2.039937697461878, -6.744888351096637], time: 39.911
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -8.73574120059829, agent episode reward: [-1.5927101267281853, -7.143031073870108], time: 40.077
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -8.546325425745591, agent episode reward: [-1.3935698715397962, -7.152755554205796], time: 40.016
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -8.569934982664092, agent episode reward: [-0.9908138253160185, -7.579121157348074], time: 39.946
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -8.514515632687267, agent episode reward: [-0.5604291585569943, -7.954086474130274], time: 39.698
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -8.76414617895409, agent episode reward: [-0.4186532151739324, -8.345492963780162], time: 39.632
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -8.586965958230358, agent episode reward: [-0.30199640088091656, -8.284969557349442], time: 39.716
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -8.676903935207488, agent episode reward: [-0.5783603214930546, -8.098543613714432], time: 39.956
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -8.797123701815863, agent episode reward: [-1.2429807217495403, -7.554142980066323], time: 39.587
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -8.690985512085676, agent episode reward: [-1.2034227678213203, -7.487562744264356], time: 39.8
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -8.82387663379136, agent episode reward: [-1.2374334975115693, -7.586443136279789], time: 39.812
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -8.802273951322757, agent episode reward: [-0.34689194754386465, -8.455382003778894], time: 39.971
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -9.120019824777785, agent episode reward: [0.4304943504919718, -9.550514175269756], time: 39.942
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -9.175825416452167, agent episode reward: [1.6595207229557136, -10.83534613940788], time: 39.85
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -8.686415956166483, agent episode reward: [-1.2301395436970952, -7.456276412469389], time: 40.099
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -8.849881180190824, agent episode reward: [-1.9970939210698402, -6.8527872591209835], time: 40.103
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -8.920278713651483, agent episode reward: [-1.8408611412340896, -7.079417572417393], time: 39.873
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -9.107665446241604, agent episode reward: [-1.817314099064014, -7.290351347177588], time: 39.569
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -9.205686956034993, agent episode reward: [-0.9641981531152537, -8.24148880291974], time: 40.21
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -9.025618143848876, agent episode reward: [-0.2998626978891352, -8.725755445959742], time: 39.919
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -9.15159612296207, agent episode reward: [0.3769088152673627, -9.528504938229434], time: 39.746
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -9.018661666294232, agent episode reward: [0.5816457538562582, -9.600307420150491], time: 40.02
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -8.803990770221521, agent episode reward: [0.3369715899695078, -9.140962360191025], time: 39.744
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -9.075509537558707, agent episode reward: [0.05740632166088883, -9.132915859219596], time: 39.916
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -9.15421288328365, agent episode reward: [0.8360123760974622, -9.990225259381113], time: 39.999
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -9.291243851057766, agent episode reward: [1.1233288402569477, -10.414572691314715], time: 39.738
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -8.941932143505783, agent episode reward: [0.34283207738038857, -9.28476422088617], time: 43.07
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -8.874596276631564, agent episode reward: [0.6191933413513427, -9.493789617982907], time: 40.63
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -8.52699274962728, agent episode reward: [0.5518964465497049, -9.078889196176984], time: 41.714
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -8.588941155413053, agent episode reward: [0.3140963723314925, -8.903037527744546], time: 41.161
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -8.985267161502245, agent episode reward: [1.2893475186520496, -10.274614680154297], time: 42.011
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -8.41553490236156, agent episode reward: [2.0928768312619748, -10.508411733623536], time: 42.779
...Finished total of 100001 episodes.
