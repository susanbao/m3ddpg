0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -22.761272471036136, agent episode reward: [-38.62041363198801, 7.929570580475935, 7.929570580475935], time: 53.967
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -11.796622009901617, agent episode reward: [-17.313897752821116, 2.7586378714597513, 2.7586378714597513], time: 75.046
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 1.6994661415073316, agent episode reward: [-10.647376588404219, 6.173421364955774, 6.173421364955774], time: 74.49
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 2.470698907985408, agent episode reward: [-10.12905619543756, 6.299877551711483, 6.299877551711483], time: 78.068
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 2.355663731128624, agent episode reward: [-9.340731394302901, 5.848197562715764, 5.848197562715764], time: 74.473
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 1.7542196736838973, agent episode reward: [-9.352251039184456, 5.553235356434177, 5.553235356434177], time: 76.441
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 1.169045491147877, agent episode reward: [-9.196699020422132, 5.182872255785005, 5.182872255785005], time: 76.822
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 0.8446818344700937, agent episode reward: [-9.419290114318116, 5.131985974394105, 5.131985974394105], time: 74.421
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 0.17686641639837475, agent episode reward: [-9.768721284281522, 4.972793850339948, 4.972793850339948], time: 74.766
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -0.12927567574720478, agent episode reward: [-10.02086850839813, 4.9457964163254635, 4.9457964163254635], time: 75.919
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -0.4458442631852415, agent episode reward: [-10.435109257921747, 4.994632497368253, 4.994632497368253], time: 76.974
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -0.16900891942252996, agent episode reward: [-10.389479538123584, 5.110235309350527, 5.110235309350527], time: 76.165
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -0.18806859088677685, agent episode reward: [-11.399947105656508, 5.605939257384866, 5.605939257384866], time: 77.919
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -1.2776046907030105, agent episode reward: [-11.635305657961784, 5.178850483629387, 5.178850483629387], time: 76.074
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -0.8415135803875449, agent episode reward: [-11.664309570398785, 5.41139799500562, 5.41139799500562], time: 77.086
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -0.4238214117799055, agent episode reward: [-11.599176960952473, 5.587677774586283, 5.587677774586283], time: 76.479
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -0.4083952846163451, agent episode reward: [-10.98261302860759, 5.287108871995622, 5.287108871995622], time: 74.587
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -0.4253122412796742, agent episode reward: [-11.891505595861515, 5.733096677290922, 5.733096677290922], time: 77.456
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -0.7449697059617686, agent episode reward: [-12.267669367165777, 5.761349830602003, 5.761349830602003], time: 76.316
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -0.8877659122828268, agent episode reward: [-11.87663340683176, 5.494433747274466, 5.494433747274466], time: 76.611
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -1.4743820341048637, agent episode reward: [-12.629303588754107, 5.577460777324621, 5.577460777324621], time: 76.963
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -0.9241040095126919, agent episode reward: [-12.780190661235407, 5.928043325861357, 5.928043325861357], time: 76.799
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -0.14459452515205065, agent episode reward: [-12.84345034062496, 6.349427907736455, 6.349427907736455], time: 77.803
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -0.36588935655538846, agent episode reward: [-13.225133368168022, 6.429622005806317, 6.429622005806317], time: 77.314
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -0.786895749711057, agent episode reward: [-13.096853269056375, 6.154978759672658, 6.154978759672658], time: 77.818
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -1.1878022973966817, agent episode reward: [-12.613060827322853, 5.712629264963086, 5.712629264963086], time: 76.013
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 0.0015884679034200246, agent episode reward: [-12.553835083332563, 6.277711775617992, 6.277711775617992], time: 78.068
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -0.0672458722414431, agent episode reward: [-12.98200674670198, 6.4573804372302686, 6.4573804372302686], time: 78.61
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 0.32383778992218165, agent episode reward: [-13.134486068386868, 6.729161929154525, 6.729161929154525], time: 77.571
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 0.2584776500847402, agent episode reward: [-13.027483139567178, 6.64298039482596, 6.64298039482596], time: 77.451
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 0.647457725913115, agent episode reward: [-12.834227070172492, 6.740842398042803, 6.740842398042803], time: 77.98
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 0.29668518164659985, agent episode reward: [-13.158176520521506, 6.727430851084053, 6.727430851084053], time: 76.679
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 0.1664185688675333, agent episode reward: [-13.210265748461296, 6.6883421586644145, 6.6883421586644145], time: 77.859
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 0.8273303599785781, agent episode reward: [-13.054800842553666, 6.941065601266122, 6.941065601266122], time: 77.524
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 1.3913648347571594, agent episode reward: [-12.749021265159582, 7.070193049958371, 7.070193049958371], time: 77.268
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 0.5775621299342808, agent episode reward: [-12.92564309480571, 6.751602612369996, 6.751602612369996], time: 78.978
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 1.5641271753460497, agent episode reward: [-13.241911582023976, 7.403019378685011, 7.403019378685011], time: 79.485
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 1.4344943819131877, agent episode reward: [-13.866956530894624, 7.650725456403906, 7.650725456403906], time: 77.32
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 1.5244835434521458, agent episode reward: [-13.186639116439489, 7.355561329945818, 7.355561329945818], time: 79.8
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 1.7029669031142725, agent episode reward: [-13.136515436632118, 7.419741169873195, 7.419741169873195], time: 76.531
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 0.9980937543127691, agent episode reward: [-13.789411678785582, 7.393752716549176, 7.393752716549176], time: 78.394
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 0.7998283064717782, agent episode reward: [-13.547013594168641, 7.173420950320209, 7.173420950320209], time: 79.608
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 1.1443247632191966, agent episode reward: [-13.213823988096236, 7.179074375657717, 7.179074375657717], time: 79.846
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 0.810472770592912, agent episode reward: [-13.226310155307493, 7.018391462950202, 7.018391462950202], time: 81.776
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 1.502839367275405, agent episode reward: [-13.386451594995837, 7.4446454811356215, 7.4446454811356215], time: 77.61
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 1.7789459565031889, agent episode reward: [-13.092626201038046, 7.435786078770616, 7.435786078770616], time: 79.811
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 1.424476789977527, agent episode reward: [-13.562667646174699, 7.493572218076113, 7.493572218076113], time: 79.104
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 1.2482861613862797, agent episode reward: [-13.067647180550923, 7.157966670968601, 7.157966670968601], time: 80.242
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 1.9253706419596297, agent episode reward: [-12.958157977637564, 7.441764309798597, 7.441764309798597], time: 78.391
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 1.8113453360945198, agent episode reward: [-12.58861296806675, 7.199979152080634, 7.199979152080634], time: 79.078
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 1.919311887557793, agent episode reward: [-13.045153751144356, 7.482232819351075, 7.482232819351075], time: 78.256
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 1.2360948610785327, agent episode reward: [-13.240519857215132, 7.238307359146831, 7.238307359146831], time: 80.434
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 1.5904770268524917, agent episode reward: [-12.51844176152494, 7.054459394188716, 7.054459394188716], time: 80.907
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 1.507845910062799, agent episode reward: [-12.981511524352625, 7.244678717207712, 7.244678717207712], time: 79.895
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 1.8962945710365104, agent episode reward: [-13.08414175041801, 7.49021816072726, 7.49021816072726], time: 81.014
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 1.422979222359509, agent episode reward: [-12.722355441994521, 7.072667332177014, 7.072667332177014], time: 79.217
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 1.8029451300368469, agent episode reward: [-12.16066903414862, 6.981807082092733, 6.981807082092733], time: 78.411
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 1.4334109411114142, agent episode reward: [-12.693252944243167, 7.063331942677291, 7.063331942677291], time: 79.955
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 2.345880846330765, agent episode reward: [-13.0447031899649, 7.695292018147833, 7.695292018147833], time: 79.754
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 2.044977730010331, agent episode reward: [-13.06486318465955, 7.554920457334941, 7.554920457334941], time: 79.644
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: 1.6136233623782328, agent episode reward: [-12.235606320002429, 6.924614841190331, 6.924614841190331], time: 81.073
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: 2.335394812521718, agent episode reward: [-12.627459803050362, 7.481427307786041, 7.481427307786041], time: 78.986
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: 2.4306337684435118, agent episode reward: [-12.16502952745101, 7.297831647947262, 7.297831647947262], time: 78.805
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: 2.019860526491514, agent episode reward: [-12.507280074633384, 7.263570300562449, 7.263570300562449], time: 79.218
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: 2.2098012673358087, agent episode reward: [-12.531969917156765, 7.370885592246286, 7.370885592246286], time: 80.208
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: 1.22244099832458, agent episode reward: [-12.193863876886375, 6.708152437605478, 6.708152437605478], time: 80.628
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: 1.019129931261178, agent episode reward: [-12.702860113164043, 6.860995022212611, 6.860995022212611], time: 78.871
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: 1.524749034651, agent episode reward: [-12.174975252411727, 6.849862143531364, 6.849862143531364], time: 78.874
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: 2.3869179278209063, agent episode reward: [-12.661160924595276, 7.52403942620809, 7.52403942620809], time: 79.619
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: 2.310818631918015, agent episode reward: [-11.838013634524552, 7.074416133221283, 7.074416133221283], time: 79.652
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: 0.6028749901711369, agent episode reward: [-12.570815735639858, 6.586845362905498, 6.586845362905498], time: 77.879
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: 2.094465810998255, agent episode reward: [-12.255521221371943, 7.174993516185099, 7.174993516185099], time: 80.08
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: 1.0329374038523764, agent episode reward: [-12.875776181295024, 6.954356792573699, 6.954356792573699], time: 80.393
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: 1.4744238107406178, agent episode reward: [-12.509566984111139, 6.991995397425879, 6.991995397425879], time: 79.946
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: 1.3200425366595083, agent episode reward: [-12.299145276969265, 6.809593906814386, 6.809593906814386], time: 78.485
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: 1.6381718467854889, agent episode reward: [-12.61143605621353, 7.1248039514995085, 7.1248039514995085], time: 78.926
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: 1.4093217225703014, agent episode reward: [-12.356032081819182, 6.88267690219474, 6.88267690219474], time: 77.684
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: 1.6164767609002366, agent episode reward: [-12.543192886318582, 7.07983482360941, 7.07983482360941], time: 80.777
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: 0.7402137293948251, agent episode reward: [-13.022666028205622, 6.881439878800224, 6.881439878800224], time: 79.96
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: 2.07210243429524, agent episode reward: [-12.621326523865717, 7.346714479080478, 7.346714479080478], time: 80.621
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: 1.2948975824997415, agent episode reward: [-13.183146638805065, 7.239022110652404, 7.239022110652404], time: 79.642
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: 1.8239520792870234, agent episode reward: [-12.548775175867819, 7.18636362757742, 7.18636362757742], time: 79.837
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: 1.2546267546882963, agent episode reward: [-12.95723411188241, 7.105930433285353, 7.105930433285353], time: 79.68
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: 0.04226627700213473, agent episode reward: [-12.847071617419642, 6.4446689472108885, 6.4446689472108885], time: 78.612
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: 0.8906193245302625, agent episode reward: [-12.794139269238352, 6.842379296884308, 6.842379296884308], time: 77.993
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: 1.2678594145352569, agent episode reward: [-12.916608682619138, 7.092234048577197, 7.092234048577197], time: 79.431
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: 1.5871387614960417, agent episode reward: [-13.401116701510327, 7.494127731503184, 7.494127731503184], time: 77.333
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: 0.38768068517142523, agent episode reward: [-12.755362074360239, 6.571521379765833, 6.571521379765833], time: 80.876
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: 1.1030114597827974, agent episode reward: [-12.930086018808, 7.0165487392953985, 7.0165487392953985], time: 80.648
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: 1.3788981306213466, agent episode reward: [-12.68534101912171, 7.032119574871529, 7.032119574871529], time: 79.492
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: 0.9464078798657144, agent episode reward: [-13.389725433042921, 7.168066656454317, 7.168066656454317], time: 76.826
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: 1.2370840773242373, agent episode reward: [-12.834022469966525, 7.03555327364538, 7.03555327364538], time: 78.584
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: 1.7228445620900439, agent episode reward: [-13.12603543631317, 7.424439999201607, 7.424439999201607], time: 77.088
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: 1.545293892397773, agent episode reward: [-13.229392526805622, 7.387343209601698, 7.387343209601698], time: 79.126
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: 1.5182199715853981, agent episode reward: [-12.839637531253521, 7.178928751419458, 7.178928751419458], time: 78.999
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: 1.497232202722001, agent episode reward: [-12.583410538696963, 7.040321370709483, 7.040321370709483], time: 76.063
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: 1.135020569946641, agent episode reward: [-13.0237684762867, 7.079394523116669, 7.079394523116669], time: 77.55
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: 1.2072203441799316, agent episode reward: [-13.537494143562839, 7.372357243871385, 7.372357243871385], time: 77.006
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: 1.3131234855080216, agent episode reward: [-13.430926195710905, 7.372024840609464, 7.372024840609464], time: 77.891
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: 1.6475547202446401, agent episode reward: [-13.30892937237247, 7.478242046308554, 7.478242046308554], time: 71.252
...Finished total of 100001 episodes.
