0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.650658688725557, agent episode reward: [1.1219706676823649, -28.772629356407926], time: 37.802
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.801500742710935, agent episode reward: [-5.517476045692486, -15.284024697018445], time: 48.786
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.839709462493468, agent episode reward: [-4.866271063541701, -8.973438398951767], time: 48.059
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.478269830947406, agent episode reward: [-3.629633016960959, -8.848636813986447], time: 48.607
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.54731071270934, agent episode reward: [-2.97685953210838, -9.570451180600962], time: 47.533
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.527112102481246, agent episode reward: [-3.304295445485079, -10.22281665699617], time: 48.341
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.942015811070142, agent episode reward: [-3.368308004570476, -10.573707806499666], time: 48.059
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.188312406952889, agent episode reward: [-3.822528394360987, -10.365784012591904], time: 48.206
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.057030496438852, agent episode reward: [-3.5610963740457473, -10.495934122393106], time: 49.32
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.315398637414585, agent episode reward: [-3.7963518489881976, -10.519046788426385], time: 48.504
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.3185407724791, agent episode reward: [-3.791473262262213, -10.527067510216888], time: 47.046
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.455436788147422, agent episode reward: [-3.8917168794844685, -10.563719908662954], time: 46.636
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.33992476376142, agent episode reward: [-3.8779833056840483, -10.461941458077371], time: 48.001
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.214724490867761, agent episode reward: [-3.5064459136319113, -10.708278577235847], time: 46.334
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.580008145565234, agent episode reward: [-3.746397091363527, -10.833611054201707], time: 46.385
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.328539017025305, agent episode reward: [-3.7069457735006632, -10.621593243524643], time: 48.575
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.256496580868069, agent episode reward: [-3.572895119995584, -10.683601460872483], time: 46.578
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.606606638447118, agent episode reward: [-3.9056493879545937, -10.700957250492525], time: 47.086
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.167147248712874, agent episode reward: [-3.520971487898756, -10.646175760814115], time: 48.486
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.555502654177962, agent episode reward: [-3.9055843526523724, -10.64991830152559], time: 48.679
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.001170243771991, agent episode reward: [-4.033985693782378, -10.967184549989614], time: 48.12
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.487308460058461, agent episode reward: [-4.072392645127582, -10.414915814930879], time: 47.623
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.606860643822552, agent episode reward: [-3.9954904361025547, -10.611370207719999], time: 47.477
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.484734126189553, agent episode reward: [-3.953212423855761, -10.53152170233379], time: 48.509
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.572336477990572, agent episode reward: [-4.184881360460979, -10.387455117529592], time: 47.438
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -14.454439935390985, agent episode reward: [-3.489305429435586, -10.965134505955398], time: 47.114
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -14.311175607664657, agent episode reward: [-3.603145382175507, -10.708030225489148], time: 47.571
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -14.482891419939804, agent episode reward: [-3.972042558182467, -10.510848861757337], time: 47.748
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.788243122469884, agent episode reward: [-4.078119401674035, -10.710123720795847], time: 48.477
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.927033856636339, agent episode reward: [-4.244111778080453, -10.682922078555887], time: 47.552
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.156169544922312, agent episode reward: [-4.3598360302295545, -10.796333514692757], time: 47.65
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -14.903496049772283, agent episode reward: [-3.9960728854529215, -10.90742316431936], time: 48.941
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -14.899771586727397, agent episode reward: [-3.890070592072945, -11.009700994654454], time: 48.367
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.090481069031384, agent episode reward: [-3.9051301591353504, -11.185350909896036], time: 48.401
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -14.896775703008569, agent episode reward: [-4.036193044344965, -10.860582658663605], time: 47.392
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -14.836183965520453, agent episode reward: [-3.8007975642138674, -11.035386401306585], time: 48.029
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -14.845931452441928, agent episode reward: [-3.782505037907164, -11.063426414534762], time: 48.354
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -14.809185719128036, agent episode reward: [-3.923294028895606, -10.885891690232429], time: 46.171
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -14.852375874687953, agent episode reward: [-3.8217066077901474, -11.030669266897805], time: 46.562
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -14.746270018484374, agent episode reward: [-3.5732536663827448, -11.17301635210163], time: 46.61
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -14.911067094494447, agent episode reward: [-3.676615108203313, -11.234451986291132], time: 48.696
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -14.85068994024049, agent episode reward: [-3.8408963797531697, -11.009793560487322], time: 49.248
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.033640948835247, agent episode reward: [-3.8556695585139233, -11.177971390321323], time: 48.002
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.134529700487924, agent episode reward: [-3.6583759191771144, -11.476153781310812], time: 47.103
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.240099902270432, agent episode reward: [-3.877975684622358, -11.362124217648073], time: 47.572
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.233509434584903, agent episode reward: [-3.3641164433904347, -11.869392991194468], time: 47.529
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.521585917953846, agent episode reward: [-3.7321007258673324, -11.789485192086511], time: 47.288
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -14.94396482645538, agent episode reward: [-3.4518008711991097, -11.49216395525627], time: 47.74
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.220476197516115, agent episode reward: [-3.2714430323942096, -11.949033165121905], time: 48.536
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -14.90994411822646, agent episode reward: [-3.2521699641004207, -11.657774154126038], time: 47.482
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.421439974951944, agent episode reward: [-3.8230241606366944, -11.598415814315253], time: 47.618
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.23005502305234, agent episode reward: [-3.7653926974566616, -11.464662325595679], time: 48.981
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.745609441130467, agent episode reward: [-4.064388298478785, -11.681221142651681], time: 46.879
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.14188057354396, agent episode reward: [-3.5273003461107724, -11.614580227433185], time: 47.485
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.11792748156106, agent episode reward: [-3.5095744059812506, -11.60835307557981], time: 48.012
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.043753281974444, agent episode reward: [-3.405351655221107, -11.638401626753337], time: 47.357
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.228172206695497, agent episode reward: [-3.6126585653867886, -11.61551364130871], time: 49.365
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.131007762055473, agent episode reward: [-3.7130820431214326, -11.41792571893404], time: 48.394
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.01429036614215, agent episode reward: [-3.405848460506768, -11.608441905635381], time: 49.117
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -14.923376138037778, agent episode reward: [-3.2341268935047927, -11.689249244532984], time: 48.212
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -14.867513156432338, agent episode reward: [-3.104930231223844, -11.762582925208493], time: 48.058
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.360988211207202, agent episode reward: [-3.665299715759063, -11.695688495448136], time: 49.3
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.19003163258348, agent episode reward: [-3.7448188266239604, -11.44521280595952], time: 49.431
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -14.902425563880515, agent episode reward: [-3.332507860050242, -11.56991770383027], time: 49.402
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.315445303561722, agent episode reward: [-3.8791892190656005, -11.436256084496119], time: 49.21
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.251590358504886, agent episode reward: [-4.019042345783121, -11.232548012721766], time: 47.897
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.313989022851734, agent episode reward: [-4.124318024033796, -11.18967099881794], time: 49.09
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.250234074242007, agent episode reward: [-3.891177473570428, -11.359056600671577], time: 48.704
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.223602942163433, agent episode reward: [-3.6656892717705767, -11.557913670392857], time: 49.828
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.201382342050703, agent episode reward: [-3.901166981014094, -11.30021536103661], time: 49.331
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.25176107076907, agent episode reward: [-3.958646592995321, -11.29311447777375], time: 49.047
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.487933827651858, agent episode reward: [-3.975788725916226, -11.51214510173563], time: 49.718
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.562282130316875, agent episode reward: [-4.161249117974791, -11.401033012342085], time: 48.581
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.50871535979076, agent episode reward: [-4.4539745144515726, -11.054740845339188], time: 48.107
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.25835875432746, agent episode reward: [-4.074850063069313, -11.183508691258146], time: 46.475
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.177281243667629, agent episode reward: [-3.968102737091543, -11.209178506576086], time: 48.439
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -15.162972283523299, agent episode reward: [-4.045729430361792, -11.117242853161509], time: 49.692
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.638549538420076, agent episode reward: [-4.349079417673458, -11.28947012074662], time: 48.61
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.261696556820349, agent episode reward: [-4.02738776426085, -11.2343087925595], time: 48.032
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.352239577597913, agent episode reward: [-4.0393376172712285, -11.312901960326684], time: 48.557
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -15.330941183848335, agent episode reward: [-4.28436510659631, -11.046576077252025], time: 49.468
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.50023843569668, agent episode reward: [-4.217363133905163, -11.282875301791515], time: 46.673
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.687153333833743, agent episode reward: [-4.351492288345341, -11.3356610454884], time: 49.17
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.677196772408024, agent episode reward: [-4.194643170574559, -11.482553601833464], time: 47.041
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.781607168021434, agent episode reward: [-4.254863515253133, -11.526743652768301], time: 46.576
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -15.477879166805028, agent episode reward: [-3.9522684041560896, -11.52561076264894], time: 48.389
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.597958611432736, agent episode reward: [-3.9206345815316572, -11.677324029901078], time: 49.838
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.281477283148336, agent episode reward: [-4.0751027648935345, -11.206374518254801], time: 47.944
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.618926056375994, agent episode reward: [-4.485913216547146, -11.133012839828847], time: 48.97
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.626416994308098, agent episode reward: [-4.33355460681879, -11.29286238748931], time: 47.172
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.864095163081549, agent episode reward: [-4.179541748659678, -11.684553414421872], time: 47.488
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.77371028918685, agent episode reward: [-3.853174404114027, -11.920535885072823], time: 48.922
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.447506660218735, agent episode reward: [-4.122895693615548, -11.324610966603188], time: 49.003
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.121931827602964, agent episode reward: [-4.185912490614302, -11.936019336988663], time: 48.612
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.817045173662088, agent episode reward: [-4.02980730029121, -11.787237873370877], time: 49.262
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.53768083454648, agent episode reward: [-3.8620193950454587, -11.67566143950102], time: 49.849
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.42964508051167, agent episode reward: [-3.8844450490381277, -11.545200031473541], time: 47.334
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.71936095723386, agent episode reward: [-3.9130898627012787, -11.806271094532583], time: 48.952
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.928612908880684, agent episode reward: [-3.9366789155756368, -11.991933993305047], time: 49.477
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.960821037497881, agent episode reward: [-4.278272634530791, -11.68254840296709], time: 47.707
...Finished total of 100001 episodes.
