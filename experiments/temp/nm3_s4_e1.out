0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -21.891760862571697, agent episode reward: [-36.372820722836465, 7.240529930132384, 7.240529930132384], time: 106.064
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -17.358501575139496, agent episode reward: [-31.604411148992266, 7.122954786926383, 7.122954786926383], time: 127.43
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 3.797674791351265, agent episode reward: [-9.914353458105223, 6.856014124728245, 6.856014124728245], time: 125.122
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 3.1446790339164985, agent episode reward: [-9.908995869484894, 6.526837451700697, 6.526837451700697], time: 126.215
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 2.3725837399413776, agent episode reward: [-9.63383385492369, 6.003208797432533, 6.003208797432533], time: 124.767
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 2.4336827809255888, agent episode reward: [-9.847781520040098, 6.140732150482844, 6.140732150482844], time: 124.283
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 2.4492385574635693, agent episode reward: [-10.690483775664203, 6.569861166563886, 6.569861166563886], time: 122.204
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 1.7245434686690169, agent episode reward: [-10.586008877311837, 6.155276172990427, 6.155276172990427], time: 123.945
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 1.0230537272325098, agent episode reward: [-11.267266047516442, 6.145159887374475, 6.145159887374475], time: 124.846
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 1.1963895416950354, agent episode reward: [-11.007964725939013, 6.102177133817023, 6.102177133817023], time: 124.533
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 0.8372642220190856, agent episode reward: [-10.821655067400442, 5.829459644709765, 5.829459644709765], time: 124.52
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 1.52055057568416, agent episode reward: [-10.012999650581081, 5.766775113132621, 5.766775113132621], time: 125.267
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 1.7341545324770995, agent episode reward: [-11.023842936578426, 6.3789987345277614, 6.3789987345277614], time: 123.412
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 0.9899213473005182, agent episode reward: [-10.914103423120828, 5.952012385210674, 5.952012385210674], time: 124.569
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 1.867343813143286, agent episode reward: [-11.281291431102947, 6.574317622123117, 6.574317622123117], time: 124.891
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 1.5394276042485098, agent episode reward: [-11.509117422350519, 6.524272513299515, 6.524272513299515], time: 124.313
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 1.8001636594946842, agent episode reward: [-12.037428956030638, 6.918796307762661, 6.918796307762661], time: 125.373
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 3.1744252585209414, agent episode reward: [-12.473701783186272, 7.824063520853606, 7.824063520853606], time: 123.557
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 3.0541571494302753, agent episode reward: [-12.26573647034079, 7.659946809885534, 7.659946809885534], time: 123.617
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 2.8038393916481126, agent episode reward: [-12.572478937546853, 7.688159164597484, 7.688159164597484], time: 124.653
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 2.877110885900385, agent episode reward: [-12.188213122473078, 7.532662004186732, 7.532662004186732], time: 125.302
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 2.5295225865606166, agent episode reward: [-12.31629329524438, 7.4229079409024985, 7.4229079409024985], time: 123.484
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 3.1001816775519315, agent episode reward: [-12.53097275834143, 7.815577217946682, 7.815577217946682], time: 122.822
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 3.261586573442828, agent episode reward: [-12.612753751359273, 7.937170162401049, 7.937170162401049], time: 124.354
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 3.763005391564404, agent episode reward: [-12.478233193263774, 8.120619292414089, 8.120619292414089], time: 124.134
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 3.23698372595882, agent episode reward: [-12.591902538990453, 7.914443132474636, 7.914443132474636], time: 123.806
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 3.2271830210553816, agent episode reward: [-12.803903849032633, 8.015543435044009, 8.015543435044009], time: 122.661
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 2.950394222020056, agent episode reward: [-12.494734370647922, 7.7225642963339896, 7.7225642963339896], time: 123.037
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 3.136246219901245, agent episode reward: [-12.18826212049186, 7.662254170196552, 7.662254170196552], time: 123.131
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 3.632493316597919, agent episode reward: [-12.48106900519783, 8.056781160897874, 8.056781160897874], time: 123.119
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 3.837615677728061, agent episode reward: [-12.925462848871579, 8.381539263299821, 8.381539263299821], time: 122.724
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 4.1120942166968915, agent episode reward: [-13.36207325758358, 8.737083737140235, 8.737083737140235], time: 124.008
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 4.72871817423125, agent episode reward: [-13.167107866576455, 8.947913020403853, 8.947913020403853], time: 126.885
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 4.603803746074567, agent episode reward: [-13.21531861960868, 8.909561182841625, 8.909561182841625], time: 122.763
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 4.460608640481257, agent episode reward: [-13.46161789327987, 8.961113266880563, 8.961113266880563], time: 122.941
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 4.601137752329516, agent episode reward: [-13.299160243504083, 8.9501489979168, 8.9501489979168], time: 124.648
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 4.481124715478502, agent episode reward: [-13.046406701680965, 8.763765708579733, 8.763765708579733], time: 124.025
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 4.358026325630286, agent episode reward: [-13.495204561533576, 8.926615443581932, 8.926615443581932], time: 122.659
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 4.697659727759276, agent episode reward: [-13.135218033639992, 8.916438880699634, 8.916438880699634], time: 126.129
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 4.806970468806374, agent episode reward: [-13.3145180438967, 9.060744256351535, 9.060744256351535], time: 126.564
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 4.435155353859389, agent episode reward: [-13.474501037092963, 8.954828195476177, 8.954828195476177], time: 126.367
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 4.31807493110162, agent episode reward: [-14.135308762089906, 9.226691846595763, 9.226691846595763], time: 127.308
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 4.011138220581213, agent episode reward: [-12.549111211237717, 8.280124715909466, 8.280124715909466], time: 125.547
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 4.214745476456783, agent episode reward: [-12.873091383304677, 8.54391842988073, 8.54391842988073], time: 124.932
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 3.616608634639741, agent episode reward: [-12.894877583931192, 8.255743109285469, 8.255743109285469], time: 125.331
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 3.388352664983119, agent episode reward: [-12.945545721768031, 8.166949193375574, 8.166949193375574], time: 127.312
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 3.7083872199109726, agent episode reward: [-12.61798758911823, 8.163187404514602, 8.163187404514602], time: 125.125
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 3.8285327796897017, agent episode reward: [-12.610396930771698, 8.2194648552307, 8.2194648552307], time: 127.162
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 3.685895445740024, agent episode reward: [-12.786034940504347, 8.235965193122185, 8.235965193122185], time: 127.216
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 3.5532030433493964, agent episode reward: [-12.631523317137823, 8.092363180243611, 8.092363180243611], time: 127.731
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 3.799021826243731, agent episode reward: [-13.461875858864705, 8.630448842554218, 8.630448842554218], time: 126.157
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 3.8163238649258915, agent episode reward: [-13.585726904648945, 8.70102538478742, 8.70102538478742], time: 126.912
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 3.5449768324975603, agent episode reward: [-13.625583513778166, 8.585280173137862, 8.585280173137862], time: 127.036
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 3.6866920271973815, agent episode reward: [-12.962771181719239, 8.32473160445831, 8.32473160445831], time: 125.026
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 3.68097331215223, agent episode reward: [-12.754402418357708, 8.217687865254968, 8.217687865254968], time: 126.693
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 3.8031140141210593, agent episode reward: [-13.497776826130258, 8.65044542012566, 8.65044542012566], time: 126.118
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 3.5126349587198598, agent episode reward: [-13.616858436455853, 8.564746697587855, 8.564746697587855], time: 127.288
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 3.5239492852302865, agent episode reward: [-13.021567940413133, 8.272758612821708, 8.272758612821708], time: 127.422
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 2.712157680808232, agent episode reward: [-13.388191166965616, 8.050174423886924, 8.050174423886924], time: 124.908
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 2.8249163587989665, agent episode reward: [-13.807555932812663, 8.316236145805817, 8.316236145805817], time: 125.726
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: 2.3469227972872115, agent episode reward: [-13.788618088682817, 8.067770442985013, 8.067770442985013], time: 125.194
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: 1.85742209930208, agent episode reward: [-14.202358761947064, 8.029890430624572, 8.029890430624572], time: 127.109
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: 0.952536144497831, agent episode reward: [-13.636105946190481, 7.294321045344155, 7.294321045344155], time: 126.457
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: 0.23725266419410884, agent episode reward: [-13.647093173279325, 6.942172918736716, 6.942172918736716], time: 125.83
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: 0.7373139577134643, agent episode reward: [-13.796648472209778, 7.266981214961622, 7.266981214961622], time: 104.898
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: 0.8864268636193874, agent episode reward: [-13.78729140452819, 7.336859134073789, 7.336859134073789], time: 96.36
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: 1.4533350225631196, agent episode reward: [-14.064207468861298, 7.758771245712209, 7.758771245712209], time: 98.291
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: 1.1449014217092122, agent episode reward: [-13.275494851777298, 7.210198136743254, 7.210198136743254], time: 95.627
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: 1.5939911496751697, agent episode reward: [-13.323744069063475, 7.458867609369322, 7.458867609369322], time: 97.408
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: 1.3484420378284268, agent episode reward: [-13.311747362265175, 7.330094700046799, 7.330094700046799], time: 96.235
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: 1.8394338443146823, agent episode reward: [-13.235034046414203, 7.537233945364442, 7.537233945364442], time: 97.005
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: 1.7595266836804517, agent episode reward: [-14.15021906832584, 7.954872876003146, 7.954872876003146], time: 95.721
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: 1.5301267177560858, agent episode reward: [-14.116167325269402, 7.823147021512745, 7.823147021512745], time: 98.933
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: 1.5557139173846057, agent episode reward: [-13.693714874446282, 7.6247143959154435, 7.6247143959154435], time: 95.406
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: 1.2385876359691856, agent episode reward: [-14.013056219893468, 7.6258219279313275, 7.6258219279313275], time: 96.757
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: 1.7157823830429475, agent episode reward: [-14.376995667765746, 8.046389025404347, 8.046389025404347], time: 98.039
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: 1.4256267177055388, agent episode reward: [-14.242148214774225, 7.833887466239881, 7.833887466239881], time: 98.265
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: 1.0425145033196639, agent episode reward: [-14.357587887254116, 7.700051195286889, 7.700051195286889], time: 97.925
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: 1.288693486014546, agent episode reward: [-13.624353506969756, 7.456523496492151, 7.456523496492151], time: 96.563
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: 1.2757659309232345, agent episode reward: [-14.099698748810235, 7.687732339866735, 7.687732339866735], time: 98.231
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: 1.1624900492061179, agent episode reward: [-13.303413089180676, 7.232951569193397, 7.232951569193397], time: 97.159
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: 1.6839098558675074, agent episode reward: [-13.496668302323407, 7.590289079095457, 7.590289079095457], time: 99.011
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: 1.549353570076099, agent episode reward: [-13.559396195224748, 7.554374882650423, 7.554374882650423], time: 98.298
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: 1.2361190303930207, agent episode reward: [-14.469525383995993, 7.8528222071945075, 7.8528222071945075], time: 96.855
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: 2.1977033287790064, agent episode reward: [-14.14945380764404, 8.173578568211523, 8.173578568211523], time: 97.575
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: 1.9603423350265485, agent episode reward: [-13.40602214357887, 7.683182239302709, 7.683182239302709], time: 97.784
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: 2.1307747165855653, agent episode reward: [-14.168439836475986, 8.149607276530777, 8.149607276530777], time: 97.972
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: 2.650629643822075, agent episode reward: [-14.361950452505159, 8.506290048163617, 8.506290048163617], time: 97.109
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: 2.8965660090777945, agent episode reward: [-13.753345402340987, 8.324955705709392, 8.324955705709392], time: 97.722
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: 3.0398892371504918, agent episode reward: [-13.453922232768722, 8.246905734959606, 8.246905734959606], time: 96.97
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: 2.6114634932335496, agent episode reward: [-14.076834261936341, 8.344148877584944, 8.344148877584944], time: 99.152
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: 3.65199102157889, agent episode reward: [-13.617653858949646, 8.634822440264266, 8.634822440264266], time: 98.85
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: 3.5894577730423154, agent episode reward: [-13.477754839834034, 8.533606306438175, 8.533606306438175], time: 95.742
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: 3.2087006597954812, agent episode reward: [-13.757701906291821, 8.48320128304365, 8.48320128304365], time: 95.326
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: 3.0086512240618335, agent episode reward: [-13.396873595312762, 8.202762409687299, 8.202762409687299], time: 97.306
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: 3.3815458237191796, agent episode reward: [-12.917244819991248, 8.149395321855215, 8.149395321855215], time: 95.385
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: 3.4629564057062225, agent episode reward: [-12.77630644189459, 8.119631423800406, 8.119631423800406], time: 98.629
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: 3.7684463554165895, agent episode reward: [-12.728748913798382, 8.248597634607485, 8.248597634607485], time: 98.531
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: 3.4147456029487393, agent episode reward: [-13.203653236453626, 8.30919941970118, 8.30919941970118], time: 96.238
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: 4.141692329883043, agent episode reward: [-12.782303839907712, 8.461998084895377, 8.461998084895377], time: 96.376
...Finished total of 100001 episodes.
