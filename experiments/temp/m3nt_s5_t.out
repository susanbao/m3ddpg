0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.690708403991223, agent episode reward: [-1.0748211921393926, -26.61588721185183], time: 10.241
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.53970780552852, agent episode reward: [-4.561224829442395, -17.978482976086124], time: 23.715
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.864757598024863, agent episode reward: [-4.784141832590396, -9.080615765434466], time: 24.332
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.77762210588128, agent episode reward: [-4.200831665348941, -8.576790440532339], time: 24.188
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.09797710471526, agent episode reward: [-3.4430872940118524, -8.65488981070341], time: 24.416
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.245796694003365, agent episode reward: [-3.779283522113693, -9.466513171889673], time: 23.709
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.853475554824008, agent episode reward: [-3.570670075149167, -10.28280547967484], time: 24.303
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -13.726585538999373, agent episode reward: [-3.5108607596660666, -10.215724779333307], time: 24.53
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.236651247690716, agent episode reward: [-3.7652804263737725, -10.471370821316944], time: 24.291
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.24647688800382, agent episode reward: [-3.7488454341939197, -10.4976314538099], time: 24.723
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.243134789064186, agent episode reward: [-3.574373116558378, -10.66876167250581], time: 23.929
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -13.99786035549554, agent episode reward: [-3.5885283354467274, -10.409332020048815], time: 24.839
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.27720933650368, agent episode reward: [-3.7735747071415116, -10.503634629362168], time: 24.555
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.1757896866957, agent episode reward: [-3.661186493115003, -10.5146031935807], time: 24.642
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.622147551167847, agent episode reward: [-3.944749487137093, -10.677398064030756], time: 24.112
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.402280561456454, agent episode reward: [-3.604086740069942, -10.798193821386512], time: 25.075
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.25545476910646, agent episode reward: [-3.7859414443236874, -10.469513324782772], time: 24.034
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.19626504543031, agent episode reward: [-3.5629538374561798, -10.63331120797413], time: 24.411
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.56624100104054, agent episode reward: [-3.72841590766715, -10.837825093373391], time: 23.834
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.322892414181851, agent episode reward: [-3.557767457254305, -10.765124956927545], time: 25.08
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.231466943175018, agent episode reward: [-3.2987713457930012, -10.932695597382018], time: 24.578
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.75530263182077, agent episode reward: [-4.1391335503621, -10.616169081458672], time: 24.509
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.479106200713964, agent episode reward: [-3.4126475023351857, -11.06645869837878], time: 24.957
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.882270318135292, agent episode reward: [-3.752802853008258, -11.129467465127036], time: 24.222
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.418222388929793, agent episode reward: [-3.574005777376597, -10.844216611553197], time: 24.916
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -14.522642010310257, agent episode reward: [-3.595966542836169, -10.926675467474087], time: 24.504
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -14.55689339122517, agent episode reward: [-3.7442725479547634, -10.812620843270407], time: 24.348
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -14.427590720867432, agent episode reward: [-3.7137443630068527, -10.713846357860579], time: 25.298
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.3571902369542, agent episode reward: [-3.566419991522899, -10.7907702454313], time: 24.34
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.296780296849501, agent episode reward: [-3.614636158052901, -10.6821441387966], time: 24.496
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -14.425913069551125, agent episode reward: [-3.3390546549353037, -11.08685841461582], time: 24.246
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.066792271596709, agent episode reward: [-3.9880132563332005, -11.078779015263509], time: 24.966
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -14.701880972699342, agent episode reward: [-3.4660454818335067, -11.235835490865835], time: 25.164
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -14.814912468705847, agent episode reward: [-3.6730992945470216, -11.141813174158822], time: 25.345
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.053734802500333, agent episode reward: [-3.7584993505065287, -11.295235451993806], time: 24.949
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -14.38288666669305, agent episode reward: [-3.1845289907209926, -11.198357675972057], time: 24.999
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -14.701545424882719, agent episode reward: [-3.5736854474785082, -11.12785997740421], time: 23.491
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.003544026414325, agent episode reward: [-3.774935820537544, -11.228608205876782], time: 25.153
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -14.803996772640032, agent episode reward: [-3.6412578058327227, -11.162738966807309], time: 25.166
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -14.695054984852511, agent episode reward: [-3.700314100897692, -10.994740883954819], time: 25.407
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -14.67635161567738, agent episode reward: [-3.526671604730893, -11.14968001094649], time: 24.798
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -14.800462418074327, agent episode reward: [-3.6540322532333898, -11.146430164840938], time: 25.02
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.123184687931843, agent episode reward: [-3.891496875777263, -11.231687812154581], time: 25.016
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.12349302557488, agent episode reward: [-3.4425968525646677, -11.680896173010213], time: 23.617
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -14.980821748512708, agent episode reward: [-3.2870301200880467, -11.69379162842466], time: 25.542
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.255772393238173, agent episode reward: [-3.047432634615245, -12.208339758622929], time: 25.221
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.21094301902914, agent episode reward: [-3.177503644145399, -12.03343937488374], time: 25.585
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.426141999059096, agent episode reward: [-3.734204578552255, -11.691937420506843], time: 25.292
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.19366087731831, agent episode reward: [-3.474259939367301, -11.71940093795101], time: 24.916
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.290161453349494, agent episode reward: [-3.596534289528864, -11.693627163820631], time: 25.072
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.154568477931468, agent episode reward: [-3.425644153002072, -11.728924324929396], time: 25.446
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.035329399309209, agent episode reward: [-3.0318728881088597, -12.00345651120035], time: 25.026
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.178322563031383, agent episode reward: [-3.282255269437084, -11.896067293594298], time: 24.955
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.53341730442298, agent episode reward: [-3.635315366322433, -11.898101938100547], time: 24.915
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.379487012680958, agent episode reward: [-3.348393005845759, -12.031094006835199], time: 25.399
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.23013974634815, agent episode reward: [-3.6364369470800564, -11.593702799268094], time: 25.731
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.654717601985881, agent episode reward: [-3.3646186063798686, -12.290098995606012], time: 25.888
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.424059159997725, agent episode reward: [-3.5380987746041415, -11.885960385393584], time: 24.636
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.464007669689968, agent episode reward: [-3.6158986110743556, -11.848109058615613], time: 25.58
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.192350773220092, agent episode reward: [-3.2462438512227276, -11.946106921997366], time: 26.214
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.3452062797913, agent episode reward: [-3.5507764137336935, -11.794429866057605], time: 25.748
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.39929693360232, agent episode reward: [-3.5971919384437823, -11.802104995158537], time: 25.526
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.292800989561254, agent episode reward: [-3.5616072058114097, -11.731193783749843], time: 25.213
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.60392808713653, agent episode reward: [-4.216265637898558, -11.38766244923797], time: 24.371
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.616164596739898, agent episode reward: [-4.03456421643363, -11.58160038030627], time: 24.936
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.522465365978904, agent episode reward: [-3.8519737515430252, -11.670491614435878], time: 24.227
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.242962574159124, agent episode reward: [-3.866001692200214, -11.376960881958912], time: 25.89
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.324122620931742, agent episode reward: [-3.582507704921657, -11.741614916010088], time: 25.939
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.581547486108422, agent episode reward: [-4.062562949470857, -11.518984536637566], time: 25.377
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.12442918769851, agent episode reward: [-3.5562180056280583, -11.568211182070453], time: 25.203
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.273222183297799, agent episode reward: [-3.7380570469087493, -11.535165136389049], time: 25.393
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.703140384251409, agent episode reward: [-4.092345633352557, -11.610794750898853], time: 25.438
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -14.894078989430987, agent episode reward: [-3.34544081915261, -11.548638170278377], time: 25.205
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -14.819425234437448, agent episode reward: [-3.4407041648316876, -11.37872106960576], time: 25.851
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.39712212330674, agent episode reward: [-3.8726028485521766, -11.524519274754564], time: 25.897
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -14.936044461585546, agent episode reward: [-3.3218007899182584, -11.614243671667285], time: 26.247
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -14.860986861899685, agent episode reward: [-3.732199731735779, -11.128787130163905], time: 25.631
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -14.961214119599227, agent episode reward: [-3.6043584419558146, -11.356855677643411], time: 25.73
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.207127940768256, agent episode reward: [-3.7204724513670206, -11.486655489401235], time: 26.053
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -14.982189777272252, agent episode reward: [-3.835171603902265, -11.147018173369986], time: 24.89
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -14.931775960889677, agent episode reward: [-3.832466609121555, -11.099309351768122], time: 25.393
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.196011514853122, agent episode reward: [-4.020572035930021, -11.175439478923101], time: 25.125
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.184283238894091, agent episode reward: [-3.9791075833696623, -11.205175655524428], time: 24.75
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.15886551668861, agent episode reward: [-3.5324990169205686, -11.626366499768043], time: 25.057
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.069429352452012, agent episode reward: [-3.661936545813164, -11.407492806638851], time: 24.785
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -15.08153652544597, agent episode reward: [-3.8702248922555853, -11.211311633190384], time: 25.406
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -14.744214818002959, agent episode reward: [-3.641472959742106, -11.10274185826085], time: 25.23
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.540182881927587, agent episode reward: [-4.166079776753908, -11.37410310517368], time: 24.36
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.587861996078576, agent episode reward: [-4.192399780595377, -11.3954622154832], time: 24.836
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.3118878339861, agent episode reward: [-3.71227295661235, -11.59961487737375], time: 24.798
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.475528468986697, agent episode reward: [-3.8224166276782636, -11.653111841308434], time: 24.92
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.106775891067496, agent episode reward: [-3.790677361609117, -11.31609852945838], time: 25.135
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.332153091687259, agent episode reward: [-3.9235476423040394, -11.408605449383218], time: 24.628
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.811072030564683, agent episode reward: [-4.277926406816427, -11.533145623748256], time: 24.848
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.526175958930729, agent episode reward: [-4.149530696599626, -11.376645262331103], time: 24.814
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.17371553027715, agent episode reward: [-3.751100948879844, -11.422614581397305], time: 25.055
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.24885463796557, agent episode reward: [-3.795497029096074, -11.453357608869494], time: 25.078
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.177012198492722, agent episode reward: [-3.833854957313617, -11.343157241179107], time: 24.881
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.338410367125226, agent episode reward: [-4.108087218522683, -11.230323148602544], time: 25.265
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.199459016683807, agent episode reward: [-3.786683641954425, -11.412775374729382], time: 24.899
...Finished total of 100001 episodes.
