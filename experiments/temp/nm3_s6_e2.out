0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -28.431998948114767, agent episode reward: [0.7727304144719519, -29.20472936258672], time: 54.918
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -23.53655470750612, agent episode reward: [-7.736941602699313, -15.79961310480681], time: 81.67
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -12.456869017335405, agent episode reward: [-5.168316596591281, -7.288552420744125], time: 78.966
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -10.355029027426484, agent episode reward: [-3.272040202028178, -7.082988825398305], time: 80.425
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -9.434862755664584, agent episode reward: [-2.6302274772235825, -6.804635278440999], time: 82.403
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -9.238495671398748, agent episode reward: [-2.5485749394832427, -6.689920731915505], time: 81.521
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -9.211056913301398, agent episode reward: [-2.1458454535537843, -7.065211459747613], time: 81.967
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -8.842682910754485, agent episode reward: [-2.1829604061528984, -6.659722504601588], time: 81.764
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -9.186739064684279, agent episode reward: [-2.4220685641494257, -6.7646705005348515], time: 80.51
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -9.027890804097808, agent episode reward: [-1.9745647925207415, -7.053326011577067], time: 81.985
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -9.223098876526544, agent episode reward: [-2.4149281704419296, -6.808170706084614], time: 81.822
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -9.085555733370985, agent episode reward: [-1.8411494100127848, -7.2444063233581995], time: 81.432
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -8.95386510061108, agent episode reward: [-2.083351038779191, -6.87051406183189], time: 80.178
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -8.86955516362732, agent episode reward: [-1.8111394955052398, -7.05841566812208], time: 79.649
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -8.91583290254728, agent episode reward: [-1.6939948703783614, -7.221838032168918], time: 82.056
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -8.845761631307498, agent episode reward: [-1.3293909777923223, -7.516370653515174], time: 79.638
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -8.902935542491724, agent episode reward: [-1.374890874213741, -7.528044668277982], time: 80.209
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -9.026353096242376, agent episode reward: [-1.2578613536140315, -7.768491742628345], time: 79.483
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -9.058786323755685, agent episode reward: [-1.1175844429091613, -7.941201880846526], time: 79.3
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -9.11421971883278, agent episode reward: [-1.5046347154258772, -7.6095850034069015], time: 80.385
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -9.033419121422543, agent episode reward: [-1.710891675162994, -7.322527446259548], time: 79.294
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -9.032225635587242, agent episode reward: [-2.1054955465976883, -6.926730088989553], time: 79.971
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -9.185552216093972, agent episode reward: [-2.296538664152685, -6.889013551941286], time: 82.812
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -8.517230584263082, agent episode reward: [-2.03588642504551, -6.481344159217572], time: 81.001
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -8.951200186407503, agent episode reward: [-2.021519211899554, -6.929680974507949], time: 79.702
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -8.868479922466133, agent episode reward: [-1.8783593785959105, -6.990120543870224], time: 81.011
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -8.795515243046127, agent episode reward: [-1.6488017318530004, -7.146713511193128], time: 79.605
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -8.410781464183266, agent episode reward: [-1.2274909556890914, -7.183290508494174], time: 80.0
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -8.811877012283553, agent episode reward: [-1.5270229647116798, -7.284854047571871], time: 80.116
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -8.507519483156177, agent episode reward: [-1.3289254965520538, -7.178593986604124], time: 82.026
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -8.764851357161627, agent episode reward: [-1.9798955086193541, -6.784955848542272], time: 81.072
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -8.673093542253064, agent episode reward: [-1.5145673671689155, -7.158526175084147], time: 81.403
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -8.740214258517685, agent episode reward: [-1.8277424345998408, -6.912471823917847], time: 81.129
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -8.908897456798053, agent episode reward: [-1.494337132411526, -7.414560324386528], time: 80.158
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -8.79292837930836, agent episode reward: [-0.7597794995144145, -8.033148879793949], time: 81.107
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -8.600483737117356, agent episode reward: [-0.34111869102754294, -8.259365046089812], time: 81.444
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -8.864045908591173, agent episode reward: [-0.4062374683652389, -8.457808440225934], time: 79.573
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -9.067445313918656, agent episode reward: [-0.15075952136650453, -8.916685792552151], time: 79.39
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -9.316849118180155, agent episode reward: [0.11131598255502663, -9.428165100735182], time: 80.511
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -9.220063239336152, agent episode reward: [-1.0251370391178578, -8.194926200218294], time: 81.204
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -8.959644920727726, agent episode reward: [-0.6367147225120168, -8.322930198215708], time: 81.948
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -9.151363478517368, agent episode reward: [-0.3889837945818717, -8.762379683935494], time: 80.124
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -9.192613760730154, agent episode reward: [-0.3313919283360683, -8.861221832394087], time: 81.337
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -8.853328445601932, agent episode reward: [0.6000057217284319, -9.453334167330366], time: 81.515
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -9.649116521350683, agent episode reward: [0.7000178554882911, -10.349134376838974], time: 81.26
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -9.328775666259382, agent episode reward: [0.5338982287450321, -9.862673895004413], time: 80.744
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -8.710895192974807, agent episode reward: [0.36234516358929253, -9.0732403565641], time: 80.339
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -8.597720083225358, agent episode reward: [0.20048873319413008, -8.798208816419487], time: 80.12
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -8.55300458635627, agent episode reward: [-1.735845396273638, -6.817159190082632], time: 79.926
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -8.677810212276436, agent episode reward: [-1.2657142240612098, -7.412095988215228], time: 81.601
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -8.821604088225838, agent episode reward: [-1.085077515896981, -7.736526572328858], time: 82.704
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -8.903814618898851, agent episode reward: [-0.5582217871185919, -8.345592831780259], time: 80.24
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -8.95401442082678, agent episode reward: [-1.0249798049734669, -7.929034615853314], time: 81.092
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -8.554559127645154, agent episode reward: [-0.25550301838309564, -8.29905610926206], time: 80.732
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -8.616602658678481, agent episode reward: [0.1558184726768946, -8.772421131355376], time: 81.437
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -8.471190937156347, agent episode reward: [-1.05869743619242, -7.412493500963929], time: 83.474
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -8.946808419169495, agent episode reward: [-1.5980989907608576, -7.348709428408637], time: 80.702
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -8.633463091961474, agent episode reward: [-1.1837822712257273, -7.449680820735746], time: 80.514
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -8.53929406151334, agent episode reward: [-0.4642683076881752, -8.075025753825164], time: 81.79
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -8.765960161407788, agent episode reward: [-0.05993872627560914, -8.706021435132177], time: 79.781
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -8.311244570008883, agent episode reward: [0.5361052234264375, -8.847349793435322], time: 80.94
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -8.752051806169366, agent episode reward: [-0.34458968052241323, -8.407462125646953], time: 82.858
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -8.6723208060533, agent episode reward: [-0.10880744728825344, -8.563513358765045], time: 82.376
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -8.642049821558418, agent episode reward: [0.3659426329623649, -9.007992454520782], time: 81.888
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -8.648892959194017, agent episode reward: [0.013976667768445162, -8.662869626962461], time: 82.703
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -8.84733581577456, agent episode reward: [0.26762867427740883, -9.114964490051971], time: 82.097
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -8.744570312012058, agent episode reward: [0.6482774558889184, -9.392847767900978], time: 82.286
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -8.836671825704107, agent episode reward: [0.8968874340918026, -9.733559259795907], time: 82.662
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -8.826473311170655, agent episode reward: [0.9693792961092422, -9.7958526072799], time: 82.093
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -8.77626213739442, agent episode reward: [0.4551709819488914, -9.231433119343313], time: 82.431
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -8.500525875371887, agent episode reward: [0.42965903156187957, -8.930184906933762], time: 82.198
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -8.317489829024076, agent episode reward: [0.6868307897897306, -9.004320618813807], time: 81.961
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -8.524904734601426, agent episode reward: [1.1026256418584321, -9.627530376459859], time: 82.535
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -8.69524329670129, agent episode reward: [1.4968564385286267, -10.192099735229917], time: 80.949
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -8.73582508971916, agent episode reward: [1.9643883585048165, -10.700213448223979], time: 82.911
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -8.759427232106946, agent episode reward: [1.628172340459655, -10.387599572566602], time: 82.076
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -8.479816103433885, agent episode reward: [1.150114603312917, -9.629930706746803], time: 81.701
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -8.545948796969839, agent episode reward: [0.35242478039546166, -8.898373577365302], time: 81.509
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -8.594469043616652, agent episode reward: [-0.5093161760993018, -8.085152867517351], time: 81.913
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -8.517474451211267, agent episode reward: [-0.4251918675055599, -8.092282583705707], time: 82.845
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -8.633201063324087, agent episode reward: [0.06613053422594271, -8.699331597550026], time: 81.806
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -8.747827633725112, agent episode reward: [-0.15005516082718587, -8.597772472897926], time: 82.693
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -8.240843506945447, agent episode reward: [0.10665157092428683, -8.347495077869734], time: 83.208
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -8.646011735316527, agent episode reward: [0.08820719863934443, -8.734218933955871], time: 82.427
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -8.653954726910703, agent episode reward: [0.2298547921276766, -8.883809519038378], time: 81.974
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -8.480889032259702, agent episode reward: [0.7405174938455454, -9.221406526105245], time: 81.636
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -8.227940507635154, agent episode reward: [0.5797993492118114, -8.807739856846965], time: 81.679
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -8.639713592531713, agent episode reward: [0.101084947418377, -8.740798539950088], time: 82.382
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -8.821155080355908, agent episode reward: [0.02809531563259226, -8.8492503959885], time: 82.72
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -8.63063399693906, agent episode reward: [0.16949250720578699, -8.800126504144849], time: 82.316
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -8.817913596721475, agent episode reward: [-0.04288138808234494, -8.775032208639132], time: 81.997
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -8.93619930352486, agent episode reward: [0.10094430226106695, -9.037143605785928], time: 83.393
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -9.128389560486049, agent episode reward: [-0.26029675407698344, -8.868092806409066], time: 81.797
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -9.180147661622854, agent episode reward: [-0.698443947584527, -8.481703714038327], time: 82.6
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -9.074524395406858, agent episode reward: [-0.8155808209554597, -8.258943574451397], time: 82.462
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -8.943090994160427, agent episode reward: [-0.46374143609711477, -8.479349558063312], time: 83.2
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -8.851345989425415, agent episode reward: [-0.07771004479385989, -8.773635944631556], time: 82.291
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -9.111430813963793, agent episode reward: [-0.07739380231582021, -9.034037011647971], time: 81.654
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -9.177578243099662, agent episode reward: [-0.1379398349875682, -9.039638408112094], time: 82.096
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -8.994232826679735, agent episode reward: [-0.14878644627083032, -8.845446380408907], time: 73.295
...Finished total of 100001 episodes.
