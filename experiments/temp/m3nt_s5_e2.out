0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -23.98830157551426, agent episode reward: [-24.159025731159904, 0.08536207782282464, 0.08536207782282464], time: 50.756
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -17.505197330012567, agent episode reward: [-17.86155756534395, 0.17818011766569425, 0.17818011766569425], time: 74.802
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -16.46142560231209, agent episode reward: [-16.77471794577303, 0.15664617173046902, 0.15664617173046902], time: 73.699
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -16.674689230612838, agent episode reward: [-16.513869565896076, -0.08040983235838146, -0.08040983235838146], time: 73.796
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -16.832541565009908, agent episode reward: [-17.10711475530571, 0.13728659514790156, 0.13728659514790156], time: 74.089
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -15.756641735313785, agent episode reward: [-15.686856129906454, -0.03489280270366575, -0.03489280270366575], time: 73.379
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -16.510967189568408, agent episode reward: [-16.39996902018658, -0.05549908469091264, -0.05549908469091264], time: 72.701
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -16.761129627908943, agent episode reward: [-16.362840595126123, -0.19914451639140884, -0.19914451639140884], time: 75.44
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -16.384776491917698, agent episode reward: [-16.467381675232726, 0.0413025916575135, 0.0413025916575135], time: 75.394
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -16.14760510179934, agent episode reward: [-16.748485891326265, 0.3004403947634625, 0.3004403947634625], time: 73.804
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -16.926809592864583, agent episode reward: [-16.43048553256791, -0.24816203014833588, -0.24816203014833588], time: 72.442
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -16.683217184913374, agent episode reward: [-16.706011788800435, 0.011397301943530789, 0.011397301943530789], time: 73.907
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -16.813746983554644, agent episode reward: [-16.435056161883594, -0.18934541083552509, -0.18934541083552509], time: 74.333
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -16.580656769466636, agent episode reward: [-16.55430819053403, -0.013174289466303122, -0.013174289466303122], time: 76.11
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -16.74389732517331, agent episode reward: [-16.80074614986598, 0.02842441234633347, 0.02842441234633347], time: 74.814
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -16.50312928762648, agent episode reward: [-16.706108570944053, 0.10148964165878502, 0.10148964165878502], time: 74.921
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -16.863153393004943, agent episode reward: [-16.92907534258641, 0.032960974790733504, 0.032960974790733504], time: 73.667
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -16.598898353628112, agent episode reward: [-16.668771332309422, 0.03493648934065455, 0.03493648934065455], time: 73.822
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -16.54809890339325, agent episode reward: [-16.83266289822105, 0.14228199741390052, 0.14228199741390052], time: 74.248
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -16.567916472715797, agent episode reward: [-16.810469998860068, 0.1212767630721376, 0.1212767630721376], time: 73.951
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -16.513360519116507, agent episode reward: [-16.59315932563103, 0.039899403257261754, 0.039899403257261754], time: 73.686
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -17.3923393611715, agent episode reward: [-17.076265072186498, -0.15803714449249948, -0.15803714449249948], time: 74.994
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -16.642886596534584, agent episode reward: [-16.572147593452673, -0.035369501540956395, -0.035369501540956395], time: 73.788
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -16.70526713391776, agent episode reward: [-16.67454105710476, -0.015363038406499836, -0.015363038406499836], time: 75.579
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -16.55080094081843, agent episode reward: [-16.870110848880817, 0.1596549540311937, 0.1596549540311937], time: 76.231
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -16.88843103681517, agent episode reward: [-16.35275868360278, -0.26783617660619635, -0.26783617660619635], time: 76.055
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -16.85798241171914, agent episode reward: [-16.879757510284197, 0.010887549282530273, 0.010887549282530273], time: 75.254
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -16.77690345102666, agent episode reward: [-16.383326114595054, -0.196788668215804, -0.196788668215804], time: 76.135
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -16.563500625210253, agent episode reward: [-16.618396116529148, 0.02744774565944624, 0.02744774565944624], time: 75.841
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -16.592133890080774, agent episode reward: [-16.694411595200897, 0.05113885256005978, 0.05113885256005978], time: 77.372
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -16.953895387730824, agent episode reward: [-16.899616266772195, -0.02713956047931262, -0.02713956047931262], time: 76.185
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -16.77869795001361, agent episode reward: [-17.090720455184993, 0.15601125258569212, 0.15601125258569212], time: 76.42
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.994039513540503, agent episode reward: [-16.32777155981149, 0.16686602313549498, 0.16686602313549498], time: 76.545
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -16.672047157547183, agent episode reward: [-16.801248669044778, 0.06460075574879812, 0.06460075574879812], time: 75.519
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -16.24568172540759, agent episode reward: [-16.923937219126078, 0.33912774685924363, 0.33912774685924363], time: 76.137
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -16.977798414517316, agent episode reward: [-16.663694556125243, -0.15705192919603775, -0.15705192919603775], time: 74.439
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -16.623518899488374, agent episode reward: [-16.505990894045944, -0.05876400272121735, -0.05876400272121735], time: 76.553
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -16.617110476458983, agent episode reward: [-17.023776232986958, 0.20333287826398688, 0.20333287826398688], time: 77.357
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -16.92548113460474, agent episode reward: [-16.79460908473251, -0.06543602493611612, -0.06543602493611612], time: 75.293
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -16.570257769640275, agent episode reward: [-16.70265693229999, 0.06619958132985647, 0.06619958132985647], time: 76.844
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -16.878674967772923, agent episode reward: [-17.15109523091289, 0.13621013156998457, 0.13621013156998457], time: 77.425
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -16.569049739300215, agent episode reward: [-16.853135133891787, 0.14204269729578295, 0.14204269729578295], time: 76.79
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -16.936124876784884, agent episode reward: [-16.691829879750024, -0.12214749851742937, -0.12214749851742937], time: 78.647
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -17.124239273753695, agent episode reward: [-16.70576267206498, -0.2092383008443571, -0.2092383008443571], time: 76.203
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -16.457066415599325, agent episode reward: [-16.606623056145548, 0.07477832027311339, 0.07477832027311339], time: 76.967
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -16.6669037207707, agent episode reward: [-17.241151717065907, 0.2871239981476037, 0.2871239981476037], time: 77.158
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.48073419655238, agent episode reward: [-16.857091382166516, 0.18817859280706972, 0.18817859280706972], time: 79.143
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.415424360545146, agent episode reward: [-16.46506634244544, 0.024820990950146216, 0.024820990950146216], time: 77.979
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -16.91638714086613, agent episode reward: [-16.75951300129773, -0.07843706978420187, -0.07843706978420187], time: 78.464
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -16.590542374522986, agent episode reward: [-16.817097540716492, 0.11327758309675254, 0.11327758309675254], time: 75.943
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.73364452743036, agent episode reward: [-16.88600639449773, 0.07618093353368391, 0.07618093353368391], time: 77.197
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -16.411514480903, agent episode reward: [-16.481092880444468, 0.034789199770730334, 0.034789199770730334], time: 79.262
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -16.71347189833194, agent episode reward: [-16.485810701675693, -0.11383059832812288, -0.11383059832812288], time: 75.674
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -16.640218859771455, agent episode reward: [-16.77366487177453, 0.0667230060015354, 0.0667230060015354], time: 76.06
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.74851448462735, agent episode reward: [-16.494246205081996, -0.12713413977267674, -0.12713413977267674], time: 78.979
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -16.405643040927657, agent episode reward: [-16.71634936268897, 0.15535316088065557, 0.15535316088065557], time: 77.293
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -16.455200140400848, agent episode reward: [-16.84008006649369, 0.19243996304641992, 0.19243996304641992], time: 77.186
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.683313413757947, agent episode reward: [-16.864926778166648, 0.09080668220435034, 0.09080668220435034], time: 78.461
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.570764666657862, agent episode reward: [-16.70098543833911, 0.06511038584062537, 0.06511038584062537], time: 76.896
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.896859143497643, agent episode reward: [-16.7003307232295, -0.09826421013407374, -0.09826421013407374], time: 79.082
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -16.457243684985873, agent episode reward: [-16.68787431815341, 0.11531531658376667, 0.11531531658376667], time: 75.33
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -17.11929775072422, agent episode reward: [-16.60799600279319, -0.2556508739655142, -0.2556508739655142], time: 76.4
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.201211942112515, agent episode reward: [-16.84563549337129, 0.32221177562938746, 0.32221177562938746], time: 77.507
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.477273024405708, agent episode reward: [-16.673704007239323, 0.09821549141680515, 0.09821549141680515], time: 74.885
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.42153326440945, agent episode reward: [-16.749851925270097, 0.1641593304303219, 0.1641593304303219], time: 72.082
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.918097957042892, agent episode reward: [-16.661869922499726, -0.1281140172715844, -0.1281140172715844], time: 71.243
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.77533715621395, agent episode reward: [-16.687895964524685, -0.04372059584463171, -0.04372059584463171], time: 70.317
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.958100640902806, agent episode reward: [-16.664914025561316, -0.14659330767074238, -0.14659330767074238], time: 71.652
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.72697719475221, agent episode reward: [-16.70755392075599, -0.009711636998108645, -0.009711636998108645], time: 71.329
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -17.094053115769313, agent episode reward: [-16.58341844755971, -0.2553173341048042, -0.2553173341048042], time: 69.056
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.510761676681373, agent episode reward: [-16.73310602425132, 0.11117217378497286, 0.11117217378497286], time: 69.732
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.782055590772934, agent episode reward: [-16.63573259034856, -0.07316150021218912, -0.07316150021218912], time: 71.376
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -16.658495118483632, agent episode reward: [-16.699727224729898, 0.020616053123133398, 0.020616053123133398], time: 70.945
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.69797167055746, agent episode reward: [-16.705678922649746, 0.0038536260461412937, 0.0038536260461412937], time: 68.874
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.927664365422014, agent episode reward: [-16.65452459492236, -0.13656988524982455, -0.13656988524982455], time: 71.654
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.48280265198223, agent episode reward: [-16.85698380280176, 0.18709057540976667, 0.18709057540976667], time: 71.134
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.757619988834247, agent episode reward: [-16.765926518386575, 0.00415326477616491, 0.00415326477616491], time: 69.844
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -17.26836875992592, agent episode reward: [-16.493642095415773, -0.3873633322550741, -0.3873633322550741], time: 71.538
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.72853756583626, agent episode reward: [-16.746074547005303, 0.008768490584523959, 0.008768490584523959], time: 71.048
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.402031313799743, agent episode reward: [-16.661788792946002, 0.12987873957312854, 0.12987873957312854], time: 71.747
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.839269745330835, agent episode reward: [-16.62517783997307, -0.10704595267888391, -0.10704595267888391], time: 70.809
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.364782329601642, agent episode reward: [-16.714146158310736, 0.17468191435454747, 0.17468191435454747], time: 72.248
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.547974937388688, agent episode reward: [-16.725019050449664, 0.08852205653048728, 0.08852205653048728], time: 69.545
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -17.00678526819698, agent episode reward: [-16.54934367059452, -0.22872079880123167, -0.22872079880123167], time: 71.736
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.717847812248813, agent episode reward: [-16.814099517801022, 0.04812585277610529, 0.04812585277610529], time: 70.908
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.60362039948425, agent episode reward: [-16.676902921914277, 0.036641261215010545, 0.036641261215010545], time: 67.97
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.78969789741621, agent episode reward: [-16.617806513833248, -0.0859456917914795, -0.0859456917914795], time: 71.146
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.627930530795414, agent episode reward: [-16.702834202741766, 0.037451835973176424, 0.037451835973176424], time: 69.062
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.157300633302132, agent episode reward: [-16.927048480829722, 0.3848739237637946, 0.3848739237637946], time: 71.685
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.728441730974197, agent episode reward: [-16.61498890167055, -0.05672641465182473, -0.05672641465182473], time: 71.353
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.24440028326268, agent episode reward: [-16.71533768741248, 0.23546870207489723, 0.23546870207489723], time: 72.61
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -17.13308342836748, agent episode reward: [-16.654060522075522, -0.23951145314597982, -0.23951145314597982], time: 70.967
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.930606698469585, agent episode reward: [-16.608758332612286, -0.16092418292864927, -0.16092418292864927], time: 69.959
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.994163519363145, agent episode reward: [-16.734701928133557, -0.12973079561479559, -0.12973079561479559], time: 68.425
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -16.664489573797656, agent episode reward: [-16.767811565752798, 0.05166099597757165, 0.05166099597757165], time: 69.712
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.35254058178783, agent episode reward: [-16.76758489197333, 0.20752215509274874, 0.20752215509274874], time: 72.14
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.781844269253614, agent episode reward: [-16.686432504054036, -0.04770588259979114, -0.04770588259979114], time: 72.006
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -16.578183716791134, agent episode reward: [-16.87357266881922, 0.1476944760140454, 0.1476944760140454], time: 67.878
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.83272981961466, agent episode reward: [-16.707161717385826, -0.0627840511144154, -0.0627840511144154], time: 70.496
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.88357545782263, agent episode reward: [-16.547623181899798, -0.1679761379614157, -0.1679761379614157], time: 67.42
...Finished total of 100001 episodes.
