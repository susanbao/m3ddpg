0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -22.735472548866326, agent episode reward: [-38.81313504132402, 8.038831246228844, 8.038831246228844], time: 55.231
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -13.903970433104057, agent episode reward: [-22.690154338784737, 4.39309195284034, 4.39309195284034], time: 73.32
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 3.829371379871414, agent episode reward: [-9.22327886401788, 6.526325121944646, 6.526325121944646], time: 71.224
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 2.167040414196588, agent episode reward: [-8.888407606248785, 5.527724010222687, 5.527724010222687], time: 71.691
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 2.643308551116151, agent episode reward: [-9.411963013999369, 6.027635782557761, 6.027635782557761], time: 72.424
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 2.8592107352081064, agent episode reward: [-9.97681917239664, 6.4180149538023725, 6.4180149538023725], time: 72.279
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 2.4642429335265197, agent episode reward: [-10.269433246738961, 6.366838090132741, 6.366838090132741], time: 71.755
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 1.4302776434810776, agent episode reward: [-10.377718860339602, 5.903998251910341, 5.903998251910341], time: 71.4
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 0.9573131417850314, agent episode reward: [-10.264806677675583, 5.611059909730307, 5.611059909730307], time: 72.133
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 1.3126855138216615, agent episode reward: [-10.210551356063307, 5.761618434942484, 5.761618434942484], time: 72.484
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 0.9051668606304245, agent episode reward: [-10.14171691715477, 5.523441888892598, 5.523441888892598], time: 71.028
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 0.7713639467768051, agent episode reward: [-10.347011002378899, 5.559187474577852, 5.559187474577852], time: 71.901
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 0.6947283549535244, agent episode reward: [-11.02731705365539, 5.861022704304456, 5.861022704304456], time: 71.712
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 1.1789669929750417, agent episode reward: [-10.664017619153626, 5.921492306064334, 5.921492306064334], time: 72.335
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 0.6625000242691926, agent episode reward: [-10.570203916194442, 5.616351970231818, 5.616351970231818], time: 72.302
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 0.6064095014340242, agent episode reward: [-10.943714703927183, 5.775062102680605, 5.775062102680605], time: 72.683
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 0.5462072569765917, agent episode reward: [-11.510924801913486, 6.028566029445039, 6.028566029445039], time: 72.117
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 0.8496543208544911, agent episode reward: [-11.557406988067594, 6.203530654461043, 6.203530654461043], time: 71.526
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 1.3330091690467016, agent episode reward: [-12.002175017992705, 6.667592093519704, 6.667592093519704], time: 72.298
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 1.3137987270259541, agent episode reward: [-12.318618311128438, 6.816208519077196, 6.816208519077196], time: 72.486
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 1.6295762984504467, agent episode reward: [-12.591162850708983, 7.110369574579714, 7.110369574579714], time: 72.603
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 1.653557625553005, agent episode reward: [-13.311691545662503, 7.482624585607754, 7.482624585607754], time: 71.488
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 1.8470432626977078, agent episode reward: [-13.41756779635489, 7.632305529526298, 7.632305529526298], time: 72.39
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 1.488300237715073, agent episode reward: [-13.520283511782015, 7.504291874748544, 7.504291874748544], time: 72.299
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 1.8184979215554267, agent episode reward: [-12.875710591573686, 7.347104256564557, 7.347104256564557], time: 72.554
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 2.086433211587035, agent episode reward: [-13.204687418887108, 7.645560315237073, 7.645560315237073], time: 72.194
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 2.2937418478205864, agent episode reward: [-13.100094598353541, 7.6969182230870645, 7.6969182230870645], time: 72.481
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 2.0310273908861753, agent episode reward: [-13.365042619653098, 7.698035005269636, 7.698035005269636], time: 71.717
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 2.3400483212202263, agent episode reward: [-13.765448043556614, 8.05274818238842, 8.05274818238842], time: 71.87
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 2.0476692961205956, agent episode reward: [-13.770871767958894, 7.909270532039743, 7.909270532039743], time: 72.415
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 2.5592169467040384, agent episode reward: [-14.699690026661079, 8.629453486682559, 8.629453486682559], time: 72.485
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 1.780817682844055, agent episode reward: [-13.41514226145581, 7.597979972149932, 7.597979972149932], time: 72.674
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 2.056755111176152, agent episode reward: [-13.94596498214173, 8.00136004665894, 8.00136004665894], time: 72.272
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 1.95385995630837, agent episode reward: [-13.963491657305795, 7.958675806807082, 7.958675806807082], time: 72.034
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 0.7092863046382795, agent episode reward: [-14.055852448270402, 7.38256937645434, 7.38256937645434], time: 72.341
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 0.9544220138384916, agent episode reward: [-14.5471137703975, 7.750767892117996, 7.750767892117996], time: 70.906
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 0.8201696085915066, agent episode reward: [-14.510793077870986, 7.665481343231246, 7.665481343231246], time: 71.651
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 0.8878302722631165, agent episode reward: [-14.923396525283449, 7.905613398773283, 7.905613398773283], time: 70.152
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 1.14288157363932, agent episode reward: [-14.622241328080483, 7.882561450859901, 7.882561450859901], time: 70.614
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 1.3827080688586721, agent episode reward: [-14.3659848923516, 7.874346480605135, 7.874346480605135], time: 70.358
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 1.0601691255380068, agent episode reward: [-14.385171161570488, 7.7226701435542475, 7.7226701435542475], time: 70.732
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 1.1094592420985991, agent episode reward: [-13.309674599203838, 7.209566920651217, 7.209566920651217], time: 71.826
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 1.0235296762661852, agent episode reward: [-13.945116059549882, 7.484322867908033, 7.484322867908033], time: 72.015
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 0.8875891213599886, agent episode reward: [-13.815392159853076, 7.351490640606532, 7.351490640606532], time: 72.157
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 0.9054330759102024, agent episode reward: [-13.526591517485983, 7.216012296698093, 7.216012296698093], time: 70.725
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 1.1212006490340918, agent episode reward: [-14.252961067308314, 7.687080858171204, 7.687080858171204], time: 69.69
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 1.4374315908420303, agent episode reward: [-13.710027579385995, 7.5737295851140125, 7.5737295851140125], time: 68.488
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 1.5936499105274071, agent episode reward: [-13.24464580250037, 7.419147856513888, 7.419147856513888], time: 71.711
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 1.7460805672148438, agent episode reward: [-13.916857824836981, 7.831469196025913, 7.831469196025913], time: 71.797
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 2.425815190858569, agent episode reward: [-13.223340051386275, 7.824577621122421, 7.824577621122421], time: 70.207
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 2.6087979151662797, agent episode reward: [-13.713678399810632, 8.161238157488459, 8.161238157488459], time: 69.925
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 3.0256485757422085, agent episode reward: [-13.460969486148148, 8.243309030945179, 8.243309030945179], time: 70.982
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 3.015153594790789, agent episode reward: [-13.478853586185663, 8.247003590488227, 8.247003590488227], time: 70.677
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 3.5177937572872544, agent episode reward: [-13.65155970073148, 8.584676729009367, 8.584676729009367], time: 70.709
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 4.00910747450176, agent episode reward: [-13.485462968779004, 8.74728522164038, 8.74728522164038], time: 71.752
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 3.888707593871182, agent episode reward: [-14.27531049044014, 9.08200904215566, 9.08200904215566], time: 70.065
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 3.388936202303108, agent episode reward: [-13.200127735632107, 8.294531968967608, 8.294531968967608], time: 71.78
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 3.409962262498816, agent episode reward: [-13.643736820415905, 8.52684954145736, 8.52684954145736], time: 71.658
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 3.242114127587305, agent episode reward: [-13.58557082415558, 8.413842475871443, 8.413842475871443], time: 71.94
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 2.764087787232626, agent episode reward: [-13.835749815192383, 8.299918801212504, 8.299918801212504], time: 71.741
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: 2.896710130514844, agent episode reward: [-13.808768873449, 8.35273950198192, 8.35273950198192], time: 71.167
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: 2.323628910461336, agent episode reward: [-12.796614419508172, 7.560121664984753, 7.560121664984753], time: 72.098
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: 2.5805937519826005, agent episode reward: [-13.068853024112352, 7.824723388047476, 7.824723388047476], time: 70.714
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: 2.5816178233154217, agent episode reward: [-12.50884164377861, 7.545229733547016, 7.545229733547016], time: 71.547
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: 3.1039803672152675, agent episode reward: [-13.168922724671358, 8.136451545943313, 8.136451545943313], time: 72.11
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: 2.9591455480154107, agent episode reward: [-13.469186792717203, 8.214166170366306, 8.214166170366306], time: 71.01
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: 2.9787286413047585, agent episode reward: [-13.508857058726127, 8.243792850015444, 8.243792850015444], time: 71.036
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: 2.8857264602902752, agent episode reward: [-12.96375732056098, 7.924741890425628, 7.924741890425628], time: 71.338
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: 2.9349441751047847, agent episode reward: [-13.274360646969546, 8.104652411037165, 8.104652411037165], time: 70.886
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: 2.8178152131461647, agent episode reward: [-13.301737917485296, 8.059776565315731, 8.059776565315731], time: 71.41
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: 2.810327645766223, agent episode reward: [-13.079565437862726, 7.944946541814474, 7.944946541814474], time: 70.914
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: 2.5984608074842517, agent episode reward: [-13.445744556701397, 8.022102682092823, 8.022102682092823], time: 69.243
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: 2.4756383192889007, agent episode reward: [-13.54791271581155, 8.011775517550225, 8.011775517550225], time: 71.081
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: 2.7193477547591156, agent episode reward: [-13.637473324722682, 8.178410539740899, 8.178410539740899], time: 71.498
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: 2.8123880099936143, agent episode reward: [-13.791745759520737, 8.302066884757176, 8.302066884757176], time: 70.198
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: 2.6831550225885747, agent episode reward: [-12.843664939489642, 7.763409981039108, 7.763409981039108], time: 70.194
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: 3.1254143091151656, agent episode reward: [-13.391859412167483, 8.258636860641325, 8.258636860641325], time: 71.708
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: 2.486580657002364, agent episode reward: [-13.578916763839421, 8.032748710420893, 8.032748710420893], time: 72.054
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: 3.046141858364045, agent episode reward: [-13.601271458602236, 8.323706658483141, 8.323706658483141], time: 69.586
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: 2.111303722254811, agent episode reward: [-13.195520302881413, 7.653412012568111, 7.653412012568111], time: 70.633
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: 2.55201475268439, agent episode reward: [-13.426774894685655, 7.989394823685024, 7.989394823685024], time: 71.323
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: 2.8473549756095307, agent episode reward: [-12.760771624682281, 7.8040633001459065, 7.8040633001459065], time: 71.672
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: 2.585497754194669, agent episode reward: [-13.151163886934405, 7.868330820564537, 7.868330820564537], time: 71.853
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: 2.582045876403527, agent episode reward: [-13.307919712396968, 7.9449827944002465, 7.9449827944002465], time: 70.782
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: 2.1425564407790314, agent episode reward: [-13.21533346088809, 7.67894495083356, 7.67894495083356], time: 71.277
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: 2.303797088696205, agent episode reward: [-13.63636757611833, 7.970082332407268, 7.970082332407268], time: 72.44
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: 1.9699657929425116, agent episode reward: [-12.61831671127429, 7.294141252108401, 7.294141252108401], time: 71.453
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: 1.6466668661358015, agent episode reward: [-13.59588973173584, 7.62127829893582, 7.62127829893582], time: 71.142
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: 2.085095529636303, agent episode reward: [-13.898308675708652, 7.991702102672477, 7.991702102672477], time: 71.888
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: 1.794589689431455, agent episode reward: [-13.774319085357005, 7.784454387394231, 7.784454387394231], time: 71.428
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: 1.633109418552276, agent episode reward: [-14.343685461585547, 7.988397440068913, 7.988397440068913], time: 71.111
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: 1.5691086313855067, agent episode reward: [-14.321648495512221, 7.945378563448863, 7.945378563448863], time: 70.527
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: 1.317293126359161, agent episode reward: [-13.650216164414452, 7.483754645386807, 7.483754645386807], time: 72.081
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: 0.7749379208980806, agent episode reward: [-13.842419495463496, 7.308678708180787, 7.308678708180787], time: 71.228
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: 0.9634749184443672, agent episode reward: [-14.312889592381552, 7.638182255412961, 7.638182255412961], time: 71.823
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: 0.20458232141057067, agent episode reward: [-14.092817024392655, 7.148699672901613, 7.148699672901613], time: 71.37
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: 1.1010676322195825, agent episode reward: [-13.510291635282329, 7.305679633750955, 7.305679633750955], time: 70.246
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: 1.6920079467510314, agent episode reward: [-13.82619865887095, 7.75910330281099, 7.75910330281099], time: 71.357
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: 1.2334170367989716, agent episode reward: [-14.272156556355968, 7.752786796577469, 7.752786796577469], time: 71.321
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: 0.7672054006611743, agent episode reward: [-13.573760015393356, 7.170482708027265, 7.170482708027265], time: 69.733
...Finished total of 100001 episodes.
