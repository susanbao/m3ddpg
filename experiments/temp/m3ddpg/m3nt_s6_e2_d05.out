0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.078138059703438, agent episode reward: [0.6839513325519966, -27.762089392255433], time: 32.463
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -19.45887143606161, agent episode reward: [-2.955768034311977, -16.503103401749634], time: 44.298
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -12.827367613239323, agent episode reward: [-4.577175850097785, -8.250191763141538], time: 44.935
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -11.315906117941168, agent episode reward: [-2.94552239831384, -8.370383719627325], time: 45.395
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -11.00492860576262, agent episode reward: [-2.547018864931869, -8.45790974083075], time: 45.414
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -11.119215523923101, agent episode reward: [-2.196000836428749, -8.923214687494353], time: 44.996
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -11.732502188227077, agent episode reward: [-2.812332547357346, -8.920169640869732], time: 46.164
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -11.856907454724606, agent episode reward: [-3.0746423836304078, -8.7822650710942], time: 45.995
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -11.876783952114792, agent episode reward: [-2.77295867094919, -9.103825281165602], time: 45.589
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -12.332554674796505, agent episode reward: [-3.2596810469026543, -9.072873627893852], time: 46.467
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -11.845340560847712, agent episode reward: [-2.812244713875524, -9.033095846972186], time: 46.284
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -12.113706488206608, agent episode reward: [-2.9486047515518594, -9.16510173665475], time: 45.07
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -12.260892556376875, agent episode reward: [-3.2511980834556824, -9.009694472921192], time: 46.569
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -12.090453702914616, agent episode reward: [-3.059144049181964, -9.031309653732652], time: 45.374
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -12.362893518996838, agent episode reward: [-2.9981180709852424, -9.364775448011596], time: 46.397
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -12.25625304395176, agent episode reward: [-2.98483735773253, -9.271415686219232], time: 45.818
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -12.180314474048556, agent episode reward: [-3.17071062682512, -9.009603847223437], time: 46.043
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -12.505232678428476, agent episode reward: [-3.3552618813008412, -9.149970797127633], time: 45.859
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -12.502663192157966, agent episode reward: [-3.1841733120510054, -9.318489880106961], time: 45.268
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -12.245520358402365, agent episode reward: [-3.279446875901502, -8.966073482500862], time: 45.906
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -11.984511670545446, agent episode reward: [-2.995404342545346, -8.9891073280001], time: 45.027
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -12.31496487473018, agent episode reward: [-3.0809977381630853, -9.233967136567097], time: 45.399
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -12.595788267681128, agent episode reward: [-3.3206662106726355, -9.275122057008492], time: 45.336
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -12.42184368227191, agent episode reward: [-3.1306698436776053, -9.291173838594302], time: 45.741
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -12.220112517214478, agent episode reward: [-3.148106786460603, -9.072005730753878], time: 46.022
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -12.820528665635354, agent episode reward: [-3.6137846565527103, -9.206744009082646], time: 45.42
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -12.592960764123527, agent episode reward: [-3.426012799156338, -9.16694796496719], time: 44.93
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -12.592862648207564, agent episode reward: [-3.51168341582543, -9.081179232382135], time: 45.639
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -12.77003884091007, agent episode reward: [-3.5909015570287504, -9.179137283881323], time: 46.197
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -12.702123215789028, agent episode reward: [-3.6096810594754007, -9.092442156313627], time: 45.79
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -12.654667780983939, agent episode reward: [-3.444144158707613, -9.210523622276323], time: 45.966
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -12.699075261873805, agent episode reward: [-3.5282664577858496, -9.170808804087955], time: 45.316
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -12.622926967615996, agent episode reward: [-3.4650733551575694, -9.15785361245843], time: 45.42
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -12.75990263952743, agent episode reward: [-3.423832776229151, -9.33606986329828], time: 45.901
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -12.830348128965957, agent episode reward: [-3.357454996443924, -9.472893132522033], time: 45.856
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -12.804684961423893, agent episode reward: [-3.375646110318633, -9.42903885110526], time: 46.328
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -12.764994459799876, agent episode reward: [-3.3080061525189164, -9.45698830728096], time: 45.635
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -12.533830750955678, agent episode reward: [-3.539446985618815, -8.994383765336865], time: 44.846
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -12.81787035374542, agent episode reward: [-3.452374769820116, -9.365495583925302], time: 46.009
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -12.760205485919117, agent episode reward: [-3.2302808120260926, -9.529924673893023], time: 45.882
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -12.56980210620394, agent episode reward: [-3.5635515478125144, -9.006250558391425], time: 46.237
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -12.70187783195583, agent episode reward: [-3.5075511621511506, -9.194326669804678], time: 46.43
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -12.94392800191918, agent episode reward: [-3.6552787475333077, -9.288649254385874], time: 46.855
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -12.910659512906317, agent episode reward: [-3.503035310385123, -9.407624202521195], time: 46.707
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -12.789819074486735, agent episode reward: [-3.486464275382646, -9.303354799104087], time: 45.974
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -13.62108911584912, agent episode reward: [-3.811630170498485, -9.809458945350636], time: 46.654
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -13.363366657867678, agent episode reward: [-3.5213093764229497, -9.842057281444726], time: 46.491
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -12.973818034244996, agent episode reward: [-3.441756669202183, -9.532061365042813], time: 47.061
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -12.869333289253518, agent episode reward: [-3.31954043225496, -9.54979285699856], time: 46.544
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -13.329328393658791, agent episode reward: [-3.366247806784794, -9.963080586873998], time: 46.742
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -13.038415347161957, agent episode reward: [-3.5379643711597626, -9.500450976002194], time: 46.428
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -13.10543982377547, agent episode reward: [-3.4421931408133815, -9.66324668296209], time: 46.243
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -13.282722923331814, agent episode reward: [-3.457483283732893, -9.82523963959892], time: 47.17
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -13.161797264364726, agent episode reward: [-3.4734249870391305, -9.688372277325595], time: 46.017
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -13.026541131031365, agent episode reward: [-3.240970275458024, -9.785570855573342], time: 47.164
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -13.364223730492732, agent episode reward: [-3.3952579539560563, -9.968965776536676], time: 47.252
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -13.338493161301274, agent episode reward: [-3.8973489427086823, -9.441144218592592], time: 46.586
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -13.488825540423216, agent episode reward: [-3.6540451034289383, -9.834780436994276], time: 46.899
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -13.501405417993755, agent episode reward: [-3.490081832616126, -10.01132358537763], time: 45.732
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -13.570088194668516, agent episode reward: [-3.8039737982927817, -9.766114396375736], time: 46.809
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -13.402976056198103, agent episode reward: [-3.6650364115329412, -9.73793964466516], time: 47.041
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -13.22413470740812, agent episode reward: [-3.4210386219341897, -9.803096085473928], time: 46.248
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -13.469776013559615, agent episode reward: [-3.5708650380767506, -9.898910975482865], time: 45.928
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -13.444500025581695, agent episode reward: [-3.479063528057641, -9.965436497524053], time: 46.685
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -13.267976305078049, agent episode reward: [-3.3075037264732177, -9.960472578604831], time: 45.87
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -13.702476645596395, agent episode reward: [-3.7166231089666515, -9.985853536629744], time: 45.999
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -13.53570188519453, agent episode reward: [-3.37468415164107, -10.16101773355346], time: 46.662
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -13.401325170730306, agent episode reward: [-3.3395045083835084, -10.061820662346795], time: 46.692
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -13.418036151987089, agent episode reward: [-3.3875088620659537, -10.030527289921135], time: 46.725
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -13.504071295403827, agent episode reward: [-3.371189788126019, -10.132881507277808], time: 45.864
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -13.68239939779723, agent episode reward: [-3.5781738589565792, -10.10422553884065], time: 46.195
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -13.513925663069582, agent episode reward: [-3.177929600763403, -10.335996062306178], time: 47.17
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -13.453649245328899, agent episode reward: [-3.254592478353206, -10.19905676697569], time: 46.41
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -13.539351650543265, agent episode reward: [-3.4333717162055537, -10.105979934337713], time: 46.783
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -13.472837917662083, agent episode reward: [-3.284298277844552, -10.188539639817533], time: 47.802
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -13.430300450010012, agent episode reward: [-3.427528196691699, -10.002772253318316], time: 47.24
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -13.401997422108932, agent episode reward: [-3.149271770314139, -10.252725651794796], time: 46.526
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -13.505770244538486, agent episode reward: [-3.276034589935393, -10.229735654603092], time: 46.374
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -13.563041454329301, agent episode reward: [-3.620047354312356, -9.942994100016946], time: 46.436
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -13.369399571899281, agent episode reward: [-3.23257123281255, -10.13682833908673], time: 46.737
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -13.441021704083235, agent episode reward: [-3.3505648484434527, -10.090456855639783], time: 45.901
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -13.515552401251572, agent episode reward: [-3.487001657468383, -10.02855074378319], time: 46.619
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -13.776719096879992, agent episode reward: [-3.9203437561445638, -9.856375340735427], time: 46.493
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -13.653823654773063, agent episode reward: [-3.784715422462842, -9.869108232310221], time: 46.331
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -13.647800811259355, agent episode reward: [-3.422286969736138, -10.225513841523213], time: 47.391
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -13.5843205319593, agent episode reward: [-3.56466381741256, -10.019656714546738], time: 47.051
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -14.028048937360941, agent episode reward: [-3.9444526243721976, -10.083596312988744], time: 47.745
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -13.683340341405273, agent episode reward: [-3.820117865386555, -9.863222476018718], time: 47.137
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -13.622589178581963, agent episode reward: [-3.9220610834665597, -9.700528095115402], time: 47.621
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -14.114237651726178, agent episode reward: [-4.267639005562387, -9.846598646163791], time: 46.712
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -13.745753197769156, agent episode reward: [-4.128508649385155, -9.617244548383999], time: 47.094
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -13.80028189648645, agent episode reward: [-3.84224501516347, -9.958036881322977], time: 47.335
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -13.655354110249137, agent episode reward: [-3.948974836073592, -9.706379274175543], time: 46.24
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -13.524035144899651, agent episode reward: [-3.76025810979388, -9.763777035105772], time: 46.56
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -13.649682719011484, agent episode reward: [-3.763589041543234, -9.88609367746825], time: 45.667
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -13.434666394469861, agent episode reward: [-3.8987571510788577, -9.535909243391005], time: 47.065
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -13.793171268095357, agent episode reward: [-4.012234883330543, -9.78093638476481], time: 46.556
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -13.811707719097969, agent episode reward: [-3.916770102230366, -9.894937616867603], time: 47.427
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -13.768191062052942, agent episode reward: [-4.005309290891851, -9.76288177116109], time: 46.871
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -13.720886623762615, agent episode reward: [-3.8545247439591255, -9.86636187980349], time: 45.292
...Finished total of 100001 episodes.
