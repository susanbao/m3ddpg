0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.51065602895417, agent episode reward: [0.7047383785486875, -28.215394407502856], time: 31.105
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -19.224810449801268, agent episode reward: [-1.8764731074443943, -17.348337342356878], time: 44.26
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.990180516629254, agent episode reward: [-5.002715417878622, -8.987465098750636], time: 44.354
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -13.343923768443256, agent episode reward: [-4.00366249137059, -9.340261277072667], time: 44.47
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.001178883797687, agent episode reward: [-2.78060067983706, -10.22057820396063], time: 44.304
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.38472737462935, agent episode reward: [-2.640234098305375, -10.744493276323974], time: 45.689
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.617868551834599, agent episode reward: [-3.6276393739078294, -10.990229177926768], time: 45.112
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.536952318442342, agent episode reward: [-3.6942943606135694, -10.842657957828774], time: 45.402
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.603943678127528, agent episode reward: [-3.6446393919083, -10.959304286219227], time: 44.63
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.811106584001372, agent episode reward: [-3.5434617244514284, -11.267644859549947], time: 44.98
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.575667557315832, agent episode reward: [-3.2177599895769946, -11.357907567738838], time: 44.432
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.258130755452214, agent episode reward: [-4.07655877386997, -11.181571981582243], time: 43.658
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.126883491353363, agent episode reward: [-3.7828123876594315, -11.344071103693931], time: 45.127
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.003124385289258, agent episode reward: [-3.2111676341700295, -11.791956751119232], time: 45.166
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.978084420114095, agent episode reward: [-3.497452116128927, -11.480632303985168], time: 45.44
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.28301360600573, agent episode reward: [-3.5805432965863893, -11.702470309419342], time: 45.777
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.22262049893482, agent episode reward: [-3.583381773923993, -11.639238725010825], time: 44.963
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.253761631343792, agent episode reward: [-3.585566234979512, -11.66819539636428], time: 45.07
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.597852280558785, agent episode reward: [-3.6983970885585986, -11.899455192000186], time: 44.51
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.324835235601025, agent episode reward: [-3.5158478797758645, -11.808987355825161], time: 45.561
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.188119059004334, agent episode reward: [-3.2877395348074123, -11.90037952419692], time: 45.13
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.112553108164258, agent episode reward: [-3.517161833862085, -11.595391274302171], time: 44.188
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.383270761570762, agent episode reward: [-3.528951166277255, -11.854319595293507], time: 45.011
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.38771904349649, agent episode reward: [-3.917398447988897, -11.470320595507594], time: 45.243
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.305427864316558, agent episode reward: [-3.6076796286818222, -11.697748235634736], time: 44.697
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.369862038344206, agent episode reward: [-3.671364763045548, -11.698497275298656], time: 45.789
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.49549863525805, agent episode reward: [-3.374619724954718, -12.120878910303333], time: 46.007
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.091675610250862, agent episode reward: [-3.151241800901929, -11.940433809348932], time: 44.947
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.110308783164937, agent episode reward: [-3.0749528263173844, -12.035355956847553], time: 44.88
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.430560445684511, agent episode reward: [-3.255987653181748, -12.174572792502763], time: 44.79
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.205587546473547, agent episode reward: [-3.4562484494950416, -11.749339096978506], time: 45.78
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.269978037699154, agent episode reward: [-3.233911840208727, -12.036066197490427], time: 44.522
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.57047432298159, agent episode reward: [-3.488406740757201, -12.08206758222439], time: 44.815
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.101172485157443, agent episode reward: [-2.8677501022609166, -12.23342238289653], time: 44.938
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.515876289749228, agent episode reward: [-3.2345550699243795, -12.281321219824848], time: 44.842
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.521322495212642, agent episode reward: [-3.332080416262411, -12.189242078950231], time: 45.194
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.67953199879209, agent episode reward: [-3.4447718377230556, -12.234760161069037], time: 44.664
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.354826821775386, agent episode reward: [-3.209776348805455, -12.145050472969931], time: 45.873
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.344506569705983, agent episode reward: [-3.044310432659527, -12.300196137046456], time: 44.694
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.149904893629127, agent episode reward: [-3.0263093033534236, -12.123595590275706], time: 45.903
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.732346074430982, agent episode reward: [-3.487212221623978, -12.245133852807005], time: 45.015
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.505532638425573, agent episode reward: [-3.4067688142114894, -12.098763824214084], time: 46.507
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.658642445386842, agent episode reward: [-3.292396887651579, -12.366245557735262], time: 45.379
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.625304995406326, agent episode reward: [-3.102820645605264, -12.522484349801063], time: 46.447
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.39316649422167, agent episode reward: [-2.8618077855450403, -12.531358708676631], time: 45.324
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.703391647521817, agent episode reward: [-3.0508810321051607, -12.652510615416658], time: 45.316
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.848924662268407, agent episode reward: [-3.580579736671991, -12.268344925596415], time: 46.243
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.13891960707517, agent episode reward: [-3.791079851561704, -12.34783975551347], time: 46.254
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -16.037183588538632, agent episode reward: [-3.4182624099517387, -12.618921178586895], time: 45.904
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.968191278264493, agent episode reward: [-3.6500564582405706, -12.318134820023923], time: 46.63
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.115014112287138, agent episode reward: [-3.629510752866926, -12.485503359420214], time: 46.439
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.743305061508744, agent episode reward: [-3.339813682148979, -12.403491379359764], time: 46.508
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -16.173258057252532, agent episode reward: [-3.63386705425146, -12.539391003001073], time: 45.567
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -16.013991075633736, agent episode reward: [-3.601736319265422, -12.412254756368315], time: 45.101
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.539338121764374, agent episode reward: [-3.150321520675986, -12.389016601088386], time: 45.917
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -16.161217490768344, agent episode reward: [-3.636401889182647, -12.524815601585699], time: 45.921
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -16.045028180941465, agent episode reward: [-3.6481030252894104, -12.396925155652054], time: 45.3
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.883867723163139, agent episode reward: [-3.373677531710956, -12.510190191452182], time: 46.163
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.068390750074272, agent episode reward: [-3.565311745772764, -12.503079004301512], time: 46.349
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.081059414378977, agent episode reward: [-3.064454418749366, -13.016604995629612], time: 46.545
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -16.094456108778306, agent episode reward: [-3.1815194789430605, -12.912936629835249], time: 46.576
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.41002330538023, agent episode reward: [-3.4124038254439792, -12.997619479936251], time: 46.046
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.100102825465584, agent episode reward: [-3.317375217033067, -12.782727608432516], time: 45.936
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.839716635600489, agent episode reward: [-3.067757661919922, -12.771958973680567], time: 46.199
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.365788172486127, agent episode reward: [-3.7794012445436698, -12.586386927942456], time: 45.646
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.202855668557092, agent episode reward: [-3.7597361696228506, -12.443119498934243], time: 45.871
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.093402934409525, agent episode reward: [-3.5417419418764426, -12.551660992533082], time: 45.862
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.47698392073001, agent episode reward: [-3.6914985209048132, -12.785485399825196], time: 46.282
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.080996369681372, agent episode reward: [-3.602680759033481, -12.478315610647895], time: 46.255
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.130754289590573, agent episode reward: [-3.3464253368532626, -12.784328952737312], time: 45.746
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.946463675438732, agent episode reward: [-3.523864778592005, -12.422598896846724], time: 45.054
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.052273144722015, agent episode reward: [-3.532897606225574, -12.51937553849644], time: 45.107
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -16.141075649413768, agent episode reward: [-3.497185619386939, -12.643890030026826], time: 45.736
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.25536130899267, agent episode reward: [-3.4251401379348394, -12.83022117105783], time: 46.046
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.652072512795808, agent episode reward: [-3.7403176415292756, -12.911754871266531], time: 46.102
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.595478308685372, agent episode reward: [-3.807845779898761, -12.787632528786613], time: 45.371
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.60074161697224, agent episode reward: [-3.950632055524973, -12.650109561447264], time: 46.368
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -16.121474189538887, agent episode reward: [-3.154330860598652, -12.967143328940233], time: 46.192
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.02933649638888, agent episode reward: [-3.300481590561644, -12.728854905827232], time: 45.415
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.298175888710546, agent episode reward: [-3.3356680672135344, -12.962507821497013], time: 45.41
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.567992721187846, agent episode reward: [-3.5417922236158383, -13.02620049757201], time: 45.698
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.976762189650941, agent episode reward: [-3.2617991053516007, -12.714963084299342], time: 45.467
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.193909919726686, agent episode reward: [-3.3852920367762462, -12.808617882950438], time: 46.623
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -16.285057121219847, agent episode reward: [-3.747129572084783, -12.537927549135064], time: 45.577
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.202423894387966, agent episode reward: [-3.459661529415026, -12.742762364972942], time: 45.101
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.216265754856334, agent episode reward: [-3.325901370454916, -12.890364384401419], time: 46.922
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.079272428808824, agent episode reward: [-3.339735245068266, -12.739537183740559], time: 46.567
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.33896095733397, agent episode reward: [-3.3687733688152526, -12.970187588518714], time: 45.902
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.384167916137855, agent episode reward: [-3.073469070348785, -13.310698845789073], time: 46.122
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.85341534776922, agent episode reward: [-3.34064253915842, -12.5127728086108], time: 46.684
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.10139245307123, agent episode reward: [-3.04255123427461, -13.058841218796621], time: 46.626
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.35593463208711, agent episode reward: [-3.2683540847689656, -13.087580547318144], time: 45.853
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.28605487921784, agent episode reward: [-3.2959365706758916, -12.990118308541948], time: 45.737
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.62624789311218, agent episode reward: [-3.6715391204379246, -12.954708772674254], time: 45.967
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -16.216169077047994, agent episode reward: [-3.431407797687154, -12.784761279360838], time: 46.077
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.18635140798466, agent episode reward: [-3.43136386621389, -12.75498754177077], time: 46.022
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.17947940448754, agent episode reward: [-3.184467037835796, -12.995012366651743], time: 45.855
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -16.488901185833644, agent episode reward: [-3.4682960999873886, -13.020605085846254], time: 45.819
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.16285574177857, agent episode reward: [-3.2787903932024483, -12.88406534857612], time: 44.568
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.240185203426826, agent episode reward: [-3.14171639833254, -13.098468805094287], time: 45.214
...Finished total of 100001 episodes.
