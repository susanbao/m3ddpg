0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -28.170970165364132, agent episode reward: [-1.038474028000614, -27.13249613736352], time: 33.111
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.261562071942883, agent episode reward: [-4.738094007076433, -17.52346806486645], time: 44.253
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -14.077425423650899, agent episode reward: [-4.5311061162014115, -9.546319307449485], time: 43.625
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -13.009063516221287, agent episode reward: [-3.7357443489906776, -9.273319167230609], time: 44.651
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.978505833348004, agent episode reward: [-3.361248729191402, -9.6172571041566], time: 44.581
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.402980167578939, agent episode reward: [-3.0295360323082807, -10.37344413527066], time: 44.667
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.121293165490181, agent episode reward: [-3.2663784746856326, -10.854914690804547], time: 45.023
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.850278422369742, agent episode reward: [-3.702316420835793, -11.147962001533948], time: 44.602
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -15.111387807421462, agent episode reward: [-3.6785044539558576, -11.432883353465604], time: 45.169
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.668864403708751, agent episode reward: [-3.37261386191986, -11.296250541788892], time: 44.722
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.868177763955961, agent episode reward: [-3.4987779991506947, -11.369399764805268], time: 45.144
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.222845832984724, agent episode reward: [-3.703696371520561, -11.519149461464163], time: 45.703
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.267800694255268, agent episode reward: [-3.76839352496456, -11.499407169290707], time: 45.513
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.163677240307791, agent episode reward: [-3.5529695205886926, -11.610707719719102], time: 45.2
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.042250299026142, agent episode reward: [-3.3128238270059494, -11.72942647202019], time: 45.048
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.069125611516228, agent episode reward: [-3.5704757281251895, -11.498649883391037], time: 45.101
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.266729520767687, agent episode reward: [-3.4543851053417227, -11.812344415425963], time: 44.789
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.195005166282737, agent episode reward: [-3.0443727200074546, -12.150632446275281], time: 45.674
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.445261983383585, agent episode reward: [-3.387933385177961, -12.057328598205624], time: 45.136
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.393565928927496, agent episode reward: [-3.1716905504444184, -12.221875378483077], time: 45.668
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.729513769287113, agent episode reward: [-2.753113590799459, -11.976400178487653], time: 45.816
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.33325969775745, agent episode reward: [-3.1043150059725986, -12.228944691784852], time: 44.993
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.84311880118493, agent episode reward: [-3.2771030174161244, -12.566015783768806], time: 45.322
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.664971935324322, agent episode reward: [-3.271909221159092, -12.393062714165229], time: 44.472
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.389914246682496, agent episode reward: [-3.246129168604383, -12.143785078078114], time: 45.089
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.415000930370047, agent episode reward: [-2.9131927390726977, -12.501808191297346], time: 45.151
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.653810066549793, agent episode reward: [-3.3775337042284654, -12.276276362321328], time: 46.021
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.271224322660546, agent episode reward: [-3.3323495394414917, -11.938874783219054], time: 45.765
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.334703553812094, agent episode reward: [-3.0001400772374804, -12.334563476574614], time: 44.851
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.041242682160929, agent episode reward: [-2.587564983594292, -12.453677698566636], time: 44.887
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.742466336450855, agent episode reward: [-3.278470414920029, -12.463995921530826], time: 44.91
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.72360144324681, agent episode reward: [-2.994079092943375, -12.729522350303435], time: 45.636
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.468864271972915, agent episode reward: [-2.887390845408165, -12.581473426564749], time: 46.305
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.705344786040088, agent episode reward: [-3.201069269587715, -12.504275516452376], time: 45.984
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.558434831152605, agent episode reward: [-3.2075709092093887, -12.350863921943215], time: 44.894
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.7248093626384, agent episode reward: [-2.909676321685467, -12.815133040952933], time: 45.641
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.897981357031378, agent episode reward: [-3.388658208641281, -12.509323148390097], time: 45.88
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.444778836822156, agent episode reward: [-2.978764542512824, -12.46601429430933], time: 46.079
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.389599590010675, agent episode reward: [-3.140616806230517, -12.248982783780159], time: 45.879
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.98086955980111, agent episode reward: [-3.1232498244601574, -12.857619735340954], time: 45.711
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.788548895216671, agent episode reward: [-3.063806520946303, -12.724742374270367], time: 45.944
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.482393111069282, agent episode reward: [-2.9690313445055754, -12.513361766563706], time: 45.505
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.716076596996274, agent episode reward: [-3.2259858181389185, -12.490090778857358], time: 45.067
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.60730368713987, agent episode reward: [-2.782033507809585, -12.825270179330285], time: 45.193
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.736830895217059, agent episode reward: [-3.030224648141224, -12.706606247075836], time: 46.091
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -16.23985585685823, agent episode reward: [-3.4812539923591217, -12.758601864499111], time: 46.195
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.22229891184138, agent episode reward: [-3.1337871984544794, -13.088511713386902], time: 46.678
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.173819581394557, agent episode reward: [-3.4783745812962734, -12.695445000098283], time: 46.414
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.972717702144703, agent episode reward: [-2.9863696703672904, -12.98634803177741], time: 46.055
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -16.358825071217908, agent episode reward: [-3.671704798332599, -12.687120272885306], time: 45.169
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.948653876361512, agent episode reward: [-3.4004755810856087, -12.548178295275902], time: 45.88
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.675534788924372, agent episode reward: [-2.9685151825350076, -12.707019606389366], time: 45.389
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -16.03229283005656, agent episode reward: [-3.437411030306789, -12.59488179974977], time: 45.665
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.995895601709641, agent episode reward: [-3.5896260171418453, -12.406269584567793], time: 46.421
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.262877726033757, agent episode reward: [-3.6246149054642904, -12.638262820569464], time: 45.236
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.718126210909444, agent episode reward: [-3.083423717292135, -12.63470249361731], time: 45.967
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.741209895988323, agent episode reward: [-3.1252396707704513, -12.615970225217872], time: 44.422
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.17654366628034, agent episode reward: [-3.363001936391739, -12.813541729888604], time: 45.796
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.519735205987164, agent episode reward: [-4.052566714267777, -12.467168491719384], time: 46.074
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.01301023850764, agent episode reward: [-3.623830975818456, -12.389179262689185], time: 44.674
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.90418345593892, agent episode reward: [-3.5401004544474457, -12.364083001491474], time: 45.595
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.11896199647158, agent episode reward: [-3.447445266305168, -12.67151673016641], time: 45.709
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.24940651087247, agent episode reward: [-3.2874904925296713, -12.961916018342796], time: 46.436
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.139961447376635, agent episode reward: [-3.659503981343931, -12.480457466032702], time: 46.787
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.711181179770756, agent episode reward: [-2.9744297908860644, -12.736751388884688], time: 46.07
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.941426987819542, agent episode reward: [-3.5903367962319916, -12.351090191587549], time: 46.009
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.236338055128698, agent episode reward: [-3.7701368718708577, -12.46620118325784], time: 45.71
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -16.24435013198384, agent episode reward: [-3.7375952820508966, -12.506754849932943], time: 46.54
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.807615573561005, agent episode reward: [-3.5418678035184437, -12.265747770042562], time: 45.673
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.205133848242124, agent episode reward: [-3.6859839849986127, -12.519149863243511], time: 44.947
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.943560988186858, agent episode reward: [-3.616450514146107, -12.327110474040753], time: 46.044
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.277124360324052, agent episode reward: [-3.8673469808706695, -12.409777379453383], time: 45.829
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.92029546507533, agent episode reward: [-3.4206610946917158, -12.499634370383614], time: 45.822
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.966959761058002, agent episode reward: [-3.251952643415452, -12.715007117642552], time: 46.272
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.905267697862527, agent episode reward: [-3.514880637145882, -12.390387060716645], time: 47.108
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.852515474220796, agent episode reward: [-3.3865762384459916, -12.465939235774806], time: 46.218
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.142226330789093, agent episode reward: [-3.4253019491941097, -12.716924381594986], time: 46.301
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.992702815614349, agent episode reward: [-3.4655703758489365, -12.527132439765413], time: 46.045
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.385959909805187, agent episode reward: [-3.763229781611226, -12.622730128193957], time: 45.645
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.98808286833215, agent episode reward: [-3.4140211498193223, -12.574061718512825], time: 46.535
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.311259422377542, agent episode reward: [-3.5666345713839847, -12.744624850993558], time: 46.411
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.321533570120735, agent episode reward: [-3.692584377858219, -12.628949192262521], time: 46.868
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.152585503383527, agent episode reward: [-3.4207851582889095, -12.73180034509462], time: 46.556
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.96999496403282, agent episode reward: [-3.5333446385558656, -12.436650325476952], time: 47.314
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.28399639116458, agent episode reward: [-3.8521401670851256, -12.43185622407946], time: 46.171
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.16670180532671, agent episode reward: [-3.659769904391803, -12.506931900934907], time: 47.021
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.839844597297436, agent episode reward: [-3.4809151075962212, -12.358929489701215], time: 46.439
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.16342925493722, agent episode reward: [-3.694219347323736, -12.469209907613484], time: 44.957
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.055153260161006, agent episode reward: [-3.2440362082161727, -12.811117051944832], time: 46.365
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.281494051768647, agent episode reward: [-3.4145124946878465, -12.866981557080802], time: 45.11
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.79052174894942, agent episode reward: [-3.48954771653425, -12.300974032415171], time: 44.791
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.95585377239636, agent episode reward: [-3.542795435453035, -12.413058336943324], time: 47.131
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -16.13029535788973, agent episode reward: [-3.492904031407461, -12.63739132648227], time: 45.811
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.64342725687117, agent episode reward: [-3.1676338387111356, -12.475793418160036], time: 46.853
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.716006045686301, agent episode reward: [-3.118732544397588, -12.597273501288715], time: 45.876
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.80709123664014, agent episode reward: [-3.194654014873474, -12.612437221766667], time: 45.852
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.190700825887212, agent episode reward: [-3.583708152462554, -12.606992673424658], time: 46.193
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.812449435979433, agent episode reward: [-3.4286226287799946, -12.38382680719944], time: 47.013
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.164214086734866, agent episode reward: [-3.4541758815684376, -12.710038205166425], time: 47.037
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.883059655774966, agent episode reward: [-3.435528328132159, -12.44753132764281], time: 43.202
...Finished total of 100001 episodes.
