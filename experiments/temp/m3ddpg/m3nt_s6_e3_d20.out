0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -27.239586751757116, agent episode reward: [-0.8676981589481104, -26.371888592809007], time: 32.026
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -19.20295277061381, agent episode reward: [-4.936448696767453, -14.266504073846358], time: 44.543
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.668230574774686, agent episode reward: [-4.437818376836036, -9.230412197938652], time: 44.347
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.64768588907353, agent episode reward: [-3.8073746729065463, -8.840311216166983], time: 44.26
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.120572329538307, agent episode reward: [-3.4380037069535936, -9.682568622584713], time: 44.709
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.469710267688038, agent episode reward: [-3.006702929244371, -10.463007338443669], time: 44.454
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.184742952784724, agent episode reward: [-3.8069238233220175, -10.377819129462702], time: 44.637
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.99027100094374, agent episode reward: [-3.7211378820616354, -11.269133118882104], time: 43.923
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.81061069646372, agent episode reward: [-3.609825074364474, -11.200785622099245], time: 45.264
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.977507443154176, agent episode reward: [-3.522857106482028, -11.454650336672149], time: 44.457
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -15.029616404460548, agent episode reward: [-3.387252732120991, -11.642363672339558], time: 45.045
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.842609033344587, agent episode reward: [-3.4476033068217085, -11.395005726522879], time: 44.89
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.011729599295148, agent episode reward: [-3.2696632258754463, -11.7420663734197], time: 44.941
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.238920802054581, agent episode reward: [-3.8523113309492474, -11.386609471105334], time: 45.513
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.012655876152945, agent episode reward: [-3.434626849676892, -11.578029026476054], time: 45.341
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.67340744440888, agent episode reward: [-2.770784327815787, -11.902623116593093], time: 45.384
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.888217923548506, agent episode reward: [-3.575935783659286, -11.31228213988922], time: 44.119
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.68359081789861, agent episode reward: [-3.0243098869340077, -11.659280930964604], time: 44.297
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.994376217614464, agent episode reward: [-2.893045831414119, -12.101330386200345], time: 44.323
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.919349863400443, agent episode reward: [-3.0335628497744525, -11.88578701362599], time: 44.887
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.8824403221818, agent episode reward: [-3.266816238658066, -11.615624083523732], time: 45.305
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.590486960590672, agent episode reward: [-2.6358472284853223, -11.95463973210535], time: 44.895
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.533121417574126, agent episode reward: [-2.8981914382605685, -11.634929979313558], time: 43.587
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.781624184724583, agent episode reward: [-2.8688237306078315, -11.912800454116752], time: 44.9
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.747476544911898, agent episode reward: [-2.6829610557749812, -12.064515489136916], time: 45.281
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.061882206773586, agent episode reward: [-3.0867394990648673, -11.975142707708716], time: 44.81
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -14.899843168072653, agent episode reward: [-2.982230762754918, -11.917612405317733], time: 44.939
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.262824000100922, agent episode reward: [-3.255324041074544, -12.007499959026378], time: 45.296
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.951095068428039, agent episode reward: [-2.725349583971567, -12.225745484456473], time: 44.066
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.239946240238167, agent episode reward: [-3.361778813384722, -11.878167426853446], time: 45.276
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.468959204179333, agent episode reward: [-3.1709061652453805, -12.298053038933952], time: 44.676
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.534145545374809, agent episode reward: [-2.8954814968162306, -12.638664048558578], time: 44.947
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.127848296188361, agent episode reward: [-2.9074560665709956, -12.220392229617365], time: 45.31
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.466193651159486, agent episode reward: [-3.0346583646717544, -12.431535286487732], time: 44.962
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.338024589827281, agent episode reward: [-2.8112493097292544, -12.526775280098027], time: 45.541
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.301981792901955, agent episode reward: [-2.99430120832327, -12.307680584578685], time: 45.338
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.166862115382456, agent episode reward: [-3.1719327847401635, -11.994929330642291], time: 45.159
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.216525796175295, agent episode reward: [-2.9485992047017398, -12.267926591473557], time: 45.164
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.413265135672635, agent episode reward: [-2.896130526923137, -12.517134608749497], time: 45.442
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.24137040029086, agent episode reward: [-2.6338327730468274, -12.607537627244033], time: 45.801
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.56272040944266, agent episode reward: [-2.9172287185607346, -12.645491690881926], time: 44.871
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.79014267246201, agent episode reward: [-2.9923861419519904, -12.797756530510021], time: 44.752
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.962560386322442, agent episode reward: [-3.3484992614574995, -12.614061124864943], time: 45.499
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.43773003052806, agent episode reward: [-2.902002395713902, -12.535727634814158], time: 44.89
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.570374417580199, agent episode reward: [-2.7470025353465553, -12.823371882233644], time: 44.403
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.381027223347864, agent episode reward: [-2.8490298620918444, -12.531997361256018], time: 46.06
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.037458110612445, agent episode reward: [-3.4547772812582345, -12.582680829354212], time: 45.365
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.787381773875188, agent episode reward: [-3.302201608903026, -12.48518016497216], time: 44.365
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.783650224110566, agent episode reward: [-3.1336925504074498, -12.649957673703115], time: 45.807
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.728531987988294, agent episode reward: [-3.345704122374631, -12.382827865613661], time: 45.98
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.245872460566233, agent episode reward: [-3.9039946124894134, -12.34187784807682], time: 45.78
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.965499429905206, agent episode reward: [-3.6483741828050107, -12.317125247100197], time: 45.842
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.891470445230963, agent episode reward: [-3.5802385180347454, -12.311231927196216], time: 46.166
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.944728637612117, agent episode reward: [-3.5403121873514665, -12.404416450260651], time: 45.687
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.042486952785026, agent episode reward: [-3.6644245159826303, -12.378062436802397], time: 46.58
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.70060847773798, agent episode reward: [-3.443452259670373, -12.257156218067607], time: 46.023
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.714766593711644, agent episode reward: [-3.337065073188848, -12.377701520522796], time: 45.643
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.989064089382612, agent episode reward: [-3.9147983654837706, -12.074265723898842], time: 44.821
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.820072738248802, agent episode reward: [-3.675577003726388, -12.144495734522414], time: 44.96
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.71833823093649, agent episode reward: [-3.5281599530588363, -12.190178277877655], time: 46.278
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.90233805649822, agent episode reward: [-3.6405497514015015, -12.261788305096717], time: 45.821
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.72152927462973, agent episode reward: [-3.6466210568944906, -12.074908217735238], time: 45.54
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.981365132977379, agent episode reward: [-3.735441332878155, -12.245923800099224], time: 45.444
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.853165356950687, agent episode reward: [-3.632449954833652, -12.220715402117035], time: 46.466
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.12307121701146, agent episode reward: [-3.6539113495879647, -12.469159867423496], time: 46.322
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.834378417012724, agent episode reward: [-3.682856086261739, -12.151522330750984], time: 46.314
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.847913188821662, agent episode reward: [-3.584853827359979, -12.26305936146168], time: 46.108
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.87636510625985, agent episode reward: [-3.559359883310205, -12.317005222949646], time: 46.533
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.033592713890748, agent episode reward: [-3.9433884463059137, -12.09020426758483], time: 45.839
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.876494261946792, agent episode reward: [-3.388086726447842, -12.48840753549895], time: 46.398
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.647618404003586, agent episode reward: [-3.5689594944308363, -12.078658909572749], time: 45.139
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.802113661230937, agent episode reward: [-3.4878077770200204, -12.314305884210915], time: 44.934
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.476700441109573, agent episode reward: [-3.1402287516532685, -12.336471689456305], time: 44.37
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.48877929236199, agent episode reward: [-3.325067377623658, -12.163711914738329], time: 45.517
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.59003683526075, agent episode reward: [-3.1248176819297933, -12.465219153330956], time: 44.404
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.579985410787321, agent episode reward: [-3.523804917467739, -12.056180493319582], time: 45.063
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.11025028436639, agent episode reward: [-3.488080234833202, -12.62217004953319], time: 45.886
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.628832009512744, agent episode reward: [-3.3692006102814314, -12.259631399231314], time: 44.777
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.67642110323271, agent episode reward: [-3.4055742547087906, -12.27084684852392], time: 46.091
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.981170337865834, agent episode reward: [-3.706286767833466, -12.274883570032365], time: 46.069
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.120678309146705, agent episode reward: [-3.730662415865141, -12.390015893281564], time: 46.194
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.663073527848956, agent episode reward: [-3.652782006118973, -12.01029152172998], time: 46.005
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.561425997291256, agent episode reward: [-3.5690695805215324, -11.992356416769722], time: 45.146
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.900565597280412, agent episode reward: [-3.398082760135868, -12.502482837144544], time: 46.016
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.155039304136945, agent episode reward: [-3.6627680734506565, -12.492271230686288], time: 45.039
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.069990761350617, agent episode reward: [-3.8892268330948765, -12.180763928255741], time: 46.139
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.250875108997768, agent episode reward: [-3.9622795618340687, -12.288595547163698], time: 46.093
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.851448663220804, agent episode reward: [-3.584945053111959, -12.266503610108845], time: 44.597
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.193892869680422, agent episode reward: [-3.7638353524334054, -12.430057517247018], time: 45.149
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.672121057131326, agent episode reward: [-3.3285011397104927, -12.343619917420835], time: 46.443
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.884744246081608, agent episode reward: [-3.2861601196116155, -12.598584126469992], time: 46.204
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.12422073019461, agent episode reward: [-3.7494879722863077, -12.374732757908303], time: 46.39
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.814686736737062, agent episode reward: [-3.655400257836965, -12.159286478900098], time: 46.513
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.720193736780372, agent episode reward: [-3.460773988311496, -12.259419748468876], time: 46.256
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.738638215156993, agent episode reward: [-3.562074514486485, -12.176563700670508], time: 45.32
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.0139777512822, agent episode reward: [-3.837882277237525, -12.176095474044676], time: 44.673
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.877052447345417, agent episode reward: [-3.4909184836601987, -12.386133963685216], time: 46.23
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.62441385151177, agent episode reward: [-3.2330910330728164, -12.391322818438951], time: 45.514
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.864916516015452, agent episode reward: [-3.40177791730613, -12.463138598709321], time: 46.315
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.928978905992478, agent episode reward: [-3.502697008379246, -12.426281897613233], time: 42.12
...Finished total of 100001 episodes.
