0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.863027548725952, agent episode reward: [0.11421328613055445, -26.97724083485651], time: 29.612
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.227489603559953, agent episode reward: [2.251559564311539, -22.479049167871494], time: 43.845
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -14.250920240901795, agent episode reward: [-4.8198606669430735, -9.43105957395872], time: 43.385
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -13.011302552359593, agent episode reward: [-4.048935530710686, -8.962367021648907], time: 43.162
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.497299533087004, agent episode reward: [-4.1871717901056265, -9.310127742981377], time: 44.262
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -14.41659180662925, agent episode reward: [-4.211614303137693, -10.204977503491559], time: 43.866
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.93945829087631, agent episode reward: [-3.966802832255968, -10.97265545862034], time: 43.362
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.41876128418258, agent episode reward: [-3.5133820739563664, -10.905379210226211], time: 44.213
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.794545871994044, agent episode reward: [-3.476702857846235, -11.31784301414781], time: 44.558
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -15.03380009755809, agent episode reward: [-3.814014999356571, -11.21978509820152], time: 44.138
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.955524140095044, agent episode reward: [-3.5013535742620365, -11.454170565833008], time: 44.091
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.976419483449998, agent episode reward: [-3.753245297403613, -11.223174186046384], time: 44.006
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.154049999761577, agent episode reward: [-3.8888844121225903, -11.265165587638988], time: 44.985
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.872129009751882, agent episode reward: [-3.850726710942571, -11.021402298809312], time: 44.513
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.78491754578804, agent episode reward: [-3.3829246828048674, -11.401992862983175], time: 43.944
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.007851485606187, agent episode reward: [-3.811647335754733, -11.196204149851452], time: 45.236
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.240187010255662, agent episode reward: [-3.6122597525464233, -11.627927257709239], time: 45.319
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.136152596072396, agent episode reward: [-3.747950103510645, -11.388202492561751], time: 44.612
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.329197857929845, agent episode reward: [-3.6752019833524354, -11.653995874577413], time: 45.412
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.915884695499429, agent episode reward: [-3.706940176181742, -11.208944519317686], time: 45.321
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.261101850426178, agent episode reward: [-3.5960233401114645, -11.66507851031471], time: 44.939
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.051575967635545, agent episode reward: [-3.691805274927799, -11.359770692707745], time: 44.402
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.035218075061556, agent episode reward: [-3.2191167866130472, -11.81610128844851], time: 43.937
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.402884606890831, agent episode reward: [-3.808841505635253, -11.594043101255577], time: 45.183
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.280593997224518, agent episode reward: [-3.5462890187451284, -11.734304978479388], time: 44.377
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.07532100826315, agent episode reward: [-3.401838296582342, -11.673482711680808], time: 44.12
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.354452838701025, agent episode reward: [-3.7874990190974307, -11.566953819603595], time: 44.895
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.008819799373788, agent episode reward: [-3.63533851358005, -11.373481285793737], time: 44.152
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.343807008696011, agent episode reward: [-3.67429331719382, -11.669513691502193], time: 44.737
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.886965763875901, agent episode reward: [-3.05958136100037, -11.82738440287553], time: 44.774
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -14.832582872179527, agent episode reward: [-2.949952243410902, -11.882630628768627], time: 45.171
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -14.69785598136109, agent episode reward: [-3.2825939443647894, -11.415262036996301], time: 44.26
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.134139087159618, agent episode reward: [-3.487355886782845, -11.646783200376772], time: 44.955
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.468921589100496, agent episode reward: [-3.597009182703634, -11.871912406396863], time: 44.305
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.394351461194237, agent episode reward: [-3.5177502726113308, -11.876601188582907], time: 44.039
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.470246453356472, agent episode reward: [-3.649769060275422, -11.82047739308105], time: 44.147
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.196856598624107, agent episode reward: [-3.4404288992112906, -11.756427699412816], time: 44.627
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.136420971603238, agent episode reward: [-3.388910750970761, -11.747510220632478], time: 45.103
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.604806731100343, agent episode reward: [-3.8595524729075996, -11.745254258192743], time: 45.034
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.457908351810783, agent episode reward: [-3.531528265732768, -11.926380086078016], time: 45.779
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.4982163356624, agent episode reward: [-3.5570635085678703, -11.94115282709453], time: 45.128
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.770286093537234, agent episode reward: [-3.890932707284892, -11.87935338625234], time: 45.993
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.769510751426484, agent episode reward: [-3.5887513752052236, -12.180759376221262], time: 44.89
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.578031515101593, agent episode reward: [-3.35451513913476, -12.223516375966836], time: 45.022
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.692348041949915, agent episode reward: [-3.291873688566399, -12.400474353383514], time: 45.73
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.426323916301458, agent episode reward: [-3.025053659233496, -12.401270257067964], time: 45.944
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.463404663161032, agent episode reward: [-3.303579382291925, -12.159825280869109], time: 44.94
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.520945035065987, agent episode reward: [-3.562548924837621, -11.958396110228362], time: 44.129
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.500371878539383, agent episode reward: [-3.1155196690616864, -12.384852209477696], time: 43.736
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.88080167254502, agent episode reward: [-3.3131788206905877, -12.567622851854434], time: 45.259
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.593164369678412, agent episode reward: [-3.304689847098602, -12.288474522579811], time: 46.213
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.89078874949165, agent episode reward: [-3.5224583604415636, -12.368330389050085], time: 45.258
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.4943123899561, agent episode reward: [-3.083929277647361, -12.410383112308738], time: 44.754
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.609777566104944, agent episode reward: [-3.145543054673999, -12.464234511430947], time: 45.422
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.806942024738849, agent episode reward: [-3.772272405661989, -12.03466961907686], time: 46.25
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -16.1139371773342, agent episode reward: [-3.7833786218267083, -12.330558555507492], time: 45.007
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.6232971814864, agent episode reward: [-3.5504578882130153, -12.072839293273384], time: 46.111
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.742066680757755, agent episode reward: [-3.9712865525299055, -11.77078012822785], time: 45.861
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.889048063775405, agent episode reward: [-3.9126955469969626, -11.976352516778443], time: 45.639
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.210455935720628, agent episode reward: [-4.115254850116121, -12.095201085604506], time: 44.182
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.602945620687912, agent episode reward: [-3.5910680226181837, -12.011877598069729], time: 44.22
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.00370740748435, agent episode reward: [-4.247319012964157, -11.756388394520188], time: 44.965
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.792947965447846, agent episode reward: [-3.9084099720338576, -11.884537993413986], time: 45.888
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.796577753892421, agent episode reward: [-3.761783979552185, -12.034793774340235], time: 45.605
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.891063051043025, agent episode reward: [-3.540918318133562, -12.350144732909463], time: 45.329
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.703356366410864, agent episode reward: [-3.914749311338673, -11.788607055072191], time: 45.021
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.42686795178023, agent episode reward: [-3.4664933236419238, -11.960374628138306], time: 45.388
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.627742994127683, agent episode reward: [-3.719299486175723, -11.90844350795196], time: 44.853
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.574356665268951, agent episode reward: [-3.260543479192799, -12.313813186076153], time: 45.915
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.12628582011988, agent episode reward: [-3.621171757418291, -12.505114062701589], time: 44.343
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.108209889583982, agent episode reward: [-3.7671685411706934, -12.341041348413288], time: 45.296
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.859959061375623, agent episode reward: [-4.0171210717396555, -11.842837989635965], time: 45.892
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.925090126217158, agent episode reward: [-4.169771032485354, -11.755319093731803], time: 44.855
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.92300153672331, agent episode reward: [-3.862442044140419, -12.06055949258289], time: 45.484
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.934993302623088, agent episode reward: [-3.8268762045036118, -12.108117098119477], time: 45.948
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.262519209347523, agent episode reward: [-4.104491801804992, -12.158027407542534], time: 44.611
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.111642689136133, agent episode reward: [-3.896547802642378, -12.215094886493752], time: 45.201
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -16.261214642272023, agent episode reward: [-4.374387128065366, -11.886827514206658], time: 45.273
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -16.16595334037143, agent episode reward: [-4.172071712553315, -11.993881627818114], time: 44.92
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.248176724617082, agent episode reward: [-4.094453028542074, -12.153723696075005], time: 45.169
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.319268847392607, agent episode reward: [-3.8382118854050513, -12.481056961987555], time: 46.158
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.235131849706182, agent episode reward: [-4.11013396004822, -12.124997889657962], time: 46.552
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.377213248683926, agent episode reward: [-4.122403092330859, -12.254810156353068], time: 45.715
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -16.068154290803594, agent episode reward: [-3.4513411441405304, -12.616813146663063], time: 46.394
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.725912510987982, agent episode reward: [-3.5503378200771336, -12.17557469091085], time: 45.976
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.179698821117412, agent episode reward: [-3.5652758656408996, -12.614422955476515], time: 45.543
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.12093441584064, agent episode reward: [-3.497366393778611, -12.623568022062027], time: 44.814
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.05661647866506, agent episode reward: [-3.4096486570042965, -12.64696782166076], time: 45.228
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.882798900421207, agent episode reward: [-3.5202906014963204, -12.362508298924888], time: 46.094
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.333204971432423, agent episode reward: [-3.8080738411257236, -12.5251311303067], time: 45.847
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.848023841292376, agent episode reward: [-3.2658962796169817, -12.582127561675394], time: 44.829
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.195231676156283, agent episode reward: [-3.4967189282700257, -12.698512747886255], time: 46.007
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.955358075910961, agent episode reward: [-3.3142947570721817, -12.64106331883878], time: 45.86
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.986153949906965, agent episode reward: [-2.9270871502238998, -13.05906679968307], time: 45.163
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -16.058284808896683, agent episode reward: [-3.5446298515892387, -12.513654957307443], time: 45.554
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.770926086450476, agent episode reward: [-3.442639527170726, -12.328286559279748], time: 45.193
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.903815568457745, agent episode reward: [-3.661698238724207, -12.242117329733537], time: 46.005
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.809220469483503, agent episode reward: [-3.451190753848387, -12.358029715635116], time: 45.496
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.752365906166025, agent episode reward: [-3.5294337733186882, -12.222932132847339], time: 46.215
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.09136418642386, agent episode reward: [-3.5993693230884594, -12.4919948633354], time: 45.538
...Finished total of 100001 episodes.
