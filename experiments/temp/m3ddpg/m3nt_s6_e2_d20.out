0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.857404800291196, agent episode reward: [-1.3586144796794546, -25.498790320611743], time: 32.127
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.551720892640837, agent episode reward: [-4.821966441738386, -17.72975445090245], time: 44.481
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.570954750135815, agent episode reward: [-4.672539591409401, -8.898415158726417], time: 44.459
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -13.177421373124204, agent episode reward: [-4.1401572605138375, -9.037264112610366], time: 44.801
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.795820037043384, agent episode reward: [-3.3123661028469775, -9.483453934196406], time: 44.765
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.891708024130795, agent episode reward: [-3.6421345348159684, -10.249573489314827], time: 45.307
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.451445322686693, agent episode reward: [-3.6579749594971127, -10.793470363189583], time: 45.418
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.743371212670292, agent episode reward: [-3.6955891480035996, -11.047782064666695], time: 44.195
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.691743439047432, agent episode reward: [-3.531890754776152, -11.159852684271279], time: 44.245
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -15.110455761026703, agent episode reward: [-3.9020116374552316, -11.208444123571473], time: 44.997
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.739392673946906, agent episode reward: [-3.462593865072118, -11.276798808874785], time: 45.407
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.988681565139734, agent episode reward: [-3.491117798159872, -11.497563766979862], time: 45.397
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.825261009274815, agent episode reward: [-3.594465721887009, -11.23079528738781], time: 44.855
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.100556921277855, agent episode reward: [-3.6867598310797676, -11.413797090198086], time: 45.273
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.873455268031146, agent episode reward: [-3.425859420942946, -11.447595847088202], time: 44.917
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.120787982521783, agent episode reward: [-3.7843579957984677, -11.336429986723317], time: 44.598
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.832676747426053, agent episode reward: [-3.586795779206183, -11.245880968219868], time: 44.131
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.768345817000816, agent episode reward: [-3.4358401801599365, -11.332505636840878], time: 45.398
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.253756550846747, agent episode reward: [-3.7488346462069395, -11.504921904639806], time: 45.52
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.209023843090694, agent episode reward: [-3.8612512023859358, -11.34777264070476], time: 45.057
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.113103571696895, agent episode reward: [-3.388096643207005, -11.72500692848989], time: 45.148
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.00796799054025, agent episode reward: [-3.307736455477366, -11.700231535062885], time: 45.129
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.141304454112214, agent episode reward: [-3.8134061191871496, -11.327898334925065], time: 45.555
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.14114989834183, agent episode reward: [-3.4654428402671185, -11.675707058074709], time: 44.18
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.873206062227462, agent episode reward: [-3.3410804068579227, -11.53212565536954], time: 44.974
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.340457080258886, agent episode reward: [-3.3140715275968464, -12.026385552662038], time: 45.458
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.112311039706322, agent episode reward: [-2.933949377393889, -12.178361662312433], time: 44.964
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.17344974320268, agent episode reward: [-3.1313846527638702, -12.04206509043881], time: 45.844
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.472694526433767, agent episode reward: [-3.146384394333583, -12.326310132100183], time: 44.807
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.248353387896081, agent episode reward: [-2.787684451917631, -12.460668935978449], time: 44.293
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.31470885108415, agent episode reward: [-2.97028386319513, -12.344424987889019], time: 44.573
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.22335662511714, agent episode reward: [-2.877995958029164, -12.345360667087974], time: 44.805
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.271080803142945, agent episode reward: [-2.846883358124684, -12.42419744501826], time: 45.712
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -15.396979379215578, agent episode reward: [-2.7483046043309294, -12.648674774884649], time: 44.801
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -15.70465728742713, agent episode reward: [-2.986589934313453, -12.718067353113677], time: 45.187
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.480061285812365, agent episode reward: [-3.005734119200846, -12.474327166611518], time: 44.089
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.562293072654363, agent episode reward: [-3.1618048144504702, -12.400488258203895], time: 44.794
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.648936006260051, agent episode reward: [-2.9818762811529655, -12.667059725107087], time: 45.402
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.522769719469114, agent episode reward: [-3.1288121468915624, -12.39395757257755], time: 44.872
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -15.564292029632846, agent episode reward: [-2.847438782066993, -12.716853247565854], time: 45.318
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -15.629170459534484, agent episode reward: [-2.687988129154073, -12.941182330380412], time: 44.532
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -15.827004408110044, agent episode reward: [-3.1805266341914105, -12.646477773918633], time: 45.224
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -15.868915333936187, agent episode reward: [-3.2455741260216384, -12.62334120791455], time: 45.881
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -15.783905249389507, agent episode reward: [-2.6362190815312667, -13.147686167858241], time: 45.307
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.954372660841512, agent episode reward: [-2.891517300942206, -13.062855359899306], time: 44.61
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.810391542207237, agent episode reward: [-2.8617997244342765, -12.94859181777296], time: 45.921
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -15.897379367086163, agent episode reward: [-2.983521771958384, -12.913857595127778], time: 46.108
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.003350840168736, agent episode reward: [-2.99067267135108, -13.012678168817656], time: 45.219
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.725994045033243, agent episode reward: [-2.9228353747931877, -12.803158670240055], time: 45.509
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.868918332397676, agent episode reward: [-3.0929576061976407, -12.775960726200035], time: 45.213
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -15.746276987857128, agent episode reward: [-2.6421585945315744, -13.104118393325553], time: 45.551
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.694353227537722, agent episode reward: [-3.0154451097883883, -12.678908117749332], time: 45.277
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.772492803666848, agent episode reward: [-2.9304722953315894, -12.842020508335256], time: 45.163
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.826286569893618, agent episode reward: [-3.1459318488893993, -12.680354721004216], time: 46.09
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.388809497266696, agent episode reward: [-3.2885216604820515, -13.100287836784647], time: 45.306
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -16.501775945672854, agent episode reward: [-3.761946308853165, -12.73982963681969], time: 46.114
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -16.09423187822829, agent episode reward: [-3.5956088210803063, -12.498623057147983], time: 45.404
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.064659177841765, agent episode reward: [-3.394219972463978, -12.670439205377788], time: 44.82
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.783482910454307, agent episode reward: [-3.4315357914547877, -12.351947118999517], time: 44.799
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -16.246085261213395, agent episode reward: [-3.282757403937298, -12.963327857276093], time: 44.637
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.886048949004278, agent episode reward: [-3.315827792451064, -12.570221156553213], time: 46.438
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.746997011052187, agent episode reward: [-3.0962322782042606, -12.650764732847925], time: 45.778
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.958555242679205, agent episode reward: [-3.533175774505492, -12.42537946817371], time: 45.604
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.965664598046816, agent episode reward: [-3.532713592420523, -12.432951005626293], time: 45.811
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.186523436861634, agent episode reward: [-3.840427980359305, -12.34609545650233], time: 44.948
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.99312948773332, agent episode reward: [-3.639529499023631, -12.353599988709691], time: 45.892
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.643933027175866, agent episode reward: [-3.1790238847287795, -12.464909142447086], time: 45.909
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.642486845357249, agent episode reward: [-3.4113071137689794, -12.231179731588266], time: 46.146
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.835306795137269, agent episode reward: [-3.523516479741645, -12.311790315395625], time: 46.045
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.02544453425383, agent episode reward: [-3.505244340951964, -12.520200193301866], time: 45.366
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.907865428003428, agent episode reward: [-3.744180593727611, -12.163684834275818], time: 46.555
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.282731660926732, agent episode reward: [-3.587587686622025, -12.695143974304706], time: 45.398
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.68976006704718, agent episode reward: [-3.359838003108842, -12.329922063938339], time: 46.164
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.89545446982691, agent episode reward: [-3.477935503425966, -12.417518966400944], time: 46.011
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.989803177298398, agent episode reward: [-3.806467141301652, -12.183336035996744], time: 45.181
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.03609368623533, agent episode reward: [-3.4674702219405473, -12.568623464294783], time: 45.775
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -15.761583303418869, agent episode reward: [-3.760153581361654, -12.001429722057214], time: 46.184
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.515670099232285, agent episode reward: [-3.5149551815545026, -12.000714917677781], time: 45.021
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.492393922660224, agent episode reward: [-3.4920531329064155, -12.000340789753809], time: 46.36
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.788045720546693, agent episode reward: [-3.676131500151312, -12.111914220395384], time: 45.066
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -15.574288346155624, agent episode reward: [-3.3837401636395605, -12.190548182516062], time: 45.488
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.187597005286552, agent episode reward: [-3.8938610987288014, -12.29373590655775], time: 46.729
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.713751057294747, agent episode reward: [-3.6122777577911136, -12.101473299503631], time: 45.845
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.836509113053111, agent episode reward: [-3.71009713697449, -12.126411976078618], time: 46.395
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.770490587166467, agent episode reward: [-3.2215812409832965, -12.548909346183171], time: 46.259
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.119752689646035, agent episode reward: [-3.703038369969913, -12.416714319676123], time: 46.235
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.847101245624332, agent episode reward: [-3.4259914681129757, -12.421109777511356], time: 44.868
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.670624229889421, agent episode reward: [-3.2888903452306586, -12.381733884658763], time: 45.949
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -16.03303819403422, agent episode reward: [-3.889147037795742, -12.143891156238476], time: 46.098
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.211291175604707, agent episode reward: [-3.899159599231343, -12.312131576373362], time: 44.993
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.007466633412715, agent episode reward: [-3.7091043228566476, -12.298362310556065], time: 45.412
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.946067664532887, agent episode reward: [-3.9418065242022147, -12.004261140330671], time: 46.313
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.873307647936103, agent episode reward: [-3.344238648716101, -12.529068999220002], time: 46.809
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.894265332881648, agent episode reward: [-3.601289135664093, -12.292976197217556], time: 46.166
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.949011750152597, agent episode reward: [-3.548704862002165, -12.400306888150432], time: 45.274
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.898629847377594, agent episode reward: [-3.5770530060986037, -12.32157684127899], time: 45.655
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.568818886933181, agent episode reward: [-3.267168308375881, -12.301650578557302], time: 44.558
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.779526569816534, agent episode reward: [-3.691059591174366, -12.088466978642169], time: 45.01
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.108087955448145, agent episode reward: [-3.729741631656064, -12.378346323792078], time: 46.406
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.065983376382523, agent episode reward: [-3.7065593396113194, -12.359424036771205], time: 42.948
...Finished total of 100001 episodes.
