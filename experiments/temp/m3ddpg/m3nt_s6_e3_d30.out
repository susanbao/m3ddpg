0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.579576062265392, agent episode reward: [1.9477941131888985, -27.527370175454294], time: 33.135
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.67651534199477, agent episode reward: [-1.6795675905431606, -18.996947751451604], time: 44.983
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -14.251557212386055, agent episode reward: [-4.779078309919674, -9.472478902466381], time: 44.533
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.9802007461811, agent episode reward: [-3.875898357363318, -9.104302388817782], time: 45.369
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.42746000591549, agent episode reward: [-3.1547066817887144, -9.272753324126777], time: 45.082
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.906094188584289, agent episode reward: [-3.8400411625408304, -10.066053026043459], time: 44.581
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.44956127144532, agent episode reward: [-3.5377691128737028, -10.91179215857162], time: 44.431
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.704077303428349, agent episode reward: [-3.5389178321201347, -11.165159471308217], time: 45.513
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.62340916437421, agent episode reward: [-3.27999754141139, -11.34341162296282], time: 44.656
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -15.452582306773234, agent episode reward: [-3.944267095193312, -11.50831521157992], time: 45.421
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -15.237501846647447, agent episode reward: [-3.780395963942076, -11.45710588270537], time: 44.835
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -15.01741376063624, agent episode reward: [-3.5750628497264514, -11.442350910909788], time: 44.828
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -15.262863711317419, agent episode reward: [-3.787874331760818, -11.474989379556602], time: 44.166
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -15.362484232974124, agent episode reward: [-3.7803598093031217, -11.582124423671003], time: 45.35
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -15.154762683334422, agent episode reward: [-3.638322367337195, -11.516440315997224], time: 44.842
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -15.366782718958403, agent episode reward: [-3.633067324435688, -11.733715394522713], time: 44.072
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -15.269421241656493, agent episode reward: [-3.388304324363471, -11.88111691729302], time: 45.55
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -15.191682225844612, agent episode reward: [-3.992957476212906, -11.198724749631706], time: 44.545
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -15.191411285408078, agent episode reward: [-3.5675548138045126, -11.623856471603565], time: 44.138
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -15.169699717675986, agent episode reward: [-3.4347634844075556, -11.734936233268428], time: 45.259
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -15.17995540198972, agent episode reward: [-3.123094414137151, -12.05686098785257], time: 45.279
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -15.828218072935815, agent episode reward: [-3.9212078820960143, -11.9070101908398], time: 45.906
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -15.870146045247656, agent episode reward: [-3.4342047449698567, -12.4359413002778], time: 45.515
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -15.695030191189115, agent episode reward: [-3.3331898926213226, -12.36184029856779], time: 45.758
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -15.890746129943594, agent episode reward: [-3.49719773739863, -12.393548392544965], time: 44.852
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -15.867732719184902, agent episode reward: [-3.3524348735504663, -12.515297845634434], time: 45.075
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.603481556060716, agent episode reward: [-2.9401705295520655, -12.663311026508651], time: 45.766
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -15.866146457263168, agent episode reward: [-2.686646498128908, -13.17949995913426], time: 45.727
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -15.93360856099951, agent episode reward: [-2.801821850460665, -13.131786710538847], time: 44.651
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -15.491454050209597, agent episode reward: [-2.6061183137888784, -12.885335736420718], time: 44.706
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -16.058049640021597, agent episode reward: [-2.858173900966799, -13.199875739054798], time: 44.757
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -15.753263235184082, agent episode reward: [-2.726830915048068, -13.026432320136013], time: 44.382
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -15.938580463278635, agent episode reward: [-3.032158934618732, -12.906421528659903], time: 45.074
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -16.09126947407459, agent episode reward: [-3.083331054547395, -13.007938419527195], time: 44.565
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -16.02332759956083, agent episode reward: [-2.980606284261642, -13.04272131529919], time: 45.881
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.691009456259387, agent episode reward: [-2.8502653181545994, -12.840744138104789], time: 45.711
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -15.907003947813843, agent episode reward: [-3.15834654244816, -12.74865740536568], time: 45.42
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -15.867648672896426, agent episode reward: [-3.1503860547395197, -12.717262618156907], time: 44.788
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.988336385350893, agent episode reward: [-3.252185941308514, -12.736150444042382], time: 45.904
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -16.473950836553776, agent episode reward: [-3.142197295585783, -13.331753540967995], time: 45.887
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -16.230996133043337, agent episode reward: [-3.157665755195477, -13.073330377847858], time: 44.937
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -16.076027189993262, agent episode reward: [-3.121431502159119, -12.954595687834143], time: 46.245
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -16.170356220319327, agent episode reward: [-3.4771873233858552, -12.693168896933473], time: 46.488
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -16.014463480557392, agent episode reward: [-3.315352180829607, -12.699111299727786], time: 46.397
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -16.173832409813244, agent episode reward: [-3.05772784317057, -13.116104566642676], time: 46.161
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -16.137474343236367, agent episode reward: [-3.433539379073746, -12.703934964162622], time: 46.208
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -16.26343911384802, agent episode reward: [-3.139472901622937, -13.123966212225083], time: 45.962
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -16.1265590393682, agent episode reward: [-2.9825228890060185, -13.144036150362183], time: 46.431
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -16.300914344849037, agent episode reward: [-3.3313421547567463, -12.969572190092288], time: 45.265
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -16.026669653049794, agent episode reward: [-3.367404359470181, -12.659265293579613], time: 44.93
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -16.337671061747805, agent episode reward: [-3.3707039152140825, -12.966967146533722], time: 45.642
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -16.142594908444927, agent episode reward: [-3.434284953062444, -12.708309955382482], time: 46.097
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.832787061238928, agent episode reward: [-3.4109955800655944, -12.421791481173335], time: 44.602
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -16.064249987847422, agent episode reward: [-3.6087500153702137, -12.45549997247721], time: 45.611
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -16.29150491086572, agent episode reward: [-3.3235991344302165, -12.967905776435506], time: 45.351
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.884384375639723, agent episode reward: [-3.7167340635686577, -12.167650312071066], time: 45.995
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -16.269683483408052, agent episode reward: [-3.940988813168095, -12.328694670239955], time: 46.703
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -16.08025586734083, agent episode reward: [-3.5544678829580727, -12.525787984382758], time: 46.823
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -16.248752252040745, agent episode reward: [-3.597293504830045, -12.651458747210699], time: 46.229
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.979317050427944, agent episode reward: [-3.7698025165696305, -12.209514533858313], time: 45.501
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -16.24530951542617, agent episode reward: [-3.837801337060035, -12.407508178366138], time: 46.241
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -16.363145813825664, agent episode reward: [-3.862654400968555, -12.500491412857107], time: 46.428
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -16.542459553022027, agent episode reward: [-3.972521312296685, -12.569938240725344], time: 46.452
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -16.46175955531516, agent episode reward: [-3.9945618631047073, -12.467197692210455], time: 46.777
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -16.30141644327068, agent episode reward: [-3.836326548776175, -12.4650898944945], time: 45.852
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -16.382555688152102, agent episode reward: [-3.967267337749011, -12.41528835040309], time: 45.577
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -16.176856067966924, agent episode reward: [-3.795819723767003, -12.381036344199922], time: 46.751
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.895142397951851, agent episode reward: [-3.553871873869703, -12.34127052408215], time: 47.064
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -16.10516458660125, agent episode reward: [-3.578507727470123, -12.52665685913113], time: 45.751
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -16.38783753729458, agent episode reward: [-3.807517179819754, -12.580320357474829], time: 46.692
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -16.291801102566144, agent episode reward: [-3.809937552017422, -12.481863550548718], time: 46.652
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -16.072051236726864, agent episode reward: [-3.371122272832773, -12.70092896389409], time: 45.758
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -16.370213492729288, agent episode reward: [-3.7328852964906996, -12.637328196238588], time: 46.259
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -16.402592097520046, agent episode reward: [-3.8812650078960327, -12.521327089624014], time: 46.47
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -16.463176044529504, agent episode reward: [-4.031142539894273, -12.432033504635228], time: 45.17
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -16.228024770631382, agent episode reward: [-3.7160490771313026, -12.51197569350008], time: 45.988
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -16.39922979082817, agent episode reward: [-3.696877267546659, -12.702352523281508], time: 46.345
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -16.141715257515514, agent episode reward: [-3.921070110988449, -12.220645146527069], time: 46.449
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.94388753931914, agent episode reward: [-3.377921916202061, -12.565965623117078], time: 45.788
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -16.093397435877705, agent episode reward: [-3.5743021154978107, -12.519095320379895], time: 45.682
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -16.157988579267546, agent episode reward: [-3.5685317928418727, -12.589456786425673], time: 46.883
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -16.059125737302054, agent episode reward: [-3.8245171790185677, -12.234608558283485], time: 45.984
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -16.163722039024456, agent episode reward: [-3.4690726727528025, -12.694649366271653], time: 46.745
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.95962213575835, agent episode reward: [-3.357710927408735, -12.601911208349613], time: 45.289
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -16.0349662727586, agent episode reward: [-3.5878630853566857, -12.447103187401913], time: 46.667
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -16.19512609970755, agent episode reward: [-3.533313882386363, -12.661812217321184], time: 45.87
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -16.04134714981965, agent episode reward: [-3.226719575615585, -12.814627574204064], time: 45.855
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -16.438149105689952, agent episode reward: [-3.924221035810133, -12.513928069879817], time: 45.912
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.763250362302278, agent episode reward: [-3.4096108604725015, -12.353639501829777], time: 46.462
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -16.427784638384896, agent episode reward: [-3.7155621136786157, -12.712222524706275], time: 45.936
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -16.052940949867484, agent episode reward: [-3.5363201162481523, -12.516620833619333], time: 46.842
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -16.02211658114711, agent episode reward: [-3.5305983862417505, -12.491518194905359], time: 46.874
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.921252468521562, agent episode reward: [-3.5129238989980665, -12.408328569523494], time: 46.764
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -16.228879401062844, agent episode reward: [-3.5240256915659947, -12.70485370949685], time: 45.657
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -16.13793850116541, agent episode reward: [-3.559316579528526, -12.578621921636888], time: 45.473
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -16.040283090808842, agent episode reward: [-3.621224754511517, -12.419058336297327], time: 46.39
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -16.084571028620662, agent episode reward: [-3.5380484830216283, -12.546522545599034], time: 45.88
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.99951253584912, agent episode reward: [-3.2501124363747205, -12.7494000994744], time: 44.95
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -16.330164724265423, agent episode reward: [-3.775414738916027, -12.554749985349394], time: 45.962
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -16.184540773285224, agent episode reward: [-3.561261479079193, -12.62327929420603], time: 42.135
...Finished total of 100001 episodes.
