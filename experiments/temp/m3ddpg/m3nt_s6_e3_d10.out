0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.69351419556976, agent episode reward: [1.0274344012741545, -27.720948596843915], time: 35.195
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.018711127584126, agent episode reward: [-6.11687669352744, -15.901834434056687], time: 48.159
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.921934518343132, agent episode reward: [-5.080168274153835, -8.841766244189296], time: 46.713
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.52911066320192, agent episode reward: [-4.058658487622688, -8.470452175579231], time: 46.46
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.857607853785943, agent episode reward: [-3.7017523759813464, -9.155855477804595], time: 46.127
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.525211075661348, agent episode reward: [-3.6947867870084194, -9.830424288652928], time: 46.537
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.818910344827097, agent episode reward: [-3.858205972499769, -9.96070437232733], time: 46.607
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -14.317553539095018, agent episode reward: [-3.7743524412811844, -10.543201097813833], time: 46.392
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -14.157545045808703, agent episode reward: [-3.6099852123265226, -10.54755983348218], time: 46.483
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.215830407531241, agent episode reward: [-3.6540489215327954, -10.561781485998447], time: 45.597
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -13.933831796113253, agent episode reward: [-3.5869020155488496, -10.346929780564404], time: 46.005
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.278117235065423, agent episode reward: [-3.872475008270534, -10.405642226794889], time: 45.539
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.14924058466719, agent episode reward: [-3.719892004855744, -10.429348579811446], time: 46.359
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.459710044467645, agent episode reward: [-3.6825477739539783, -10.777162270513669], time: 45.887
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.27205967589461, agent episode reward: [-3.547880888818523, -10.724178787076086], time: 45.883
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.012704534893201, agent episode reward: [-3.145550975127985, -10.867153559765214], time: 46.482
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.184456979208267, agent episode reward: [-3.708222642872137, -10.476234336336132], time: 46.972
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.09718891995857, agent episode reward: [-3.708663213359193, -10.38852570659938], time: 46.581
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.41786755247223, agent episode reward: [-3.918135681983147, -10.499731870489084], time: 47.034
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.56546812675308, agent episode reward: [-3.9192064370148274, -10.646261689738251], time: 46.584
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.613188898880427, agent episode reward: [-3.7990679188574497, -10.814120980022977], time: 47.899
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.473279550225438, agent episode reward: [-3.6601358629666896, -10.813143687258748], time: 48.677
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.437588144454905, agent episode reward: [-3.7337504008620472, -10.703837743592857], time: 48.118
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.71988691876041, agent episode reward: [-3.6720452159759818, -11.047841702784428], time: 48.29
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.38853301481693, agent episode reward: [-3.4915846332331353, -10.896948381583796], time: 48.14
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -14.68395348796303, agent episode reward: [-3.691683648318737, -10.99226983964429], time: 46.655
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -14.770186965821507, agent episode reward: [-3.830905892586529, -10.939281073234978], time: 48.117
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -14.52471671822947, agent episode reward: [-3.896820617102555, -10.627896101126915], time: 48.58
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.815586541553927, agent episode reward: [-4.029190410223891, -10.786396131330035], time: 48.222
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.948293786985568, agent episode reward: [-4.021531577368184, -10.926762209617385], time: 48.752
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -14.8098332393926, agent episode reward: [-3.999095392653433, -10.810737846739165], time: 47.161
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -14.864080640442069, agent episode reward: [-3.938401301577829, -10.92567933886424], time: 47.777
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -14.761093144232337, agent episode reward: [-3.78222785965988, -10.978865284572455], time: 48.652
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -14.687846565119573, agent episode reward: [-3.858472476392816, -10.829374088726757], time: 49.116
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -14.730628313229369, agent episode reward: [-3.7704899829292, -10.960138330300168], time: 48.542
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.02206493166975, agent episode reward: [-4.2688271241349485, -10.753237807534799], time: 47.42
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -14.849480965832406, agent episode reward: [-3.6907113280608947, -11.158769637771512], time: 47.758
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -14.937620810782917, agent episode reward: [-3.898720595861307, -11.038900214921611], time: 47.073
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -14.696520414378027, agent episode reward: [-3.657112123830612, -11.039408290547415], time: 45.303
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -14.701079733295897, agent episode reward: [-3.987056450132968, -10.714023283162932], time: 47.818
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -14.620816905130075, agent episode reward: [-3.478617790950575, -11.142199114179501], time: 47.147
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -14.885279254730928, agent episode reward: [-3.6678920121594967, -11.217387242571432], time: 47.276
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -14.802092384882629, agent episode reward: [-3.6332524492078337, -11.168839935674798], time: 47.787
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -14.876512135310655, agent episode reward: [-3.847685845013144, -11.028826290297511], time: 46.734
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -14.874242876499633, agent episode reward: [-3.3438001684593117, -11.530442708040322], time: 46.674
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -14.908178100561985, agent episode reward: [-3.4768418319507375, -11.43133626861125], time: 46.751
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -14.942932439189244, agent episode reward: [-3.4370661001973564, -11.505866338991888], time: 46.706
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -15.149024545270343, agent episode reward: [-3.689648117269899, -11.459376428000446], time: 47.037
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -14.558823355148881, agent episode reward: [-3.0055665478136184, -11.553256807335263], time: 46.99
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.10928463734988, agent episode reward: [-3.2483450855934026, -11.860939551756479], time: 47.706
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -14.911852503314318, agent episode reward: [-3.2994577244117953, -11.612394778902523], time: 48.248
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -15.067947426502272, agent episode reward: [-3.765981033747354, -11.301966392754919], time: 47.247
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -15.277508599518576, agent episode reward: [-3.557961944405636, -11.71954665511294], time: 47.882
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -14.95629890857491, agent episode reward: [-3.163332050462383, -11.792966858112527], time: 47.164
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -14.61570631044732, agent episode reward: [-3.2690546878833633, -11.346651622563957], time: 47.209
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -14.790739561502814, agent episode reward: [-3.5864591936844405, -11.204280367818374], time: 47.597
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.115405038864242, agent episode reward: [-4.185511965088075, -10.929893073776165], time: 47.484
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -14.773895591325648, agent episode reward: [-3.85319606253707, -10.920699528788576], time: 47.503
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -14.852723055932724, agent episode reward: [-3.7934310727561757, -11.059291983176546], time: 46.971
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -14.96854982293582, agent episode reward: [-3.7837792794322542, -11.184770543503566], time: 46.271
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.006788266908714, agent episode reward: [-3.756338268457793, -11.25044999845092], time: 46.006
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.116118730738538, agent episode reward: [-3.9276355991180045, -11.188483131620535], time: 46.066
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -14.847424851006986, agent episode reward: [-3.4395557311736455, -11.40786911983334], time: 46.629
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.139190276173183, agent episode reward: [-3.7834483424430427, -11.35574193373014], time: 47.526
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.266932506258202, agent episode reward: [-3.805551574863133, -11.46138093139507], time: 47.614
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.097757425158356, agent episode reward: [-3.891730362889094, -11.206027062269262], time: 48.094
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.122078576752868, agent episode reward: [-3.6070137555118653, -11.515064821241005], time: 47.676
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.25543168705899, agent episode reward: [-4.233757480398408, -11.021674206660585], time: 47.953
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.05360763200633, agent episode reward: [-3.9275071650039735, -11.126100467002356], time: 47.949
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.567187500657159, agent episode reward: [-4.255985172159164, -11.311202328497995], time: 47.411
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.479905126194058, agent episode reward: [-4.263325935367013, -11.216579190827048], time: 47.657
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -14.928413659794838, agent episode reward: [-3.8063882621902585, -11.122025397604581], time: 47.705
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.55009255829631, agent episode reward: [-4.273628290886597, -11.276464267409713], time: 47.65
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.344604434698637, agent episode reward: [-3.807309051883598, -11.537295382815039], time: 47.768
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.370358608440105, agent episode reward: [-4.285636904029319, -11.084721704410786], time: 47.367
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.345169633240443, agent episode reward: [-4.215666688112275, -11.129502945128166], time: 47.969
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -15.461603714533371, agent episode reward: [-4.527133455294668, -10.934470259238704], time: 47.403
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.525514441803384, agent episode reward: [-4.471115828370684, -11.0543986134327], time: 47.457
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.586808173079438, agent episode reward: [-4.747827774829522, -10.838980398249918], time: 47.68
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.544054268631502, agent episode reward: [-4.2713045344842575, -11.272749734147242], time: 47.645
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -15.661406250836961, agent episode reward: [-4.088892294708812, -11.57251395612815], time: 47.189
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.481326673731251, agent episode reward: [-4.185188867385068, -11.29613780634618], time: 47.883
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -15.495777248193615, agent episode reward: [-4.004025047717238, -11.491752200476379], time: 48.395
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.460832364204093, agent episode reward: [-4.424735187493982, -11.036097176710111], time: 47.406
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.75415675804079, agent episode reward: [-4.370974482002338, -11.383182276038449], time: 47.468
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -15.204759397515048, agent episode reward: [-4.310712515155291, -10.894046882359758], time: 48.835
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.016513859511754, agent episode reward: [-3.7889002715152276, -11.227613587996524], time: 48.827
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.091178350228653, agent episode reward: [-4.029551751426115, -11.06162659880254], time: 48.222
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.084213722072434, agent episode reward: [-3.882666249074493, -11.201547472997941], time: 47.555
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.044967849765351, agent episode reward: [-3.831587467431845, -11.213380382333508], time: 48.212
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.10907299609099, agent episode reward: [-3.8121841714252294, -11.296888824665759], time: 47.634
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.142135175665521, agent episode reward: [-3.6080598474379824, -11.534075328227537], time: 47.648
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.135058250148905, agent episode reward: [-3.4975784996038284, -11.637479750545078], time: 47.729
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -14.91599425161182, agent episode reward: [-3.8611393644233845, -11.054854887188437], time: 47.893
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -14.82080342737834, agent episode reward: [-3.5943845609244773, -11.226418866453862], time: 47.488
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -14.729791357689832, agent episode reward: [-3.4364575030685676, -11.293333854621265], time: 47.63
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.127786593989336, agent episode reward: [-3.8162973620487954, -11.311489231940541], time: 48.088
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.0107063986724, agent episode reward: [-3.852724754044104, -11.157981644628297], time: 47.93
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -14.874699393815298, agent episode reward: [-3.725673405538795, -11.149025988276504], time: 47.569
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.251337538666618, agent episode reward: [-3.658987867858024, -11.592349670808595], time: 47.254
...Finished total of 100001 episodes.
