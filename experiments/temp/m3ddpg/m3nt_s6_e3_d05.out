0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.572942421768143, agent episode reward: [0.07410950501469415, -26.64705192678284], time: 32.505
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.209075913261724, agent episode reward: [-4.472022077299731, -17.737053835961994], time: 45.548
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.28488592104607, agent episode reward: [-5.01185343935927, -8.273032481686801], time: 45.289
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -11.555135103619302, agent episode reward: [-3.28048270103703, -8.274652402582271], time: 45.472
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -11.46381515722667, agent episode reward: [-2.938920221849123, -8.524894935377544], time: 45.017
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -11.558602879898384, agent episode reward: [-2.7723825901872887, -8.786220289711094], time: 45.296
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -11.867370565295015, agent episode reward: [-2.9489859623350023, -8.918384602960014], time: 44.828
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -11.99238218077979, agent episode reward: [-2.9617588731879287, -9.030623307591863], time: 45.249
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -12.144392721748579, agent episode reward: [-3.187199491138746, -8.957193230609834], time: 45.937
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -11.963183641050584, agent episode reward: [-2.9784375843379047, -8.984746056712677], time: 45.322
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -11.738147376349135, agent episode reward: [-2.758861965149148, -8.979285411199985], time: 44.817
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -12.119311309592137, agent episode reward: [-2.917167825975454, -9.202143483616682], time: 45.317
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -11.951283168195191, agent episode reward: [-2.9182640965441538, -9.03301907165104], time: 45.55
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -12.176320344016109, agent episode reward: [-2.914720909592008, -9.2615994344241], time: 46.701
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -11.929181989036664, agent episode reward: [-3.1707239398946863, -8.758458049141979], time: 45.472
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -12.136529877499843, agent episode reward: [-3.1031846959330185, -9.033345181566823], time: 46.326
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -12.240667906673629, agent episode reward: [-3.0774364148448647, -9.163231491828764], time: 46.249
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -12.118143565518727, agent episode reward: [-3.183444571875937, -8.934698993642789], time: 46.098
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -12.28884690245353, agent episode reward: [-3.2287702540069003, -9.06007664844663], time: 46.336
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -12.593785814062052, agent episode reward: [-3.4825177031513133, -9.11126811091074], time: 45.609
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -12.378373397126849, agent episode reward: [-3.204260593467657, -9.174112803659192], time: 45.449
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -12.21254064201227, agent episode reward: [-3.218374531848962, -8.994166110163308], time: 46.627
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -12.587897350898734, agent episode reward: [-3.3709654244417515, -9.216931926456983], time: 46.347
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -12.65934049639296, agent episode reward: [-3.423144314055022, -9.236196182337938], time: 45.359
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -12.125211004676988, agent episode reward: [-2.9993599275457936, -9.125851077131196], time: 45.732
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -12.085037753356106, agent episode reward: [-2.9408778095522847, -9.144159943803817], time: 46.382
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -12.531234895405836, agent episode reward: [-3.354871268653012, -9.176363626752824], time: 45.959
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -12.254832092677143, agent episode reward: [-3.001523414383536, -9.253308678293608], time: 46.095
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -12.464661840626075, agent episode reward: [-3.101445449745369, -9.363216390880705], time: 46.298
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -12.648645781618477, agent episode reward: [-3.242106057377786, -9.406539724240691], time: 46.136
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -12.54423616802644, agent episode reward: [-3.1786187829229187, -9.365617385103523], time: 46.883
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -12.770542082188232, agent episode reward: [-3.3928999578114483, -9.377642124376786], time: 46.146
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -12.579161128035558, agent episode reward: [-3.398235517680989, -9.180925610354569], time: 45.721
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -13.142811946468187, agent episode reward: [-3.5163570894174856, -9.626454857050703], time: 46.451
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -12.971187357068972, agent episode reward: [-3.5756597841987157, -9.395527572870254], time: 46.857
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -12.831209678065235, agent episode reward: [-3.234259488635076, -9.59695018943016], time: 45.294
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -13.014473005114509, agent episode reward: [-3.4599633246513997, -9.554509680463111], time: 45.928
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -12.343146974131503, agent episode reward: [-3.025013962938696, -9.31813301119281], time: 46.208
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -12.974037429283237, agent episode reward: [-3.5689399879219708, -9.405097441361265], time: 45.682
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -12.828392433957235, agent episode reward: [-3.3129155641643138, -9.515476869792922], time: 46.714
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -12.577132313112022, agent episode reward: [-3.4967350376327735, -9.080397275479248], time: 46.585
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -12.274989307672916, agent episode reward: [-3.2380421856652273, -9.036947122007692], time: 45.694
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -13.098059217883383, agent episode reward: [-3.6744766359498096, -9.423582581933573], time: 46.171
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -12.734186260966098, agent episode reward: [-3.2423872491745853, -9.491799011791514], time: 47.161
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -13.031086576481531, agent episode reward: [-3.288709051551713, -9.74237752492982], time: 46.979
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -13.03240102291031, agent episode reward: [-3.2494097868770977, -9.782991236033213], time: 46.37
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -13.12950832209772, agent episode reward: [-3.171832560364729, -9.95767576173299], time: 46.551
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -13.446763347038724, agent episode reward: [-3.3961411006659263, -10.050622246372797], time: 46.611
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -13.297015416899745, agent episode reward: [-3.5360712323508037, -9.760944184548942], time: 46.138
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -13.224442380200577, agent episode reward: [-3.208537623485604, -10.015904756714972], time: 46.094
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -13.030096584490687, agent episode reward: [-3.306673304383106, -9.723423280107582], time: 46.093
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -13.119797618710109, agent episode reward: [-3.6056299225225743, -9.514167696187535], time: 46.263
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -13.236573160398896, agent episode reward: [-3.5363026441527534, -9.700270516246142], time: 46.748
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -13.285251054631155, agent episode reward: [-3.5675966126643806, -9.717654441966774], time: 45.829
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -13.295769189746316, agent episode reward: [-3.6866059191253586, -9.609163270620957], time: 46.512
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -12.846913761286507, agent episode reward: [-3.270849517993655, -9.576064243292853], time: 45.915
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -12.99308573071389, agent episode reward: [-3.4532729182509057, -9.539812812462984], time: 47.524
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -13.112835832171502, agent episode reward: [-3.5461334714355908, -9.566702360735912], time: 46.81
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -13.22134941400191, agent episode reward: [-3.4610952804281117, -9.760254133573799], time: 47.044
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -12.893550665962422, agent episode reward: [-3.4376948093473785, -9.455855856615045], time: 47.275
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -13.048929987346744, agent episode reward: [-3.3600952756895923, -9.688834711657153], time: 47.499
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -12.939284399842306, agent episode reward: [-3.299047536459427, -9.640236863382878], time: 46.476
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -13.316299092469851, agent episode reward: [-3.7876048649611382, -9.528694227508714], time: 46.852
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -12.990459148840609, agent episode reward: [-3.7669131013684174, -9.22354604747219], time: 47.157
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -13.16712633673373, agent episode reward: [-3.5813611471572115, -9.585765189576522], time: 46.971
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -13.082419881702291, agent episode reward: [-3.434693318403526, -9.647726563298766], time: 46.583
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -13.455919996581157, agent episode reward: [-3.566999503279644, -9.88892049330151], time: 45.84
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -13.374695894795984, agent episode reward: [-3.657793341459148, -9.716902553336839], time: 47.763
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -13.163039899087554, agent episode reward: [-3.4551534265564006, -9.707886472531154], time: 47.035
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -13.33451492965787, agent episode reward: [-3.6487448016164716, -9.685770128041396], time: 46.577
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -13.23512438103403, agent episode reward: [-3.6292196470955704, -9.605904733938457], time: 47.16
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -13.092980904592203, agent episode reward: [-3.4454258215101072, -9.647555083082098], time: 47.013
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -13.23223497304205, agent episode reward: [-3.24186231284253, -9.990372660199519], time: 46.179
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -13.176664725502858, agent episode reward: [-3.6395432186578027, -9.537121506845057], time: 47.469
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -13.103999246806755, agent episode reward: [-3.4719061667322824, -9.632093080074473], time: 46.426
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -13.272273234018062, agent episode reward: [-3.288712591848499, -9.983560642169563], time: 45.837
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -13.236032998745719, agent episode reward: [-3.5228386700021472, -9.713194328743572], time: 45.743
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -13.122379266388096, agent episode reward: [-3.382279214693551, -9.740100051694542], time: 45.975
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -13.362461089842647, agent episode reward: [-3.462270841871828, -9.90019024797082], time: 46.761
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -13.533248386882477, agent episode reward: [-3.71368970138464, -9.819558685497839], time: 46.536
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -13.394271384518616, agent episode reward: [-3.524401711598467, -9.869869672920148], time: 47.275
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -13.399455383916058, agent episode reward: [-3.570648635645967, -9.82880674827009], time: 46.477
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -13.192030990350927, agent episode reward: [-3.502892138391648, -9.68913885195928], time: 46.901
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -13.189394220264969, agent episode reward: [-3.504551404083834, -9.684842816181131], time: 46.811
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -13.171885688642869, agent episode reward: [-3.3246265333249703, -9.847259155317898], time: 47.205
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -13.378511675251673, agent episode reward: [-3.670816208104753, -9.707695467146921], time: 47.491
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -13.495525533355854, agent episode reward: [-3.777724101308536, -9.71780143204732], time: 46.819
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -13.593000412086651, agent episode reward: [-3.3978805290155383, -10.195119883071115], time: 47.201
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -13.511894706768679, agent episode reward: [-3.566329107719945, -9.945565599048733], time: 46.452
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -13.813617993435408, agent episode reward: [-3.5042469841124384, -10.309371009322971], time: 46.181
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -13.73591307689449, agent episode reward: [-3.6452434407927807, -10.09066963610171], time: 46.022
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -13.554415687723443, agent episode reward: [-3.705492177632817, -9.848923510090625], time: 46.31
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -13.516058199961673, agent episode reward: [-3.5967417398281634, -9.91931646013351], time: 46.835
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -13.652081611721924, agent episode reward: [-3.541474747182514, -10.110606864539411], time: 46.492
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -13.644646602144585, agent episode reward: [-3.7490742611486185, -9.895572340995969], time: 47.059
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -13.662588727642422, agent episode reward: [-3.761382883125562, -9.90120584451686], time: 45.547
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -13.547978018691422, agent episode reward: [-3.43055186508719, -10.117426153604232], time: 45.458
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -13.506323384115293, agent episode reward: [-4.108956932660639, -9.397366451454655], time: 46.381
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -13.253444458947467, agent episode reward: [-3.720687822374284, -9.532756636573183], time: 46.69
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -13.547629883508185, agent episode reward: [-4.0011923666852, -9.546437516822985], time: 43.874
...Finished total of 100001 episodes.
