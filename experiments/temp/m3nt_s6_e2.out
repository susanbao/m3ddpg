0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.79607764805184, agent episode reward: [-0.7698169550385845, -26.026260693013253], time: 30.392
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -19.157263228562584, agent episode reward: [-2.9625842726964304, -16.194678955866152], time: 47.848
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.679686795736812, agent episode reward: [-4.539874213192031, -9.13981258254478], time: 46.257
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -12.508934699844364, agent episode reward: [-3.872775851199199, -8.636158848645165], time: 46.64
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -12.20171458410731, agent episode reward: [-3.2128497999803085, -8.988864784127], time: 47.02
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.065483030432967, agent episode reward: [-3.5236889349379097, -9.541794095495053], time: 45.881
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.75993144208216, agent episode reward: [-3.875549405528566, -9.884382036553594], time: 46.244
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -13.880859739218868, agent episode reward: [-3.3497485179952666, -10.5311112212236], time: 46.328
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -13.976274557952578, agent episode reward: [-3.312509667284125, -10.663764890668453], time: 46.013
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.370699522489431, agent episode reward: [-3.9106628231109215, -10.46003669937851], time: 45.867
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -14.055404731146975, agent episode reward: [-3.698491195230835, -10.35691353591614], time: 46.351
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -14.527521196389982, agent episode reward: [-3.988591388227876, -10.538929808162106], time: 47.204
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -14.390776599037567, agent episode reward: [-3.8154287436268537, -10.57534785541071], time: 46.648
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.88028877333695, agent episode reward: [-4.173225538521756, -10.707063234815196], time: 46.441
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -14.276622441859887, agent episode reward: [-3.5090135020414635, -10.767608939818423], time: 47.086
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -14.208228286407026, agent episode reward: [-3.740885556780767, -10.467342729626258], time: 46.489
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -14.657750341937229, agent episode reward: [-3.9282497276083994, -10.729500614328831], time: 46.163
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -14.488684421723864, agent episode reward: [-3.866093772208343, -10.62259064951552], time: 47.106
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -14.831724527554575, agent episode reward: [-3.9680569724641273, -10.86366755509045], time: 47.547
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -14.734396230417785, agent episode reward: [-3.866843454118584, -10.8675527762992], time: 47.729
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -14.867436777303231, agent episode reward: [-4.07720051516377, -10.790236262139464], time: 47.837
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -14.547199456845972, agent episode reward: [-3.7623455404068578, -10.784853916439115], time: 48.531
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -14.826526419357437, agent episode reward: [-3.864526339996324, -10.962000079361115], time: 48.22
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -14.670972629851848, agent episode reward: [-3.7443453437808993, -10.926627286070946], time: 46.637
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -14.504463058572888, agent episode reward: [-3.789688300370246, -10.71477475820264], time: 47.938
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -14.619673909006552, agent episode reward: [-3.912096914014167, -10.707576994992385], time: 47.618
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -15.028002721534328, agent episode reward: [-3.8326619778790545, -11.195340743655274], time: 47.731
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -14.309895148018034, agent episode reward: [-3.657478562520336, -10.652416585497699], time: 48.403
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -14.704794156994252, agent episode reward: [-3.628838655918538, -11.075955501075715], time: 48.953
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -14.556011707112296, agent episode reward: [-3.838940977221577, -10.717070729890718], time: 48.987
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -15.160651895553302, agent episode reward: [-4.101933057320743, -11.058718838232558], time: 48.845
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -14.920490908141677, agent episode reward: [-3.7111348502480324, -11.209356057893647], time: 48.332
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -14.737534851712617, agent episode reward: [-3.565566419590095, -11.17196843212252], time: 47.633
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -14.935771758429, agent episode reward: [-3.7157470434421986, -11.2200247149868], time: 48.189
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -14.605851279494537, agent episode reward: [-3.75376338540656, -10.852087894087974], time: 48.543
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -15.003108836087641, agent episode reward: [-3.8181807243690518, -11.184928111718586], time: 48.059
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -14.682964933259697, agent episode reward: [-3.6185204079062263, -11.06444452535347], time: 46.831
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -14.46560795306434, agent episode reward: [-3.332658541202797, -11.132949411861544], time: 46.891
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -14.728616318432792, agent episode reward: [-3.8951465522706976, -10.833469766162093], time: 47.767
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -14.856592596879109, agent episode reward: [-3.7160351256580095, -11.1405574712211], time: 46.569
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -14.954555659338341, agent episode reward: [-3.666098573516843, -11.288457085821495], time: 45.272
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -14.639945909949525, agent episode reward: [-3.413074984379263, -11.226870925570262], time: 47.088
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -14.84186901605874, agent episode reward: [-3.4946175879488433, -11.347251428109896], time: 46.753
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -14.907465455273025, agent episode reward: [-3.3250120888289447, -11.582453366444078], time: 46.489
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -15.216718534840213, agent episode reward: [-3.4336297219533796, -11.783088812886835], time: 46.487
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -15.114621002029901, agent episode reward: [-3.4417219589763937, -11.672899043053507], time: 47.068
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -14.931159833456254, agent episode reward: [-3.2081370223235792, -11.723022811132676], time: 47.327
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -14.957289057545418, agent episode reward: [-3.067699822991376, -11.889589234554043], time: 47.602
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -15.337136732221358, agent episode reward: [-3.4051446244371393, -11.93199210778422], time: 47.113
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -15.291873943620972, agent episode reward: [-3.4235537478595615, -11.868320195761411], time: 48.444
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -14.929920642428431, agent episode reward: [-3.3822267643480073, -11.547693878080423], time: 47.875
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -14.884020077113195, agent episode reward: [-3.3436154244227727, -11.540404652690421], time: 47.58
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -14.960702325056142, agent episode reward: [-3.233531963153284, -11.727170361902859], time: 47.853
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -15.345042697839048, agent episode reward: [-3.412987140790655, -11.932055557048391], time: 47.819
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -15.209738924620957, agent episode reward: [-3.8814392151131076, -11.32829970950785], time: 47.429
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -15.438594499790035, agent episode reward: [-3.6501825489206987, -11.788411950869337], time: 47.455
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -15.147833288307469, agent episode reward: [-3.5738849493240017, -11.573948338983469], time: 47.679
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -15.207093939988475, agent episode reward: [-3.2686733531604517, -11.938420586828022], time: 47.67
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -15.362078533712504, agent episode reward: [-3.836316707966385, -11.525761825746118], time: 47.776
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.340450910269848, agent episode reward: [-3.815723109380183, -11.524727800889664], time: 47.49
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -15.407170014979824, agent episode reward: [-4.084952291740386, -11.322217723239437], time: 48.342
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: -15.678107531931582, agent episode reward: [-3.9895627102986633, -11.68854482163292], time: 48.711
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: -15.20717438332465, agent episode reward: [-3.68974325538383, -11.51743112794082], time: 49.369
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: -15.460912975421678, agent episode reward: [-3.7285201010387374, -11.732392874382938], time: 47.844
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: -15.38991865120169, agent episode reward: [-4.027234240229247, -11.36268441097244], time: 47.776
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: -15.588895737329477, agent episode reward: [-4.0858727869070055, -11.503022950422471], time: 47.745
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -15.412498474425366, agent episode reward: [-4.192214254521054, -11.220284219904313], time: 47.906
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: -15.807889615031204, agent episode reward: [-4.149629820020084, -11.658259795011121], time: 47.776
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: -15.096038862598228, agent episode reward: [-3.690361036720932, -11.405677825877296], time: 47.699
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: -15.652777665559952, agent episode reward: [-3.7827419702694955, -11.870035695290454], time: 47.453
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: -15.415706954918035, agent episode reward: [-3.857110139396243, -11.55859681552179], time: 47.201
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: -15.740959867188868, agent episode reward: [-3.978098795577487, -11.762861071611379], time: 48.009
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: -15.23366006576044, agent episode reward: [-3.8979331839093385, -11.3357268818511], time: 49.353
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: -15.244069187088323, agent episode reward: [-4.0050689637562344, -11.23900022333209], time: 48.071
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: -15.449828103196104, agent episode reward: [-4.027962547923591, -11.421865555272515], time: 47.615
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: -15.558831674112513, agent episode reward: [-4.2897664117306284, -11.269065262381883], time: 47.767
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: -15.195724718401758, agent episode reward: [-4.190309545772577, -11.005415172629178], time: 48.191
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: -15.386575870635493, agent episode reward: [-4.340486551974963, -11.046089318660531], time: 47.14
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: -15.201656079290403, agent episode reward: [-4.1111117493112, -11.090544329979206], time: 48.073
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: -15.153590581426648, agent episode reward: [-4.465745674715464, -10.687844906711184], time: 47.724
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: -15.251966634053295, agent episode reward: [-4.518595261827366, -10.733371372225928], time: 47.926
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: -15.307657809955538, agent episode reward: [-4.446734485060375, -10.860923324895163], time: 48.241
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: -14.965153914754119, agent episode reward: [-4.312148588363682, -10.653005326390437], time: 48.307
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: -15.437414621967589, agent episode reward: [-4.288812390603537, -11.148602231364054], time: 47.772
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: -15.068358909882473, agent episode reward: [-3.9202627462237527, -11.14809616365872], time: 47.379
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: -15.336692651141918, agent episode reward: [-3.90717666202951, -11.429515989112408], time: 47.914
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: -15.257304617577143, agent episode reward: [-4.12220366196292, -11.135100955614224], time: 47.557
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: -15.003752671584403, agent episode reward: [-3.945717764360889, -11.058034907223515], time: 47.553
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: -15.152843656910125, agent episode reward: [-4.371347635635342, -10.781496021274785], time: 47.983
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: -15.242020737960395, agent episode reward: [-3.7589600159972063, -11.48306072196319], time: 47.807
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: -15.40471383923422, agent episode reward: [-3.7273049238537586, -11.677408915380463], time: 47.372
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: -15.356154989378437, agent episode reward: [-4.1274255015884025, -11.228729487790035], time: 47.583
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: -15.110777594734685, agent episode reward: [-4.122646977551245, -10.98813061718344], time: 47.947
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: -15.135631091652968, agent episode reward: [-4.137040833005434, -10.998590258647535], time: 47.853
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: -15.284962775621556, agent episode reward: [-3.8727446228189746, -11.412218152802582], time: 47.415
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: -15.636942000791805, agent episode reward: [-4.185980208781099, -11.450961792010709], time: 47.715
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: -15.260745893969753, agent episode reward: [-4.026461245282615, -11.234284648687138], time: 48.052
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: -15.441531762118089, agent episode reward: [-3.9230547727299725, -11.518476989388116], time: 47.712
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: -15.519843442116514, agent episode reward: [-4.179149461542075, -11.340693980574441], time: 47.932
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: -15.065691767847827, agent episode reward: [-3.691940539009557, -11.37375122883827], time: 46.908
...Finished total of 100001 episodes.
