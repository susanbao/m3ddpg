0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -22.08528408567341, agent episode reward: [-36.060583355117735, 6.98764963472216, 6.98764963472216], time: 14.362
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -15.870440507637223, agent episode reward: [-26.18615217711441, 5.157855834738592, 5.157855834738592], time: 40.935
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 1.611299172160544, agent episode reward: [-11.317593965894782, 6.464446569027663, 6.464446569027663], time: 40.148
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 2.232757942119941, agent episode reward: [-10.508493569100816, 6.370625755610378, 6.370625755610378], time: 40.485
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 1.8969376533070446, agent episode reward: [-9.495108291301774, 5.69602297230441, 5.69602297230441], time: 40.522
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 1.7063428330086459, agent episode reward: [-9.311065918355986, 5.508704375682316, 5.508704375682316], time: 40.27
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 1.0676598742592127, agent episode reward: [-9.88734356296519, 5.4775017186122, 5.4775017186122], time: 40.45
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 0.919665104676997, agent episode reward: [-9.66540164687972, 5.292533375778358, 5.292533375778358], time: 41.664
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 0.7344442593380698, agent episode reward: [-9.705797580676448, 5.2201209200072585, 5.2201209200072585], time: 40.43
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 0.8848434775982982, agent episode reward: [-9.967056279769203, 5.425949878683751, 5.425949878683751], time: 39.797
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -0.19119508626248, agent episode reward: [-9.269472270098387, 4.539138591917956, 4.539138591917956], time: 40.33
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 0.2860635587934891, agent episode reward: [-10.070186834795207, 5.178125196794349, 5.178125196794349], time: 40.876
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 0.6017091385566466, agent episode reward: [-9.77998235987601, 5.1908457492163285, 5.1908457492163285], time: 41.366
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 0.03177189312757028, agent episode reward: [-10.347686739901913, 5.189729316514742, 5.189729316514742], time: 42.437
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -0.47949738873793696, agent episode reward: [-11.062090201727454, 5.291296406494757, 5.291296406494757], time: 42.254
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -0.6137150505151023, agent episode reward: [-11.539512262550463, 5.4628986060176805, 5.4628986060176805], time: 42.307
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -0.7356009305372879, agent episode reward: [-12.453732056914912, 5.859065563188813, 5.859065563188813], time: 42.104
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -0.4795169069435698, agent episode reward: [-11.216475422396593, 5.368479257726512, 5.368479257726512], time: 41.961
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -0.5620839565996435, agent episode reward: [-11.824978336767641, 5.631447190084, 5.631447190084], time: 42.165
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -0.9365878491033407, agent episode reward: [-11.568597220231904, 5.316004685564281, 5.316004685564281], time: 42.802
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 0.08081241978493833, agent episode reward: [-11.619064094860146, 5.849938257322541, 5.849938257322541], time: 42.441
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -0.0114934532868524, agent episode reward: [-11.952637752186037, 5.970572149449593, 5.970572149449593], time: 41.048
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 0.13570075087669692, agent episode reward: [-11.978252141852495, 6.056976446364596, 6.056976446364596], time: 42.366
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -0.3731501311938363, agent episode reward: [-11.831688201931508, 5.729269035368836, 5.729269035368836], time: 42.398
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -0.14189171231453324, agent episode reward: [-12.871343341843701, 6.364725814764585, 6.364725814764585], time: 42.104
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 0.21060638853724362, agent episode reward: [-13.064069125096827, 6.637337756817035, 6.637337756817035], time: 42.982
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 0.10638285556532615, agent episode reward: [-13.317443327354194, 6.71191309145976, 6.71191309145976], time: 42.808
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -0.41450414547560865, agent episode reward: [-13.519705483712176, 6.5526006691182825, 6.5526006691182825], time: 41.818
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 0.00942159076166562, agent episode reward: [-12.706921064662948, 6.3581713277123075, 6.3581713277123075], time: 43.277
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 0.7575457243869912, agent episode reward: [-13.144335247843381, 6.950940486115186, 6.950940486115186], time: 43.478
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 0.559610474065238, agent episode reward: [-13.166633784081714, 6.863122129073477, 6.863122129073477], time: 42.808
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 1.7785668158863928, agent episode reward: [-13.14699935102984, 7.462783083458117, 7.462783083458117], time: 42.864
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 0.9347480322562747, agent episode reward: [-13.388550123646484, 7.161649077951378, 7.161649077951378], time: 42.759
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 1.2616452962481188, agent episode reward: [-13.725736378638238, 7.49369083744318, 7.49369083744318], time: 43.413
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 1.5397025037080387, agent episode reward: [-13.461434676761444, 7.500568590234741, 7.500568590234741], time: 41.528
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 1.9011635791335615, agent episode reward: [-13.275852762123025, 7.588508170628293, 7.588508170628293], time: 42.12
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 1.907637784105487, agent episode reward: [-13.5509225756269, 7.729280179866194, 7.729280179866194], time: 43.168
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 1.7338923018823411, agent episode reward: [-13.613203636081122, 7.673547968981731, 7.673547968981731], time: 43.123
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 2.5087344196431647, agent episode reward: [-13.694868864645043, 8.101801642144103, 8.101801642144103], time: 44.347
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 2.0229330260839404, agent episode reward: [-13.700866960882163, 7.861899993483051, 7.861899993483051], time: 43.634
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 1.6468210516989603, agent episode reward: [-14.472721373508715, 8.059771212603838, 8.059771212603838], time: 42.846
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 1.6549326292242998, agent episode reward: [-13.94172067528138, 7.79832665225284, 7.79832665225284], time: 43.039
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 2.0592195445996984, agent episode reward: [-13.933953802172951, 7.996586673386325, 7.996586673386325], time: 43.553
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 1.6501319995203312, agent episode reward: [-13.762133503542547, 7.706132751531439, 7.706132751531439], time: 43.72
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 1.625356937733563, agent episode reward: [-13.913048851971931, 7.7692028948527465, 7.7692028948527465], time: 44.243
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 1.3333499907580617, agent episode reward: [-13.316375939249339, 7.3248629650037005, 7.3248629650037005], time: 43.076
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 1.159676338488393, agent episode reward: [-13.17114954122125, 7.165412939854822, 7.165412939854822], time: 44.826
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 1.5029017718231577, agent episode reward: [-12.903040161094369, 7.2029709664587624, 7.2029709664587624], time: 44.117
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 0.9439518895256911, agent episode reward: [-12.929124447026373, 6.936538168276032, 6.936538168276032], time: 43.667
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 0.9608037309376544, agent episode reward: [-13.152168306888182, 7.056486018912918, 7.056486018912918], time: 43.42
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 0.8947277479791907, agent episode reward: [-12.833544824915622, 6.8641362864474065, 6.8641362864474065], time: 44.358
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 0.5127168806814644, agent episode reward: [-13.306764132572026, 6.909740506626745, 6.909740506626745], time: 43.755
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 0.734265376659642, agent episode reward: [-12.850440286161, 6.79235283141032, 6.79235283141032], time: 43.869
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 0.6646313106014862, agent episode reward: [-13.137601075800815, 6.90111619320115, 6.90111619320115], time: 43.092
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 1.2251629261850283, agent episode reward: [-12.849498799467542, 7.037330862826285, 7.037330862826285], time: 43.523
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 0.647890792731172, agent episode reward: [-13.020791790734396, 6.834341291732784, 6.834341291732784], time: 43.076
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 0.9383068773893892, agent episode reward: [-12.449958425764766, 6.694132651577077, 6.694132651577077], time: 43.476
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 0.7239796030474019, agent episode reward: [-12.433067007043345, 6.578523305045373, 6.578523305045373], time: 42.175
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 0.3648865700489383, agent episode reward: [-12.66120337443323, 6.513044972241084, 6.513044972241084], time: 42.111
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 0.1131353395252475, agent episode reward: [-12.577429214874634, 6.34528227719994, 6.34528227719994], time: 42.412
mmmaddpg vs mmmaddpg steps: 1524975, episodes: 61000, mean episode reward: -0.20198673475829365, agent episode reward: [-12.586665786098763, 6.192339525670234, 6.192339525670234], time: 42.519
mmmaddpg vs mmmaddpg steps: 1549975, episodes: 62000, mean episode reward: 0.4364654769928271, agent episode reward: [-12.625989515522779, 6.531227496257803, 6.531227496257803], time: 42.807
mmmaddpg vs mmmaddpg steps: 1574975, episodes: 63000, mean episode reward: 0.023981948520806512, agent episode reward: [-12.314385430965576, 6.16918368974319, 6.16918368974319], time: 42.573
mmmaddpg vs mmmaddpg steps: 1599975, episodes: 64000, mean episode reward: 0.2730375718741755, agent episode reward: [-11.96628869338783, 6.119663132631003, 6.119663132631003], time: 42.713
mmmaddpg vs mmmaddpg steps: 1624975, episodes: 65000, mean episode reward: 0.13878222107752186, agent episode reward: [-12.663657831252726, 6.401220026165124, 6.401220026165124], time: 40.943
mmmaddpg vs mmmaddpg steps: 1649975, episodes: 66000, mean episode reward: 0.8757657767727492, agent episode reward: [-12.46739847506033, 6.671582125916539, 6.671582125916539], time: 41.028
mmmaddpg vs mmmaddpg steps: 1674975, episodes: 67000, mean episode reward: -0.03119199249722412, agent episode reward: [-12.895651037398482, 6.432229522450629, 6.432229522450629], time: 43.308
mmmaddpg vs mmmaddpg steps: 1699975, episodes: 68000, mean episode reward: 0.9220591523808862, agent episode reward: [-12.693003026422057, 6.807531089401472, 6.807531089401472], time: 42.261
mmmaddpg vs mmmaddpg steps: 1724975, episodes: 69000, mean episode reward: 0.7428915011670404, agent episode reward: [-12.719596079804134, 6.731243790485587, 6.731243790485587], time: 42.763
mmmaddpg vs mmmaddpg steps: 1749975, episodes: 70000, mean episode reward: 0.41814342134023275, agent episode reward: [-13.23153122942835, 6.824837325384291, 6.824837325384291], time: 42.174
mmmaddpg vs mmmaddpg steps: 1774975, episodes: 71000, mean episode reward: 1.7034477645329953, agent episode reward: [-12.897214898957115, 7.3003313317450536, 7.3003313317450536], time: 42.407
mmmaddpg vs mmmaddpg steps: 1799975, episodes: 72000, mean episode reward: 1.5954660629614452, agent episode reward: [-12.856869259094474, 7.226167661027959, 7.226167661027959], time: 41.823
mmmaddpg vs mmmaddpg steps: 1824975, episodes: 73000, mean episode reward: 2.0384661745709995, agent episode reward: [-12.77163439967579, 7.405050287123395, 7.405050287123395], time: 40.12
mmmaddpg vs mmmaddpg steps: 1849975, episodes: 74000, mean episode reward: 1.5216182585460643, agent episode reward: [-13.269068122611118, 7.395343190578592, 7.395343190578592], time: 43.045
mmmaddpg vs mmmaddpg steps: 1874975, episodes: 75000, mean episode reward: 1.3883824598745027, agent episode reward: [-12.728264399066799, 7.058323429470651, 7.058323429470651], time: 43.059
mmmaddpg vs mmmaddpg steps: 1899975, episodes: 76000, mean episode reward: 2.3591753293146023, agent episode reward: [-13.231981525162537, 7.795578427238569, 7.795578427238569], time: 42.503
mmmaddpg vs mmmaddpg steps: 1924975, episodes: 77000, mean episode reward: 1.39358606869913, agent episode reward: [-12.094301229431563, 6.743943649065346, 6.743943649065346], time: 41.566
mmmaddpg vs mmmaddpg steps: 1949975, episodes: 78000, mean episode reward: 1.0961139975791483, agent episode reward: [-12.96506073433703, 7.030587365958089, 7.030587365958089], time: 40.872
mmmaddpg vs mmmaddpg steps: 1974975, episodes: 79000, mean episode reward: 2.022011514222316, agent episode reward: [-12.994302010380794, 7.508156762301555, 7.508156762301555], time: 40.415
mmmaddpg vs mmmaddpg steps: 1999975, episodes: 80000, mean episode reward: 2.5800251425503857, agent episode reward: [-13.302828844106154, 7.941426993328269, 7.941426993328269], time: 40.745
mmmaddpg vs mmmaddpg steps: 2024975, episodes: 81000, mean episode reward: 2.4297347992239176, agent episode reward: [-13.043205081900338, 7.736469940562128, 7.736469940562128], time: 40.689
mmmaddpg vs mmmaddpg steps: 2049975, episodes: 82000, mean episode reward: 1.8390049404123674, agent episode reward: [-12.92913272799822, 7.384068834205294, 7.384068834205294], time: 40.517
mmmaddpg vs mmmaddpg steps: 2074975, episodes: 83000, mean episode reward: 1.5584174195727662, agent episode reward: [-12.627784333986337, 7.093100876779551, 7.093100876779551], time: 40.787
mmmaddpg vs mmmaddpg steps: 2099975, episodes: 84000, mean episode reward: 1.6752554463163285, agent episode reward: [-12.61485172463509, 7.145053585475708, 7.145053585475708], time: 40.954
mmmaddpg vs mmmaddpg steps: 2124975, episodes: 85000, mean episode reward: 1.9616301123687188, agent episode reward: [-12.302966303576259, 7.132298207972489, 7.132298207972489], time: 40.686
mmmaddpg vs mmmaddpg steps: 2149975, episodes: 86000, mean episode reward: 1.011034056774809, agent episode reward: [-12.589838193615632, 6.800436125195222, 6.800436125195222], time: 38.097
mmmaddpg vs mmmaddpg steps: 2174975, episodes: 87000, mean episode reward: 2.028201881922923, agent episode reward: [-12.922884543566271, 7.475543212744596, 7.475543212744596], time: 39.157
mmmaddpg vs mmmaddpg steps: 2199975, episodes: 88000, mean episode reward: 1.2925868268136824, agent episode reward: [-12.786489466518404, 7.039538146666043, 7.039538146666043], time: 41.254
mmmaddpg vs mmmaddpg steps: 2224975, episodes: 89000, mean episode reward: 1.319607862505932, agent episode reward: [-12.983674214590726, 7.151641038548329, 7.151641038548329], time: 41.352
mmmaddpg vs mmmaddpg steps: 2249975, episodes: 90000, mean episode reward: 0.7130909747085571, agent episode reward: [-12.03912645704104, 6.376108715874798, 6.376108715874798], time: 40.853
mmmaddpg vs mmmaddpg steps: 2274975, episodes: 91000, mean episode reward: 1.543376362492194, agent episode reward: [-13.061524438262774, 7.3024504003774835, 7.3024504003774835], time: 40.741
mmmaddpg vs mmmaddpg steps: 2299975, episodes: 92000, mean episode reward: 1.0547245467956805, agent episode reward: [-12.419507140217974, 6.737115843506826, 6.737115843506826], time: 40.621
mmmaddpg vs mmmaddpg steps: 2324975, episodes: 93000, mean episode reward: 1.7270683051172524, agent episode reward: [-13.104614797372891, 7.415841551245072, 7.415841551245072], time: 39.997
mmmaddpg vs mmmaddpg steps: 2349975, episodes: 94000, mean episode reward: 1.8661278953967413, agent episode reward: [-12.390386151669416, 7.128257023533078, 7.128257023533078], time: 40.781
mmmaddpg vs mmmaddpg steps: 2374975, episodes: 95000, mean episode reward: 1.7678833706227133, agent episode reward: [-12.820611395843523, 7.294247383233118, 7.294247383233118], time: 40.781
mmmaddpg vs mmmaddpg steps: 2399975, episodes: 96000, mean episode reward: 1.2729813021316805, agent episode reward: [-13.20401063285159, 7.238495967491636, 7.238495967491636], time: 40.352
mmmaddpg vs mmmaddpg steps: 2424975, episodes: 97000, mean episode reward: 1.780707335090255, agent episode reward: [-12.615225683984615, 7.1979665095374346, 7.1979665095374346], time: 40.596
mmmaddpg vs mmmaddpg steps: 2449975, episodes: 98000, mean episode reward: 1.7308331917103243, agent episode reward: [-13.04490431741318, 7.38786875456175, 7.38786875456175], time: 40.506
mmmaddpg vs mmmaddpg steps: 2474975, episodes: 99000, mean episode reward: 1.6122338498143987, agent episode reward: [-13.05059490072219, 7.331414375268294, 7.331414375268294], time: 40.703
mmmaddpg vs mmmaddpg steps: 2499975, episodes: 100000, mean episode reward: 1.246787288504211, agent episode reward: [-12.872536726463204, 7.059662007483708, 7.059662007483708], time: 40.445
...Finished total of 100001 episodes.
